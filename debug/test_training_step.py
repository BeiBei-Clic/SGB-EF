#!/usr/bin/env python3
"""
è°ƒè¯•è®­ç»ƒæ­¥éª¤ - æµ‹è¯•å•ä¸ªè®­ç»ƒå‰å‘ä¼ æ’­
"""

import sys
sys.path.append('/home/xyh/SGB-EF')

import torch
import numpy as np
from src.training.flow import tokens_to_prob, sample_conditional_path, remove_gap_tokens, KappaScheduler
from src.symbolic.data_generator import generate_flow_samples
from src.utils.special_tokens import SpecialTokensManager
from src.modeling.editflow_transformer import EditFlowTransformer, EditFlowConfig
from src.modeling.condition_encoder import ConditionEncoder
from transformers import AutoTokenizer

def test_single_forward_pass():
    """æµ‹è¯•å•ä¸ªå‰å‘ä¼ æ’­"""
    print("=== æµ‹è¯•å•ä¸ªå‰å‘ä¼ æ’­ ===")

    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # åˆå§‹åŒ–tokenizer
    tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-uncased")
    special_tokens_manager = SpecialTokensManager(tokenizer, max_dim=10)

    # ç”Ÿæˆæ•°æ®
    print("1. ç”Ÿæˆè®­ç»ƒæ•°æ®...")
    samples = generate_flow_samples(2, max_dim=2, n_points=5, max_depth=2)

    # åˆå§‹åŒ–æ¨¡åž‹
    print("2. åˆå§‹åŒ–æ¨¡åž‹...")
    condition_encoder = ConditionEncoder("Qwen/Qwen3-Embedding-0.6B")
    config = EditFlowConfig(
        condition_dim=condition_encoder.output_dim,
        base_model_name="google-bert/bert-base-uncased",
    )
    model = EditFlowTransformer(config)

    model = model.to(device)
    condition_encoder = condition_encoder.to(device)

    print(f"æ¨¡åž‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")

    # å¤„ç†æ•°æ®
    print("3. å¤„ç†æ•°æ®...")
    sample = samples[0]

    # Tokenizeè¡¨è¾¾å¼
    z0_tokens = []
    z1_tokens = []
    for token in sample['z0_tokens']:
        z0_tokens.extend(special_tokens_manager.tokenize_expression(token))
    for token in sample['z1_tokens']:
        z1_tokens.extend(special_tokens_manager.tokenize_expression(token))

    # åˆ›å»ºå¼ é‡
    max_len = 128
    bos_token = tokenizer.cls_token_id
    pad_token = tokenizer.pad_token_id

    z0_tensor = torch.zeros(1, max_len, dtype=torch.long, device=device)
    z1_tensor = torch.zeros(1, max_len, dtype=torch.long, device=device)

    z0_seq = [bos_token] + z0_tokens[:max_len-1]
    z1_seq = [bos_token] + z1_tokens[:max_len-1]

    z0_tensor[0, :len(z0_seq)] = torch.tensor(z0_seq)
    z1_tensor[0, :len(z1_seq)] = torch.tensor(z1_seq)

    print(f"z0åºåˆ—é•¿åº¦: {len(z0_seq)}")
    print(f"z1åºåˆ—é•¿åº¦: {len(z1_seq)}")
    print(f"z0_tokenèŒƒå›´: [{z0_tensor.min()}, {z0_tensor.max()}]")
    print(f"z1_tokenèŒƒå›´: [{z1_tensor.min()}, {z1_tensor.max()}]")

    # å‡†å¤‡æ¡ä»¶ç¼–ç 
    print("4. å‡†å¤‡æ¡ä»¶ç¼–ç ...")
    x_values = torch.FloatTensor([sample['x_values']]).to(device)
    residuals = torch.FloatTensor([sample['residuals']]).to(device)

    condition_embeddings = condition_encoder(x_values, residuals)
    print(f"æ¡ä»¶åµŒå…¥å½¢çŠ¶: {condition_embeddings.shape}")

    # å‰å‘ä¼ æ’­
    print("5. æ‰§è¡Œå‰å‘ä¼ æ’­...")
    batch_size = 1

    # æ—¶é—´æ­¥
    t = torch.rand(batch_size, 1, device=device)
    print(f"æ—¶é—´æ­¥: {t}")

    # è½¬æ¢ä¸ºæ¦‚çŽ‡åˆ†å¸ƒ
    print("6. è½¬æ¢ä¸ºæ¦‚çŽ‡åˆ†å¸ƒ...")
    z0_probs = tokens_to_prob(z0_tensor, tokenizer.vocab_size)
    z1_probs = tokens_to_prob(z1_tensor, tokenizer.vocab_size)
    print(f"z0_probså½¢çŠ¶: {z0_probs.shape}")
    print(f"z1_probså½¢çŠ¶: {z1_probs.shape}")
    print(f"z0_probsæ¦‚çŽ‡å’Œ: {z0_probs.sum(dim=-1)}")
    print(f"z1_probsæ¦‚çŽ‡å’Œ: {z1_probs.sum(dim=-1)}")

    # æ£€æŸ¥æ¦‚çŽ‡åˆ†å¸ƒæœ‰æ•ˆæ€§
    print(f"z0_probsæ˜¯å¦åŒ…å«NaN: {torch.isnan(z0_probs).any()}")
    print(f"z1_probsæ˜¯å¦åŒ…å«NaN: {torch.isnan(z1_probs).any()}")
    print(f"z0_probsæ˜¯å¦åŒ…å«Inf: {torch.isinf(z0_probs).any()}")
    print(f"z1_probsæ˜¯å¦åŒ…å«Inf: {torch.isinf(z1_probs).any()}")

    # é‡‡æ ·æ¡ä»¶è·¯å¾„
    print("7. é‡‡æ ·æ¡ä»¶è·¯å¾„...")
    scheduler = KappaScheduler('cubic')
    z_t = sample_conditional_path(z0_probs, z1_probs, t, scheduler)
    print(f"z_tå½¢çŠ¶: {z_t.shape}")
    print(f"z_tèŒƒå›´: [{z_t.min()}, {z_t.max()}]")
    print(f"z_tæœ‰æ•ˆèŒƒå›´: {(z_t >= 0).all() and (z_t < tokenizer.vocab_size).all()}")

    # ç§»é™¤gap tokens
    print("8. ç§»é™¤gap tokens...")
    gap_token = special_tokens_manager.get_gap_token_id()
    x_t, x_pad_mask, z_gap_mask, z_pad_mask = remove_gap_tokens(
        z_t, tokenizer.vocab_size, gap_token
    )
    print(f"x_tå½¢çŠ¶: {x_t.shape}")
    print(f"x_pad_maskå½¢çŠ¶: {x_pad_mask.shape}")
    print(f"x_tæœ‰æ•ˆèŒƒå›´: {(x_t >= 0).all() and (x_t < tokenizer.vocab_size).all()}")

    # åˆ›å»ºattention mask
    attention_mask = (~x_pad_mask).float()
    print(f"attention_maskå½¢çŠ¶: {attention_mask.shape}")
    print(f"attention_maskèŒƒå›´: [{attention_mask.min()}, {attention_mask.max()}]")

    # æ¨¡åž‹å‰å‘ä¼ æ’­
    print("9. æ¨¡åž‹å‰å‘ä¼ æ’­...")
    pred_rates, pred_ins_probs, pred_sub_probs = model(
        input_ids=x_t,
        time_steps=t,
        condition=condition_embeddings,
        attention_mask=attention_mask
    )

    print(f"pred_rateså½¢çŠ¶: {pred_rates.shape}")
    print(f"pred_ins_probså½¢çŠ¶: {pred_ins_probs.shape}")
    print(f"pred_sub_probså½¢çŠ¶: {pred_sub_probs.shape}")

    # æ£€æŸ¥è¾“å‡ºæœ‰æ•ˆæ€§
    print(f"pred_ratesæ˜¯å¦åŒ…å«NaN: {torch.isnan(pred_rates).any()}")
    print(f"pred_ins_probsæ˜¯å¦åŒ…å«NaN: {torch.isnan(pred_ins_probs).any()}")
    print(f"pred_sub_probsæ˜¯å¦åŒ…å«NaN: {torch.isnan(pred_sub_probs).any()}")

    print(f"pred_ratesæ˜¯å¦åŒ…å«Inf: {torch.isinf(pred_rates).any()}")
    print(f"pred_ins_probsæ˜¯å¦åŒ…å«Inf: {torch.isinf(pred_ins_probs).any()}")
    print(f"pred_sub_probsæ˜¯å¦åŒ…å«Inf: {torch.isinf(pred_sub_probs).any()}")

    print(f"pred_ratesèŒƒå›´: [{pred_rates.min():.6f}, {pred_rates.max():.6f}]")
    print(f"pred_ins_probsèŒƒå›´: [{pred_ins_probs.min():.6f}, {pred_ins_probs.max():.6f}]")
    print(f"pred_sub_probsèŒƒå›´: [{pred_sub_probs.min():.6f}, {pred_sub_probs.max():.6f}]")

    print("âœ… å‰å‘ä¼ æ’­å®Œæˆï¼Œæ— CUDAé”™è¯¯!")

def main():
    print("å¼€å§‹è°ƒè¯•è®­ç»ƒæ­¥éª¤...")

    test_single_forward_pass()

    print("\nðŸŽ‰ å•æ­¥è®­ç»ƒæµ‹è¯•å®Œæˆ!")

if __name__ == "__main__":
    main()