"""
ç¬¦å·å›å½’æ•°æ®ç”Ÿæˆå™¨ï¼Œç”¨äºEditFlowé¢„è®­ç»ƒ
"""

import numpy as np
import random
import os
import warnings
import time
import json
import multiprocessing
import subprocess
from typing import List, Dict, Tuple
from tqdm import tqdm
from src.utils.timeout_utils import TimeoutError, with_timeout
from src.utils.logger import Logger
from src.symbolic.symbolic_utils import generate_random_expr, evaluate_expression_safe, expr_to_tree
from src.symbolic.corruption import corrupt_expression
from src.symbolic.sample_generator import generate_single_sample, set_logger

warnings.filterwarnings('ignore', category=RuntimeWarning)

# åˆ›å»ºå…¨å±€ Logger å®ä¾‹ç”¨äºæ ·æœ¬ç”Ÿæˆæ—¥å¿—
_sample_logger = Logger(enabled=True)

# å¸¸é‡å®šä¹‰
MAX_RETRIES = 5  # è¡¨è¾¾å¼ç”Ÿæˆå’Œè®¡ç®—çš„æœ€å¤§é‡è¯•æ¬¡æ•°

def generate_batch_worker(args: Tuple) -> Tuple[int, List[Dict], Dict[int, int]]:
    """å•ä¸ªè¿›ç¨‹å¤„ç†ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®ç”Ÿæˆ

    Args:
        args: (batch_idx, current_batch_size, max_dim, n_points, max_depth,
               max_expr_length, batch_filename, verbose, process_id)

    Returns:
        (æ‰¹æ¬¡ç´¢å¼•, ç”Ÿæˆçš„æ ·æœ¬åˆ—è¡¨, ç»´åº¦ç»Ÿè®¡)
    """
    (batch_idx, current_batch_size, max_dim, n_points, max_depth,
     max_expr_length, batch_filename, verbose, process_id) = args

    # è®¾ç½®è¿›ç¨‹ç‰¹å®šçš„éšæœºç§å­
    current_time_ms = int(time.time() * 1000000)
    seed_base = current_time_ms + (process_id << 16) + (batch_idx << 8) + os.getpid()
    seed_val = hash(str(seed_base)) & 0x7fffffff

    random.seed(seed_val)
    np.random.seed(seed_val)

    process_prefix = f"[B{batch_idx+1}]"
    batch_samples = []
    dimension_count = {}
    sample_count = 0
    attempt_count = 0
    fail_count = 0
    consecutive_fails = 0
    SAMPLE_TIMEOUT = 10.0

    while sample_count < current_batch_size:
        attempt_count += 1
        consecutive_fails += 1
        unique_factor = random.randint(0, 999999)
        sample_id = f"{process_prefix}_sample{sample_count}_{os.getpid()}_{unique_factor}"

        try:
            generated_samples = with_timeout(
                generate_single_sample,
                SAMPLE_TIMEOUT,
                sample_id,
                max_dim,
                n_points,
                max_depth,
                max_expr_length,
                batch_idx,
                current_batch_size,
                sample_count
            )

            if generated_samples:
                dim = generated_samples[0]["input_dimension"]
                for sample in generated_samples:
                    if sample_count >= current_batch_size:
                        break
                    batch_samples.append(sample)
                    sample_count += 1
                    dimension_count[dim] = dimension_count.get(dim, 0) + 1
                consecutive_fails = 0
            else:
                fail_count += 1
                if fail_count % 100 == 0:
                    pass
                _sample_logger.sample_failed(sample_id, "No samples generated")
                continue

        except TimeoutError:
            fail_count += 1
            _sample_logger.sample_timeout(sample_id, SAMPLE_TIMEOUT)
            continue

        except Exception as e:
            _sample_logger.sample_error(sample_id, type(e).__name__, str(e))

            if batch_samples:
                try:
                    os.makedirs(os.path.dirname(batch_filename), exist_ok=True)
                    with open(batch_filename, 'w', encoding='utf-8') as f:
                        for sample in batch_samples:
                            f.write(json.dumps(sample, ensure_ascii=False) + '\n')
                except Exception as save_error:
                    pass

            return batch_idx, -1, {}

    if batch_samples:
        os.makedirs(os.path.dirname(batch_filename), exist_ok=True)
        with open(batch_filename, 'w', encoding='utf-8') as f:
            for sample in batch_samples:
                f.write(json.dumps(sample, ensure_ascii=False) + '\n')

    return batch_idx, len(batch_samples), dimension_count

def generate_flow_samples(
    num_samples: int,
    max_dim: int = 5,
    n_points: int = 100,
    max_depth: int = 4,
    max_expr_length: int = 15,
    batch_size: int = 50000,
    verbose: bool = True,
    num_processes: int = None,
    alignment_method: str = 'randomized',
):
    """ç”Ÿæˆç”¨äºEditFlowè¿ç»­æµè®­ç»ƒçš„æ•°æ®æ–‡ä»¶ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ å’Œå¤šè¿›ç¨‹å¹¶è¡Œå¤„ç†

    Args:
        num_samples: æ€»æ ·æœ¬æ•°
        max_dim: æœ€å¤§ç»´åº¦
        n_points: æ¯ä¸ªæ ·æœ¬çš„æ•°æ®ç‚¹æ•°
        max_depth: è¡¨è¾¾å¼æœ€å¤§æ·±åº¦
        max_expr_length: è¡¨è¾¾å¼æœ€å¤§tokenæ•°é‡ï¼ˆå‰åºéå†ï¼Œé»˜è®¤15ï¼‰
        batch_size: æ‰¹æ¬¡å¤§å°
        verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†è¾“å‡º
        num_processes: è¿›ç¨‹æ•°ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨æ‰€æœ‰å¯ç”¨CPUæ ¸å¿ƒ
        alignment_method: å¯¹é½æ–¹æ³•ï¼Œ'levenshtein' (ç¡®å®šæ€§) æˆ– 'randomized' (éšæœºåŒ–ï¼Œæ¥è‡ªEdit Flowsè®ºæ–‡)
    """
    set_logger(_sample_logger)

    from src.symbolic.sample_generator import set_alignment_method
    set_alignment_method(alignment_method)

    # è®¾ç½®ä¸»éšæœºç§å­
    main_time_ms = int(time.time() * 1000000)
    main_seed_base = main_time_ms + os.getpid() + (num_samples & 0xffff)
    seed_val = hash(str(main_seed_base)) & 0x7fffffff
    random.seed(seed_val)
    np.random.seed(seed_val)

    # ä¸»æ–‡ä»¶ä½¿ç”¨parquetæ ¼å¼
    filename = f"data/flow_samples_{num_samples}_{max_dim}dim_{n_points}pts_{max_depth}depth_{max_expr_length}len.parquet"
    num_batches = (num_samples + batch_size - 1) // batch_size
    temp_dir = "data/temp"
    os.makedirs(temp_dir, exist_ok=True)
    batch_filenames = [f"{temp_dir}/{os.path.basename(filename).replace('.parquet', f'_batch_{i + 1}.txt')}" for i in range(num_batches)]

    # æ–­ç‚¹ç»­ä¼ æ£€æŸ¥é€»è¾‘ï¼ˆåªæ£€æŸ¥parquetï¼‰
    # æƒ…å†µ1ï¼šparquetæ–‡ä»¶å­˜åœ¨ â†’ æ•°æ®å®Œæ•´ï¼Œç›´æ¥è¿”å›
    if os.path.exists(filename):
        if verbose:
            print(f"âœ“ Parquetæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ç”Ÿæˆ: {filename}")
        return

    # æƒ…å†µ2ï¼šparquetä¸å­˜åœ¨ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰ä¸­æ–­çš„ç”Ÿæˆä»»åŠ¡
    txt_filename = filename.replace('.parquet', '.txt')
    if os.path.exists(txt_filename) and any(os.path.exists(f) for f in batch_filenames):
        if verbose:
            print(f"æ£€æµ‹åˆ°ä¸­æ–­çš„ç”Ÿæˆä»»åŠ¡ï¼Œæ­£åœ¨æ¢å¤...")
        merge_batches_to_main_file(txt_filename, batch_filenames, num_batches, verbose=verbose)
        return

    # æƒ…å†µ3ï¼štxtæ–‡ä»¶å­˜åœ¨ä½†æ‰¹æ¬¡æ–‡ä»¶éƒ½ä¸å­˜åœ¨ â†’ æ•°æ®ç”Ÿæˆå·²å®Œæˆï¼Œç›´æ¥ç”Ÿæˆparquet
    if os.path.exists(txt_filename) and not any(os.path.exists(f) for f in batch_filenames):
        if verbose:
            print(f"æ£€æµ‹åˆ°å·²å®Œæˆçš„æ•°æ®ç”Ÿæˆ(txtæ–‡ä»¶å­˜åœ¨ï¼Œæ‰¹æ¬¡æ–‡ä»¶å·²åˆå¹¶)ï¼Œæ­£åœ¨ç”Ÿæˆ Parquet æ–‡ä»¶...")
        merge_batches_to_main_file(txt_filename, batch_filenames, num_batches, verbose=verbose)
        return

    if num_processes is None:
        num_processes = multiprocessing.cpu_count()

    total_dimension_count = {}
    retry_count = 0
    all_success = False

    while not all_success:
        batch_tasks = []
        for batch_idx in range(num_batches):
            batch_filename = batch_filenames[batch_idx]
            current_batch_size = min(batch_size, num_samples - batch_idx * batch_size)

            if os.path.exists(batch_filename):
                continue

            process_id = len(batch_tasks) % num_processes
            batch_tasks.append((
                batch_idx, current_batch_size, max_dim, n_points, max_depth,
                max_expr_length, batch_filename, verbose, process_id
            ))

        if not batch_tasks:
            all_success = True
        else:
            try:
                with multiprocessing.Pool(processes=num_processes) as pool:
                    chunksize = max(1, len(batch_tasks) // (num_processes * 4))
                    results_iter = pool.imap_unordered(
                        generate_batch_worker,
                        batch_tasks,
                        chunksize=chunksize
                    )

                    failed_batches = []
                    completed_count = 0

                    for result in results_iter:
                        batch_idx, sample_count, dimension_count = result
                        completed_count += 1

                        if sample_count == -1:
                            failed_batches.append(batch_idx)
                            batch_filename = batch_filenames[batch_idx]
                            if os.path.exists(batch_filename):
                                os.remove(batch_filename)
                        else:
                            for dim, count in dimension_count.items():
                                total_dimension_count[dim] = total_dimension_count.get(dim, 0) + count

                if verbose:
                    print(f"\næ‰€æœ‰ {len(batch_tasks)} ä¸ªæ‰¹æ¬¡ä»»åŠ¡å¤„ç†å®Œæˆ")

                if failed_batches:
                    retry_count += 1
                else:
                    all_success = True

            except (BrokenPipeError, KeyboardInterrupt, Exception) as e:
                if isinstance(e, (BrokenPipeError, KeyboardInterrupt)):
                    raise
                else:
                    retry_count += 1

        # éªŒè¯æ‰¹æ¬¡å®Œæ•´æ€§
        missing_batches = [batch_idx for batch_idx, batch_filename in enumerate(batch_filenames) if not os.path.exists(batch_filename)]

        if missing_batches:
            retry_count += 1
        else:
            all_success = True
            break

    if verbose and total_dimension_count:
        dim_dist = ', '.join(f"{dim}ç»´:{count}ä¸ª" for dim, count in sorted(total_dimension_count.items()))
        print(f"\nå·²å®Œæˆæ‰¹æ¬¡çš„ç»´åº¦åˆ†å¸ƒ: {dim_dist}")

    # åˆå¹¶æ‰¹æ¬¡æ–‡ä»¶åˆ°txtï¼Œç„¶åç”Ÿæˆparquet
    txt_filename = filename.replace('.parquet', '.txt')
    merge_batches_to_main_file(txt_filename, batch_filenames, num_batches, verbose=verbose)


def merge_batches_to_main_file(filename: str, batch_filenames: List[str], num_batches: int, verbose: bool = True):
    """åˆå¹¶æ‰¹æ¬¡æ–‡ä»¶åˆ°ä¸»æ–‡ä»¶ï¼Œå¹¶ç”ŸæˆParquetæ ¼å¼

    Args:
        filename: txtä¸»æ–‡ä»¶å
        batch_filenames: æ‰¹æ¬¡æ–‡ä»¶åˆ—è¡¨
        num_batches: æ€»æ‰¹æ¬¡æ•°
        verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†è¾“å‡º
    """
    index_filename = filename.replace('.txt', '_dimension_index.json')
    parquet_filename = filename.replace('.txt', '.parquet')
    dimension_samples = {}

    if os.path.exists(index_filename) and os.path.exists(filename):
        with open(index_filename, 'r', encoding='utf-8') as f:
            index_data = json.load(f)
        dimension_samples = {int(dim_str): positions for dim_str, positions in index_data.items()}

    with open(filename, 'a', encoding='utf-8') as main_file:
        for batch_idx in range(num_batches):
            batch_filename = batch_filenames[batch_idx]
            if os.path.exists(batch_filename):
                batch_samples = []
                with open(batch_filename, 'r', encoding='utf-8') as f:
                    for line in f:
                        line = line.strip()
                        if line:
                            batch_samples.append(json.loads(line))

                for sample in batch_samples:
                    pos = main_file.tell()
                    dim = sample['input_dimension']
                    if dim not in dimension_samples:
                        dimension_samples[dim] = []
                    dimension_samples[dim].append(pos)

                    main_file.write(json.dumps(sample, ensure_ascii=False) + '\n')

                os.remove(batch_filename)

    os.makedirs(os.path.dirname(index_filename), exist_ok=True)
    with open(index_filename, 'w', encoding='utf-8') as f:
        json.dump({str(dim): [int(pos) for pos in positions] for dim, positions in dimension_samples.items()}, f, indent=2)

    # ç”ŸæˆParquetæ–‡ä»¶ï¼ˆæ›´é«˜æ•ˆçš„æ ¼å¼ï¼‰- ä½¿ç”¨åˆ†æ‰¹è¯»å–é¿å…å†…å­˜æº¢å‡º
    if not os.path.exists(parquet_filename):
        if verbose:
            print(f"\n{'='*70}")
            print(f"ğŸ”„ æ­£åœ¨ç”Ÿæˆ Parquet æ–‡ä»¶")
            print(f"{'='*70}")
            print(f"ğŸ“ æºæ–‡ä»¶: {filename}")
            print(f"ğŸ“ ç›®æ ‡æ–‡ä»¶: {parquet_filename}")

        import pandas as pd
        from tqdm import tqdm
        import pyarrow as pa
        import pyarrow.parquet as pq
        import time
        import psutil

        # åˆ†æ‰¹è¯»å–txtæ–‡ä»¶å¹¶å†™å…¥parquetï¼Œé¿å…ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰æ•°æ®åˆ°å†…å­˜
        BATCH_SIZE = 50000  # æ¯æ‰¹å¤„ç†5ä¸‡ä¸ªæ ·æœ¬
        samples_batch = []
        total_samples = 0
        batch_num = 0

        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()

        # è·å–æ€»è¡Œæ•°ç”¨äºè¿›åº¦æ˜¾ç¤ºï¼ˆä½¿ç”¨wcå‘½ä»¤å¿«é€Ÿç»Ÿè®¡ï¼‰
        if verbose:
            print(f"\nâ³ æ­£åœ¨ç»Ÿè®¡æ€»æ ·æœ¬æ•°...")
        result = subprocess.run(['wc', '-l', filename], capture_output=True, text=True)
        total_lines = int(result.stdout.split()[0])

        if verbose:
            print(f"ğŸ“Š è½¬æ¢é…ç½®:")
            print(f"  â€¢ æ€»æ ·æœ¬æ•°: {total_lines:,}")
            print(f"  â€¢ æ‰¹æ¬¡å¤§å°: {BATCH_SIZE:,} æ ·æœ¬/æ‰¹")
            print(f"  â€¢ é¢„è®¡æ‰¹æ¬¡æ•°: {(total_lines + BATCH_SIZE - 1) // BATCH_SIZE}")
            print(f"\n{'='*70}\n")

        # ä½¿ç”¨pyarrow.ParquetWriterè¿›è¡Œè¿½åŠ å†™å…¥
        writer = None
        schema = None

        # åˆ›å»ºå¢å¼ºçš„è¿›åº¦æ¡
        with open(filename, 'r', encoding='utf-8') as f:
            pbar = tqdm(
                total=total_lines,
                desc="ğŸ“¦ è½¬æ¢è¿›åº¦",
                unit="æ ·æœ¬",
                unit_scale=True,
                ncols=100,
                bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]'
            )

            try:
                for line in f:
                    line = line.strip()
                    if line:
                        sample = json.loads(line)
                        samples_batch.append(sample)

                        # å½“æ‰¹æ¬¡è¾¾åˆ°æŒ‡å®šå¤§å°æ—¶ï¼Œå†™å…¥parquet
                        if len(samples_batch) >= BATCH_SIZE:
                            df_batch = pd.DataFrame(samples_batch)

                            # ç¬¬ä¸€æ¬¡å†™å…¥æ—¶åˆå§‹åŒ–writerå’Œschema
                            if writer is None:
                                schema = pa.Table.from_pandas(df_batch).schema
                                writer = pq.ParquetWriter(
                                    parquet_filename,
                                    schema=schema,
                                    compression='snappy'
                                )

                            # å†™å…¥å½“å‰æ‰¹æ¬¡
                            table = pa.Table.from_pandas(df_batch, schema=schema)
                            writer.write_table(table)

                            total_samples += len(samples_batch)
                            batch_num += 1

                            # æ›´æ–°è¿›åº¦æ¡
                            pbar.update(BATCH_SIZE)

                            # æ¯æ‰¹æ¬¡æ›´æ–°è¯¦ç»†ç»Ÿè®¡
                            if verbose:
                                elapsed = time.time() - start_time
                                speed = total_samples / elapsed if elapsed > 0 else 0
                                progress_pct = 100 * total_samples / total_lines
                                eta = (total_lines - total_samples) / speed if speed > 0 else 0

                                # è·å–å†…å­˜ä½¿ç”¨æƒ…å†µ
                                process = psutil.Process()
                                memory_mb = process.memory_info().rss / (1024**2)

                                # æ¯5ä¸ªæ‰¹æ¬¡æ˜¾ç¤ºä¸€æ¬¡è¯¦ç»†ç»Ÿè®¡
                                if batch_num % 5 == 0:
                                    pbar.write(
                                        f"  ğŸ“Š æ‰¹æ¬¡ #{batch_num:3d} | "
                                        f"è¿›åº¦: {progress_pct:6.2f}% | "
                                        f"é€Ÿåº¦: {speed:8.1f} æ ·æœ¬/ç§’ | "
                                        f"ETA: {eta/60:5.1f}åˆ†é’Ÿ | "
                                        f"å†…å­˜: {memory_mb:6.1f}MB"
                                    )

                            samples_batch = []  # æ¸…ç©ºæ‰¹æ¬¡ï¼Œé‡Šæ”¾å†…å­˜

                # å¤„ç†æœ€åå‰©ä½™çš„æ ·æœ¬
                if samples_batch:
                    df_batch = pd.DataFrame(samples_batch)

                    if writer is None:
                        schema = pa.Table.from_pandas(df_batch).schema
                        writer = pq.ParquetWriter(
                            parquet_filename,
                            schema=schema,
                            compression='snappy'
                        )

                    table = pa.Table.from_pandas(df_batch, schema=schema)
                    writer.write_table(table)
                    total_samples += len(samples_batch)
                    pbar.update(len(samples_batch))

            finally:
                pbar.close()

        # å…³é—­writer
        if writer is not None:
            writer.close()

        # è®¡ç®—æ€»è€—æ—¶
        end_time = time.time()
        total_time = end_time - start_time
        avg_speed = total_samples / total_time if total_time > 0 else 0

        if verbose:
            txt_size = os.path.getsize(filename) / (1024**3)
            parquet_size = os.path.getsize(parquet_filename) / (1024**3)
            compression_ratio = (1 - parquet_size / txt_size) * 100

            print(f"\n{'='*70}")
            print(f"âœ… Parquet æ–‡ä»¶ç”Ÿæˆå®Œæˆ")
            print(f"{'='*70}")
            print(f"ğŸ“ æ–‡ä»¶ä¿¡æ¯:")
            print(f"  â€¢ TXT å¤§å°:     {txt_size:.2f} GB")
            print(f"  â€¢ Parquet å¤§å°:  {parquet_size:.2f} GB")
            print(f"  â€¢ å‹ç¼©ç‡:       {compression_ratio:.1f}%")
            print(f"  â€¢ æ ·æœ¬æ•°é‡:     {total_samples:,}")
            print(f"\nâ±ï¸  æ€§èƒ½ç»Ÿè®¡:")
            print(f"  â€¢ æ€»è€—æ—¶:       {total_time:.1f} ç§’ ({total_time/60:.1f} åˆ†é’Ÿ)")
            print(f"  â€¢ å¹³å‡é€Ÿåº¦:     {avg_speed:.1f} æ ·æœ¬/ç§’")
            print(f"  â€¢ æ‰¹æ¬¡æ€»æ•°:     {batch_num} æ‰¹")
            print(f"  â€¢ å¹³å‡æ‰¹æ¬¡è€—æ—¶: {total_time/batch_num if batch_num > 0 else 0:.2f} ç§’/æ‰¹")
            print(f"{'='*70}\n")
    elif verbose:
        print(f"âœ“ Parquet æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ç”Ÿæˆ: {parquet_filename}")