"""
EditFlowè¿­ä»£ä¼˜åŒ–è®­ç»ƒå™¨ - å®ç°åŸºäºè¿­ä»£å¼ç¼–è¾‘æ“ä½œçš„ç¬¦å·å›å½’æ¨¡å‹è®­ç»ƒ
ä½¿ç”¨ Hugging Face Accelerate è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒåŠ é€Ÿ
"""

import os
import time

import torch
import torch.nn.functional as F
import numpy as np
from tqdm import tqdm
from accelerate import Accelerator
from accelerate.utils import set_seed

from ..symbolic.data_generator import generate_flow_samples
from .flow import (
    remove_gap_tokens, fill_gap_tokens_with_repeats,
    ContinuousFlowLoss, FlowDataset, custom_collate_fn
)
from ..modeling.condition_encoder import SetTransformerConditionEncoder
from ..modeling.llama_editflow import LlamaEditFlowBackbone
from ..utils.misc_utils import find_latest_checkpoint, load_checkpoint
from ..utils.logger import Logger
from .greedy_search import SimpleSymbolicRegression


class EditFlowManager:
    """EditFlowæ¨¡å‹ç®¡ç†å™¨ - æ”¯æŒè®­ç»ƒå’Œæ¨ç†åŠŸèƒ½

    æ¶æ„ç‰¹ç‚¹ï¼šè¿­ä»£ä¼˜åŒ–æ¨¡å¼
    - æ¨¡å‹ç›´æ¥é¢„æµ‹ä»z0åˆ°z1çš„ç¼–è¾‘æ“ä½œï¼ˆæ’å…¥ã€åˆ é™¤ã€æ›¿æ¢ï¼‰
    - æ—¶é—´æ­¥å›ºå®šä¸º0ï¼Œå­¦ä¹ ä»èµ·ç‚¹åˆ°ç›®æ ‡çš„ç›´æ¥ç¼–è¾‘è·¯å¾„
    - ä½¿ç”¨ç›®æ ‡å€¼y_targetä½œä¸ºæ¡ä»¶ï¼ˆè€Œéæ®‹å·®ï¼‰ï¼Œä¿æŒæ¡ä»¶æ’å®šä½œä¸º"åŒ—ææ˜Ÿ"
    """

    # ç±»å¸¸é‡ï¼šè®­ç»ƒå’Œæ¨ç†é…ç½®å‚æ•°
    GRADIENT_CLIP_NORM = 10.0
    NUMERICAL_CLIP_THRESHOLD = 1e6
    MAX_EXPRESSION_LENGTH = 50
    MIN_ACTION_SCORE = 0.01

    def __init__(self, args):
        self.args = args

        # åˆå§‹åŒ– Accelerate - è‡ªåŠ¨å¤„ç†åˆ†å¸ƒå¼è®­ç»ƒè®¾ç½®
        # æ³¨æ„ï¼šmixed_precision ç”± accelerate launch å‘½ä»¤è¡Œå‚æ•°æ§åˆ¶
        self.accelerator = Accelerator(
            gradient_accumulation_steps=args.gradient_accumulation_steps,
            log_with=args.log_with
        )

        set_seed(args.seed)
        self.debug_mode = args.debug
        self.logger = Logger(self.accelerator, enabled=True, debug_mode=self.debug_mode)
        self.device = self.accelerator.device

        if self.accelerator.is_local_main_process:
            print("=== EditFlowç¬¦å·å›å½’é¢„è®­ç»ƒ (ä½¿ç”¨ Accelerate åŠ é€Ÿ) ===")
            print(f"æ ·æœ¬æ•°: {self.args.num_samples}")
            print(f"æœ€å¤§ç»´åº¦: {self.args.max_dim}")
            print(f"è¡¨è¾¾å¼æœ€å¤§é•¿åº¦: {self.args.max_expr_length}")
            print(f"æ‰¹æ¬¡å¤§å°: {self.args.batch_size}")
            print(f"è®­ç»ƒè½®æ•°: {self.args.num_epochs}")
            print(f"å­¦ä¹ ç‡: {self.args.learning_rate}")
            print(f"æµ‹è¯•é›†æ¯”ä¾‹: {self.args.test_split}")
            print(f"è¯„ä¼°é¢‘ç‡: æ¯{self.args.eval_every}è½®")
            print(f"LLaMAæ¨¡å‹é…ç½®: hidden_dim={self.args.hidden_dim}, n_layers={self.args.n_layers}, n_heads={self.args.n_heads}")
            print(f"æ¡ä»¶åµŒå…¥æ¨¡å‹: {self.args.condition_model_name}")
            print(f"æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {self.args.gradient_accumulation_steps}")
            print(f"FP16æ··åˆç²¾åº¦: {self.args.use_fp16}")
            print(f"\nAccelerate åˆå§‹åŒ–å®Œæˆ")
            print(f"  è®¾å¤‡: {self.device}")
            print(f"  åˆ†å¸ƒå¼è®­ç»ƒ: {self.accelerator.distributed_type}")
            print(f"  è¿›ç¨‹æ•°: {self.accelerator.num_processes}")
            print(f"  æ··åˆç²¾åº¦: {self.accelerator.mixed_precision}")
            print(f"  è°ƒè¯•æ¨¡å¼: {'å¯ç”¨' if self.debug_mode else 'ç¦ç”¨'}")

        self.logger.training_start(self.args)

    def _gather_average_loss(self, total_loss, num_batches, default_value=0.0):
        """è·¨è¿›ç¨‹æ”¶é›†å¹¶è®¡ç®—å¹³å‡æŸå¤±"""
        self.accelerator.wait_for_everyone()
        total_loss_tensor = torch.tensor(total_loss, device=self.device)
        num_batches_tensor = torch.tensor(num_batches, device=self.device)
        gathered_losses = self.accelerator.gather(total_loss_tensor)
        gathered_batches = self.accelerator.gather(num_batches_tensor)
        total_batches = gathered_batches.sum().item()
        return gathered_losses.sum().item() / total_batches if total_batches > 0 else default_value

    def _gather_metrics_across_processes(self, num_batches, total_loss, total_grad_norm):
        """è·¨è¿›ç¨‹æ”¶é›†è®­ç»ƒæŒ‡æ ‡ï¼ˆæ‰¹æ¬¡æ•°ã€æ€»æŸå¤±ã€æ€»æ¢¯åº¦èŒƒæ•°ï¼‰

        Args:
            num_batches: å½“å‰è¿›ç¨‹çš„æ‰¹æ¬¡æ•°
            total_loss: å½“å‰è¿›ç¨‹çš„æ€»æŸå¤±
            total_grad_norm: å½“å‰è¿›ç¨‹çš„æ€»æ¢¯åº¦èŒƒæ•°

        Returns:
            tuple: (gathered_batches, gathered_total_losses, gathered_total_grad_norms)
        """
        if self.accelerator.num_processes > 1:
            gathered_batches = self.accelerator.gather(
                torch.tensor(num_batches, device=self.device)
            )
            gathered_total_losses = self.accelerator.gather(
                torch.tensor(total_loss, device=self.device)
            )
            gathered_total_grad_norms = self.accelerator.gather(
                torch.tensor(total_grad_norm, device=self.device)
            )
        else:
            gathered_batches = torch.tensor([num_batches], device=self.device)
            gathered_total_losses = torch.tensor([total_loss], device=self.device)
            gathered_total_grad_norms = torch.tensor([total_grad_norm], device=self.device)

        return gathered_batches, gathered_total_losses, gathered_total_grad_norms

    def _format_gpu_metrics_summary(self, gathered_batches, gathered_total_losses,
                                    gathered_total_grad_norms, include_lr=False, lr=None):
        """æ ¼å¼åŒ–GPUæŒ‡æ ‡ç»Ÿè®¡æ‘˜è¦

        Args:
            gathered_batches: å„è¿›ç¨‹çš„æ‰¹æ¬¡æ•°
            gathered_total_losses: å„è¿›ç¨‹çš„æ€»æŸå¤±
            gathered_total_grad_norms: å„è¿›ç¨‹çš„æ€»æ¢¯åº¦èŒƒæ•°
            include_lr: æ˜¯å¦åŒ…å«å­¦ä¹ ç‡ä¿¡æ¯
            lr: å­¦ä¹ ç‡å€¼ï¼ˆå½“include_lr=Trueæ—¶ä½¿ç”¨ï¼‰

        Returns:
            tuple: (gpu_metrics_list, global_total_batches, global_avg_loss, global_avg_grad_norm)
        """
        gpu_metrics = []
        global_total_batches = 0
        global_total_loss = 0.0
        global_total_grad_norm = 0.0

        for gpu_idx in range(self.accelerator.num_processes):
            gpu_batches = gathered_batches[gpu_idx].item()
            gpu_total_loss = gathered_total_losses[gpu_idx].item()
            gpu_total_grad_norm = gathered_total_grad_norms[gpu_idx].item()

            gpu_avg_loss = gpu_total_loss / gpu_batches if gpu_batches > 0 else 0.0
            gpu_avg_grad_norm = gpu_total_grad_norm / gpu_batches if gpu_batches > 0 else 0.0

            gpu_metrics.append(
                f"  [GPU {gpu_idx}] batches={gpu_batches} | "
                f"total_loss={gpu_total_loss:.2f} | avg_loss={gpu_avg_loss:.6f} | "
                f"avg_grad_norm={gpu_avg_grad_norm:.3f}"
            )

            global_total_batches += gpu_batches
            global_total_loss += gpu_total_loss
            global_total_grad_norm += gpu_total_grad_norm

        global_avg_loss = global_total_loss / global_total_batches if global_total_batches > 0 else 0.0
        global_avg_grad_norm = global_total_grad_norm / self.accelerator.num_processes if self.accelerator.num_processes > 0 else 0.0

        return gpu_metrics, global_total_batches, global_avg_loss, global_avg_grad_norm


    def prepare_data(self, tokenizer):
        """å‡†å¤‡è®­ç»ƒæ•°æ®ï¼Œä½¿ç”¨ Hugging Face datasets åŠ è½½"""
        cache_filename = f"data/flow_samples_{self.args.num_samples}_{self.args.max_dim}dim_{self.args.n_points}pts_{self.args.max_depth}depth_{self.args.max_expr_length}len.parquet"

        # ä¸»è¿›ç¨‹è´Ÿè´£æ•°æ®ç”Ÿæˆ
        if self.accelerator.is_local_main_process:
            print(f"å‡†å¤‡è¿ç»­æµè®­ç»ƒæ•°æ® (å•è¿›ç¨‹ç”Ÿæˆæ¨¡å¼)...")
            print(f"ä½¿ç”¨å¯¹é½æ–¹æ³•: {self.args.alignment_method}")
            generate_flow_samples(
                num_samples=self.args.num_samples,
                max_dim=self.args.max_dim,
                n_points=self.args.n_points,
                max_depth=self.args.max_depth,
                max_expr_length=self.args.max_expr_length,
                verbose=True,
                alignment_method=self.args.alignment_method,
            )
        else:
            print(f"[Rank {self.accelerator.process_index}] è·³è¿‡æ•°æ®ç”Ÿæˆï¼Œç­‰å¾…ä¸»è¿›ç¨‹å®Œæˆ...")

        self.accelerator.wait_for_everyone()

        if self.accelerator.is_local_main_process:
            print("[ä¸»è¿›ç¨‹] æ•°æ®ç”Ÿæˆå®Œæˆï¼Œå¼€å§‹åŠ è½½è®­ç»ƒæ•°æ®")

        print(f"[Rank {self.accelerator.process_index}] å‡†å¤‡å¼€å§‹è®­ç»ƒé˜¶æ®µ...")
        self.accelerator.wait_for_everyone()

        # åŠ è½½æ•°æ®
        use_stream = self.args.dataset_stream
        num_proc = self.args.dataset_num_proc

        if self.accelerator.is_local_main_process:
            print(f"ä½¿ç”¨ Hugging Face datasets åŠ è½½æ•°æ® (stream={use_stream})...")

        # åˆ†å‰²è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        train_dataset, test_dataset, train_size_estimate, test_size_estimate = self._split_train_test(
            cache_filename, tokenizer, use_stream, num_proc
        )

        # åˆ›å»ºDataLoader
        train_dataloader, test_dataloader = self._create_dataloaders(
            train_dataset, test_dataset
        )

        # å‡†å¤‡åˆ†å¸ƒå¼è®­ç»ƒ
        if self.accelerator.is_local_main_process:
            print(f"æ­£åœ¨å‡†å¤‡åˆ†å¸ƒå¼è®­ç»ƒ (accelerator.prepare)...")

        train_dataloader, test_dataloader = self.accelerator.prepare(
            train_dataloader, test_dataloader
        )

        if self.accelerator.is_local_main_process:
            is_stream_mode = getattr(train_dataset, 'stream', False)
            train_shuffle = not is_stream_mode
            num_workers = 0 if is_stream_mode else self.accelerator.num_processes
            expected_train_batches = train_size_estimate // self.args.batch_size
            expected_test_batches = test_size_estimate // self.args.batch_size

            print(f"âœ“ åˆ†å¸ƒå¼è®­ç»ƒå‡†å¤‡å®Œæˆ")
            print(f"æ•°æ®å‡†å¤‡å®Œæˆ: è®­ç»ƒé›†çº¦ {train_size_estimate} æ ·æœ¬, æµ‹è¯•é›†çº¦ {test_size_estimate} æ ·æœ¬")

            self.logger.log(
                "DATALOADER_VERIFY",
                f"DataLoaderåˆ›å»ºå®Œæˆ | é¢„æœŸè®­ç»ƒæ‰¹æ¬¡æ•°={expected_train_batches} | "
                f"é¢„æœŸæµ‹è¯•æ‰¹æ¬¡æ•°={expected_test_batches} | "
                f"num_workers={num_workers} | is_stream_mode={is_stream_mode} | "
                f"train_shuffle={train_shuffle} | "
                f"æ”¯æŒset_epoch={hasattr(train_dataset, 'set_epoch')}",
                "data_loading",
                level=1
            )

        return train_dataloader, train_dataset, test_dataloader, test_dataset

    def _split_train_test(self, cache_filename, tokenizer, use_stream, num_proc):
        """åˆ†å‰²è®­ç»ƒé›†å’Œæµ‹è¯•é›†"""
        # å½“æ ·æœ¬æ•°å¾ˆå°‘æ—¶ï¼Œè®©æ‰€æœ‰æ ·æœ¬åŒæ—¶ç”¨äºè®­ç»ƒå’Œæµ‹è¯•
        if self.args.num_samples <= self.args.batch_size:
            return self._create_full_datasets(cache_filename, tokenizer, use_stream, num_proc)

        # æ­£å¸¸åˆ†å‰²é€»è¾‘
        if use_stream:
            return self._split_stream_mode(cache_filename, tokenizer, num_proc)
        else:
            return self._split_nonstream_mode(cache_filename, tokenizer, num_proc)

    def _create_full_datasets(self, cache_filename, tokenizer, use_stream, num_proc):
        """æ ·æœ¬æ•°å¾ˆå°‘æ—¶ï¼Œåˆ›å»ºå®Œæ•´çš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†"""
        if self.accelerator.is_local_main_process:
            mode_str = "æµå¼" if use_stream else "éæµå¼"
            print(f"{mode_str}æ¨¡å¼: æ ·æœ¬æ•°({self.args.num_samples}) â‰¤ batch_size({self.args.batch_size})")
            print(f"        æ‰€æœ‰æ ·æœ¬å°†åŒæ—¶ç”¨äºè®­ç»ƒå’Œæµ‹è¯•")

        full_dataset = FlowDataset(
            data_file=cache_filename, tokenizer=tokenizer,
            max_dim=self.args.max_dim, max_expr_length=self.args.max_expr_length,
            stream=use_stream, num_proc=num_proc, logger=self.logger
        )
        return full_dataset, full_dataset, self.args.num_samples, self.args.num_samples

    def _split_stream_mode(self, cache_filename, tokenizer, num_proc):
        """æµå¼æ¨¡å¼ä¸‹çš„æ•°æ®åˆ†å‰²"""
        split_ratio = 1 - self.args.test_split
        train_size = int(self.args.num_samples * split_ratio)
        test_size = self.args.num_samples - train_size

        if self.accelerator.is_local_main_process:
            print(f"æµå¼æ¨¡å¼: è®­ç»ƒé›†çº¦ {train_size} æ ·æœ¬, æµ‹è¯•é›†çº¦ {test_size} æ ·æœ¬")

        train_dataset = FlowDataset(
            data_file=cache_filename, tokenizer=tokenizer,
            max_dim=self.args.max_dim, max_expr_length=self.args.max_expr_length,
            stream=True, num_proc=num_proc, logger=self.logger,
            skip=0, take=train_size
        )
        test_dataset = FlowDataset(
            data_file=cache_filename, tokenizer=tokenizer,
            max_dim=self.args.max_dim, max_expr_length=self.args.max_expr_length,
            stream=True, num_proc=num_proc, logger=self.logger,
            skip=train_size, take=test_size
        )
        return train_dataset, test_dataset, train_size, test_size

    def _split_nonstream_mode(self, cache_filename, tokenizer, num_proc):
        """éæµå¼æ¨¡å¼ä¸‹çš„æ•°æ®åˆ†å‰²"""
        full_dataset = FlowDataset(
            data_file=cache_filename, tokenizer=tokenizer,
            max_dim=self.args.max_dim, max_expr_length=self.args.max_expr_length,
            stream=False, num_proc=num_proc, logger=self.logger
        )

        total_size = len(full_dataset)
        train_size = int(total_size * (1 - self.args.test_split))

        from torch.utils.data import Subset
        indices = list(range(total_size))
        np.random.shuffle(indices)

        train_indices = indices[:train_size]
        test_indices = indices[train_size:]

        if self.accelerator.is_local_main_process:
            print(f"éæµå¼æ¨¡å¼: è®­ç»ƒé›† {len(train_indices)} æ ·æœ¬, æµ‹è¯•é›† {len(test_indices)} æ ·æœ¬")

        train_dataset = Subset(full_dataset, train_indices)
        test_dataset = Subset(full_dataset, test_indices)
        return train_dataset, test_dataset, len(train_indices), len(test_indices)

    def _create_dataloaders(self, train_dataset, test_dataset):
        """åˆ›å»ºè®­ç»ƒå’Œæµ‹è¯•DataLoader"""
        is_stream_mode = getattr(train_dataset, 'stream', False)
        train_size = len(train_dataset)
        test_size = len(test_dataset)
        train_drop_last = train_size >= self.args.batch_size
        test_drop_last = test_size >= self.args.batch_size

        if self.accelerator.is_local_main_process:
            if not train_drop_last:
                print(f"è­¦å‘Š: è®­ç»ƒé›†å¤§å°({train_size}) < batch_size({self.args.batch_size})ï¼Œç¦ç”¨drop_last")
            if not test_drop_last:
                print(f"è­¦å‘Š: æµ‹è¯•é›†å¤§å°({test_size}) < batch_size({self.args.batch_size})ï¼Œç¦ç”¨drop_last")

        train_shuffle = not is_stream_mode
        num_workers = 0 if is_stream_mode else self.accelerator.num_processes

        if self.accelerator.is_local_main_process:
            print(f"æ­£åœ¨åˆ›å»º DataLoader (batch_size={self.args.batch_size}, num_workers={num_workers}, shuffle={train_shuffle})...")

        train_dataloader = torch.utils.data.DataLoader(
            train_dataset, batch_size=self.args.batch_size, shuffle=train_shuffle,
            num_workers=num_workers, collate_fn=custom_collate_fn,
            drop_last=train_drop_last, pin_memory=True
        )

        test_dataloader = torch.utils.data.DataLoader(
            test_dataset, batch_size=self.args.batch_size, shuffle=False,
            num_workers=num_workers, collate_fn=custom_collate_fn, drop_last=test_drop_last
        )

        if self.accelerator.is_local_main_process:
            print(f"âœ“ DataLoader åˆ›å»ºå®Œæˆ")

        return train_dataloader, test_dataloader

    def setup_models(self, checkpoint_path=None):
        """åˆå§‹åŒ–æ¨¡å‹å’Œtokenizerï¼Œæ”¯æŒä»æ£€æŸ¥ç‚¹åŠ è½½"""
        if self.accelerator.is_local_main_process:
            print("åˆå§‹åŒ–tokenizerå’Œæ¨¡å‹...")

        # åˆå§‹åŒ–tokenizer
        from ..utils.special_tokens import SymbolicRegressionTokenizer, SymbolicVocab
        tokenizer = SymbolicRegressionTokenizer(max_dim=self.args.max_dim)

        if self.accelerator.is_local_main_process:
            print(f"âœ“ ç¬¦å·å›å½’Tokenizeråˆå§‹åŒ–å®Œæˆ")
            print(f"  è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size} (ç¬¦å·å›å½’ä¸“å±å°è¯æ±‡è¡¨)")
            print(f"  æœ€å¤§ç»´åº¦: {self.args.max_dim}")
            print(f"  è¿ç®—ç¬¦: {len(SymbolicVocab.OPERATORS)}ä¸ª - {', '.join(SymbolicVocab.OPERATORS)}")
            print(f"  å‡½æ•°: {len(SymbolicVocab.FUNCTIONS)}ä¸ª - {', '.join(SymbolicVocab.FUNCTIONS)}")
            print(f"  ç‰¹æ®Štoken: {len(SymbolicVocab.SPECIAL_TOKENS)}ä¸ª")
            print(f"  å˜é‡token: x0 ~ x{self.args.max_dim-1} (å…±{self.args.max_dim}ä¸ª)")

        # åˆå§‹åŒ–æ¡ä»¶ç¼–ç å™¨
        if self.accelerator.is_local_main_process:
            print("åˆå§‹åŒ–æ¡ä»¶ç¼–ç å™¨...")
        condition_encoder = SetTransformerConditionEncoder(
            max_input_dim=self.args.condition_max_input_dim,
            dim_hidden=self.args.condition_dim_hidden,
            num_heads=self.args.condition_num_heads,
            num_inds=self.args.condition_num_inds,
            num_layers=self.args.condition_num_layers,
            num_seeds=self.args.condition_num_seeds,
            dim_output=self.args.condition_dim_output,
            verbose=self.accelerator.is_local_main_process
        ).to(self.device)

        # åˆå§‹åŒ–LLaMA EditFlowæ¨¡å‹
        if self.accelerator.is_local_main_process:
            print("åˆå§‹åŒ–LLaMA EditFlowæ¨¡å‹ï¼ˆè‡ªå®šä¹‰æ¶æ„ï¼Œä¸åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼‰...")

        model = LlamaEditFlowBackbone(
            vocab_size=len(tokenizer.get_vocab()),
            hidden_dim=self.args.hidden_dim,
            n_layers=self.args.n_layers,
            n_heads=self.args.n_heads,
            condition_dim=self.args.condition_dim_hidden,
            dropout=self.args.dropout,
            max_seq_len=self.args.max_expr_length,
            use_condition_injection=self.args.use_condition_injection,
            verbose=self.accelerator.is_local_main_process
        ).to(self.device)

        # åˆ›å»ºä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        criterion = ContinuousFlowLoss(debug_mode=self.debug_mode)
        optimizer = torch.optim.AdamW(
            list(model.parameters()) + list(condition_encoder.parameters()),
            lr=self.args.learning_rate,
            weight_decay=self.args.weight_decay,
            eps=1e-8,
            betas=(0.9, 0.999)
        )

        # æ·»åŠ å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆä½™å¼¦é€€ç«ï¼‰
        from torch.optim.lr_scheduler import CosineAnnealingLR
        scheduler = CosineAnnealingLR(optimizer, T_max=self.args.num_epochs, eta_min=1e-6)

        # åŠ è½½æ£€æŸ¥ç‚¹
        load_checkpoint(checkpoint_path, model, condition_encoder, self.device, optimizer, verbose=self.accelerator.is_local_main_process)

        # ä½¿ç”¨ Accelerate å‡†å¤‡æ¨¡å‹å’Œä¼˜åŒ–å™¨
        if self.accelerator.is_local_main_process:
            print(f"ä½¿ç”¨ Accelerate å‡†å¤‡æ¨¡å‹å’Œä¼˜åŒ–å™¨...")
            print(f"  è¿›ç¨‹æ•°: {self.accelerator.num_processes}")
            print(f"  è®¾å¤‡: {self.accelerator.device}")
            print(f"  æ··åˆç²¾åº¦: {self.accelerator.mixed_precision}")

        model, condition_encoder, optimizer = self.accelerator.prepare(model, condition_encoder, optimizer)

        # å¦‚æœæœ‰checkpointï¼Œä½¿ç”¨Accelerateçš„load_stateæ–¹æ³•åŠ è½½å®Œæ•´çŠ¶æ€
        if checkpoint_path:
            if self.accelerator.is_local_main_process:
                print(f"Loading complete training state from {checkpoint_path}")
            self.accelerator.load_state(checkpoint_path)

        total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        if self.accelerator.is_local_main_process:
            print(f"âœ“ LLaMA EditFlowæ¨¡å‹å‚æ•°æ•°é‡: {total_params:,}")

        self.tokenizer = tokenizer

        return model, condition_encoder, criterion, optimizer, scheduler, tokenizer

  
    def forward_pass(self, model, condition_embeddings, z0_token_ids, z1_token_ids, debug_info=None):
        """å‰å‘ä¼ æ’­ï¼šç›´æ¥é¢„æµ‹ä»z0åˆ°z1çš„ç¼–è¾‘æ“ä½œ"""
        batch_size = z0_token_ids.size(0)
        vocab_size = self.tokenizer.vocab_size
        batch_size, seq_len = z0_token_ids.shape

        # ç§»é™¤gap tokenå¾—åˆ°è¾“å…¥åºåˆ—x_tï¼ˆåŸå§‹åºåˆ—ç©ºé—´ï¼Œæ— gapé‡å¤ï¼‰
        x_t, x_pad_mask, z_gap_mask, z_pad_mask = remove_gap_tokens(z0_token_ids, self.tokenizer)
        attention_mask = (~x_pad_mask).float()

        # è°ƒç”¨æ¨¡å‹
        output = model(input_ids=x_t, condition=condition_embeddings, attention_mask=attention_mask)
        pred_rates = output['rates_logits']

        # è®°å½•debugä¿¡æ¯ï¼ˆä½¿ç”¨Loggerçš„é€šç”¨å‡½æ•°ï¼‰
        if debug_info:
            debug_info['sample_idx'] = 0
            self.logger.log_forward_debug(
                debug_info,
                self.tokenizer,
                z0_token_ids=z0_token_ids,
                z1_token_ids=z1_token_ids,
                condition_embeddings=condition_embeddings,
                x_t=x_t,
                attention_mask=attention_mask,
                pred_rates=pred_rates,
                insert_logits=output['insert_logits'],
                substitute_logits=output['substitute_logits']
            )

        return {
            'pred_rates': pred_rates,
            'pred_ins_logits': output['insert_logits'],
            'pred_sub_logits': output['substitute_logits'],
            'x_t': x_t,
            'z0': z0_token_ids,
            'z1_token_ids': z1_token_ids,
            'z_gap_mask': z_gap_mask,
            'z_pad_mask': z_pad_mask,
            'attention_mask': attention_mask,
            'vocab_size': vocab_size,
        }

    def compute_loss(self, forward_results, criterion, debug_info=None):
        pred_rates = forward_results['pred_rates']
        x_t = forward_results['x_t']
        z0 = forward_results['z0']
        z1_token_ids = forward_results['z1_token_ids']
        z_gap_mask = forward_results['z_gap_mask']
        z_pad_mask = forward_results['z_pad_mask']
        effective_vocab_size = forward_results['vocab_size']
        gap_token = self.tokenizer.convert_tokens_to_ids('<gap>')

        # æ‹†åˆ†æ“ä½œlogitsï¼šins, del, sub, keep
        ins_logits_rate = pred_rates[:, :, 0:1]
        del_logits_rate = pred_rates[:, :, 1:2]
        sub_logits_rate = pred_rates[:, :, 2:3]
        keep_logits_rate = pred_rates[:, :, 3:4]

        # åœ¨logitç©ºé—´ç›¸åŠ 
        ins_logits = forward_results['pred_ins_logits'] + ins_logits_rate
        sub_logits = forward_results['pred_sub_logits'] + sub_logits_rate
        del_logits = del_logits_rate
        keep_logits = keep_logits_rate

        # æ‹¼æ¥æ‰€æœ‰æ“ä½œlogitsï¼šins | del | sub | keep
        u_cat_x = torch.cat([ins_logits, del_logits, sub_logits, keep_logits], dim=-1)

        # å°†Xç©ºé—´çš„é¢„æµ‹æ‰©å±•åˆ°Zç©ºé—´
        u_z = fill_gap_tokens_with_repeats(u_cat_x, z_gap_mask, z_pad_mask)

        # ç”Ÿæˆç¼–è¾‘æ“ä½œæ©ç 
        u_mask_x = criterion.make_ut_mask_from_z(z0, z1_token_ids, effective_vocab_size, gap_token, self.tokenizer, x_t)
        u_mask = fill_gap_tokens_with_repeats(u_mask_x, z_gap_mask, z_pad_mask)

        # è®°å½•debugä¿¡æ¯ï¼ˆä½¿ç”¨Loggerçš„æ–¹æ³•ï¼‰
        if self.accelerator.is_local_main_process and self.debug_mode:
            self.logger.log_compute_loss_debug(debug_info, z0, z1_token_ids, x_t, pred_rates, u_cat_x, u_mask_x, effective_vocab_size, self.tokenizer)

        loss = criterion(u_cat_x, u_z, u_mask, effective_vocab_size,
                        accelerator=self.accelerator, logger=self.logger)

        return loss

    def train_epoch(self, model, condition_encoder, criterion, optimizer, dataloader, dataset, epoch, dimension):
        model.train()
        condition_encoder.train()

        # å…³é”®ä¿®å¤ï¼šå¯¹äºæµå¼æ•°æ®é›†ï¼Œåœ¨æ¯ä¸ª epoch å¼€å§‹æ—¶è°ƒç”¨ set_epoch
        if hasattr(dataset, 'set_epoch'):
            dataset.set_epoch(epoch)
            if self.accelerator.is_local_main_process:
                self.logger.log(
                    "DATASET_SET_EPOCH",
                    f"è°ƒç”¨ dataset.set_epoch({epoch}) | è¿­ä»£å™¨å·²é‡ç½®ï¼Œæ•°æ®å·²é‡æ–°æ´—ç‰Œ",
                    f"epoch{epoch+1}_data_reset",
                    level=1
                )

        total_loss = 0.0
        num_batches = 0
        local_total_grad_norm = 0.0

        # è®¡ç®—æ•°æ®é›†ä¿¡æ¯
        dataset_size = len(dataset)
        num_batches_estimate = dataset_size // self.args.batch_size

        # è®°å½•epochå¼€å§‹å’Œæ•°æ®é›†ä¿¡æ¯
        if self.accelerator.is_local_main_process:
            self.logger.log(
                "EPOCH_START",
                f"å¼€å§‹ Epoch {epoch+1}/{self.args.num_epochs} | ç»´åº¦={dimension} | "
                f"æ•°æ®é›†å¤§å°={dataset_size} | é¢„è®¡æ‰¹æ¬¡æ•°={num_batches_estimate} | "
                f"æ‰¹æ¬¡å¤§å°={self.args.batch_size}",
                f"epoch{epoch+1}_dim{dimension}",
                level=1
            )

        progress_bar = tqdm(dataloader, desc=f"Epoch {epoch+1}/{self.args.num_epochs} - Dim {dimension}",
                          disable=not self.accelerator.is_local_main_process)

        if self.accelerator.is_local_main_process:
            progress_bar.set_postfix({'loss': '0.0000', 'grad_norm': '0.000'})

        for batch_idx, batch in enumerate(progress_bar):
            batch_start_time = time.time()

            # è®°å½•æ•°æ®åŠ è½½è¿›åº¦ï¼ˆä¸å—debugæ§åˆ¶ï¼Œå§‹ç»ˆè®°å½•ï¼‰
            if self.accelerator.is_local_main_process:
                progress_pct = (batch_idx + 1) / num_batches_estimate * 100 if num_batches_estimate > 0 else 0
                self.logger.log(
                    "BATCH_LOAD_START",
                    f"å¼€å§‹åŠ è½½ Batch {batch_idx+1}/{num_batches_estimate} | "
                    f"è¿›åº¦={progress_pct:.1f}% | timestamp={time.time():.2f}",
                    f"epoch{epoch+1}_dim{dimension}_batch{batch_idx}",
                    level=1
                )

            loss, grad_norm = self._process_batch(
                model, condition_encoder, criterion, optimizer, batch, batch_idx, epoch, dimension
            )

            total_loss += loss
            num_batches += 1
            local_total_grad_norm += grad_norm

            batch_total_time = time.time() - batch_start_time

            # æ›´æ–°è¿›åº¦æ¡
            if self.accelerator.is_local_main_process:
                postfix_dict = {
                    'loss': f'{loss:.4f}',
                    'grad_norm': f'{grad_norm:.3f}',
                    'time': f'{batch_total_time:.2f}s' if self.debug_mode else ''
                }
                progress_bar.set_postfix(postfix_dict)

            if self.accelerator.is_local_main_process and self.debug_mode:
                self.logger.log("BATCH_COMPLETE", f"Batch {batch_idx} å®Œæˆ | æ€»è€—æ—¶={batch_total_time:.3f}s | timestamp={time.time():.2f}",
                                f"ç»´åº¦{dimension}_batch{batch_idx}", level=2)

        # è·¨è¿›ç¨‹æ”¶é›†å¹¶è®¡ç®—å¹³å‡æŸå¤±
        avg_loss = self._gather_average_loss(total_loss, num_batches, default_value=0.0)

        # åœ¨æ‰€æœ‰è¿›ç¨‹ä¸Šæ”¶é›†è®­ç»ƒæŒ‡æ ‡
        num_processes = self.accelerator.num_processes
        gathered_batches, gathered_total_losses, gathered_total_grad_norms = \
            self._gather_metrics_across_processes(num_batches, total_loss, local_total_grad_norm)

        # æ•°æ®æ¶ˆè€—ç›‘æ§ï¼šè®°å½•å®é™…å¤„ç†çš„ batch æ•°ï¼ˆåªåœ¨ä¸»è¿›ç¨‹ï¼‰
        if self.accelerator.is_local_main_process:
            expected_batches = dataset_size // self.args.batch_size
            actual_batches = num_batches
            total_batches_all_processes = gathered_batches.sum().item()

            # è®¡ç®—æ ·æœ¬è¦†ç›–ç‡
            total_samples_processed = total_batches_all_processes * self.args.batch_size
            coverage_rate = (total_samples_processed / dataset_size * 100) if dataset_size > 0 else 0.0

            # æ ¹æ®æ˜¯å¦åˆ†å¸ƒå¼è®­ç»ƒï¼Œæ˜¾ç¤ºä¸åŒçš„æ—¥å¿—æ ¼å¼
            if num_processes > 1:
                # ä½¿ç”¨è¾…åŠ©æ–¹æ³•æ ¼å¼åŒ–GPUæŒ‡æ ‡ï¼ˆè¿”å›çš„total_batches_all_processesåº”è¯¥ä¸ä¹‹å‰è®¡ç®—çš„ä¸€è‡´ï¼‰
                gpu_metrics, _, global_avg_loss, global_avg_grad_norm = \
                    self._format_gpu_metrics_summary(gathered_batches, gathered_total_losses, gathered_total_grad_norms)

                # æ„å»ºå®Œæ•´çš„æ—¥å¿—æ¶ˆæ¯
                gpu_metrics_summary = "\n" + "\n".join(gpu_metrics)
                data_allocation_summary = (
                    f"\n--- æ•°æ®åˆ†é… --- | è¿›ç¨‹æ•°={num_processes} | æ•°æ®é›†å¤§å°={dataset_size} | "
                    f"æ‰¹æ¬¡å¤§å°={self.args.batch_size} | é¢„æœŸå•è¿›ç¨‹æ‰¹æ¬¡æ•°={expected_batches} | "
                    f"è¦†ç›–ç‡={coverage_rate:.1f}%"
                )
                global_summary = (
                    f"\n--- å…¨å±€æ±‡æ€» --- | æ€»æ‰¹æ¬¡æ•°={total_batches_all_processes} | "
                    f"avg_loss={global_avg_loss:.6f} | avg_grad_norm={global_avg_grad_norm:.3f}"
                )

                self.logger.log(
                    "EPOCH_BATCH_COUNT",
                    f"Epoch {epoch+1} å®Œæˆ [åˆ†å¸ƒå¼è®­ç»ƒè¯¦ç»†] |" +
                    gpu_metrics_summary +
                    data_allocation_summary +
                    global_summary,
                    f"epoch{epoch+1}_dim{dimension}_detailed",
                    level=1
                )
            else:
                # å•GPUè®­ç»ƒ
                avg_grad_norm = local_total_grad_norm / num_batches if num_batches > 0 else 0.0
                self.logger.log(
                    "EPOCH_BATCH_COUNT",
                    f"Epoch {epoch+1} å®Œæˆ | é¢„æœŸæ‰¹æ¬¡æ•°={expected_batches} | "
                    f"å®é™…æ‰¹æ¬¡æ•°={actual_batches} | æ€»æŸå¤±={total_loss:.2f} | "
                    f"å¹³å‡æŸå¤±={avg_loss:.6f} | å¹³å‡æ¢¯åº¦èŒƒæ•°={avg_grad_norm:.3f} | "
                    f"æ•°æ®é›†å¤§å°={dataset_size} | æ‰¹æ¬¡å¤§å°={self.args.batch_size}",
                    f"epoch{epoch+1}_dim{dimension}_monitor",
                    level=1
                )

            # è­¦å‘Šï¼šå¦‚æœå®é™…æ‰¹æ¬¡æ•°è¿œå°‘äºé¢„æœŸï¼Œå¯èƒ½æ˜¯æ•°æ®åŠ è½½é—®é¢˜
            if epoch > 0 and actual_batches == 0:
                self.logger.error(
                    "NO_DATA_LOADED",
                    f"ä¸¥é‡é”™è¯¯ï¼šEpoch {epoch+1} æ²¡æœ‰å¤„ç†ä»»ä½• batchï¼"
                    f"è¿™é€šå¸¸æ„å‘³ç€ IterableDataset è¿­ä»£å™¨å·²è€—å°½ä¸”æœªæ­£ç¡®é‡ç½®ã€‚",
                    f"epoch{epoch+1}_critical"
                )
            elif epoch > 0 and actual_batches < expected_batches * 0.5:
                self.logger.error(
                    "INSUFFICIENT_DATA",
                    f"è­¦å‘Šï¼šEpoch {epoch+1} å®é™…æ‰¹æ¬¡æ•°({actual_batches}) "
                    f"è¿œå°‘äºé¢„æœŸ({expected_batches})ï¼Œå¯èƒ½å­˜åœ¨æ•°æ®åŠ è½½é—®é¢˜ã€‚",
                    f"epoch{epoch+1}_warning"
                )

        # è¿”å›å¹³å‡æŸå¤±ã€æ‰¹æ¬¡æ•°ã€æ€»æŸå¤±å’Œæ€»æ¢¯åº¦èŒƒæ•°ï¼ˆç”¨äºGPUçº§åˆ«ä¿¡æ¯æ±‡æ€»ï¼‰
        return avg_loss, num_batches, total_loss, local_total_grad_norm

    def _process_batch(self, model, condition_encoder, criterion, optimizer, batch, batch_idx, epoch, dimension):
        """å¤„ç†å•ä¸ªè®­ç»ƒbatch"""
        if self.accelerator.is_local_main_process and self.debug_mode:
            self.logger.log("BATCH_START", f"å¼€å§‹å¤„ç† Batch {batch_idx} | timestamp={time.time():.2f}",
                            f"ç»´åº¦{dimension}_batch{batch_idx}", level=2)

        with self.accelerator.accumulate([model, condition_encoder]):
            data_load_start = time.time()
            x_values = batch['x_values'].to(self.device)
            y_target = batch['y_target'].to(self.device)
            z0_token_ids = batch['z0_token_ids'].to(self.device)
            z1_token_ids = batch['z1_token_ids'].to(self.device)
            point_mask = batch['point_mask'].to(self.device) if 'point_mask' in batch else None

            if self.accelerator.is_local_main_process and self.debug_mode:
                self.logger.log("DATA_LOAD", f"æ•°æ®åŠ è½½å®Œæˆ | è€—æ—¶={time.time() - data_load_start:.3f}s",
                                f"ç»´åº¦{dimension}_batch{batch_idx}", level=2)

            # ç¼–ç æ¡ä»¶
            condition_start = time.time()
            condition_embeddings = condition_encoder(x_values, y_target, point_mask)
            condition_time = time.time() - condition_start

            if self.accelerator.is_local_main_process and self.debug_mode:
                self.logger.log("CONDITION_ENCODE", f"æ¡ä»¶ç¼–ç å®Œæˆ | è€—æ—¶={condition_time:.3f}s",
                                f"ç»´åº¦{dimension}_batch{batch_idx}", level=2)
                context = f'ç»´åº¦{dimension}'
                self.logger.tensor_values(f"x_values_batch{batch_idx}", x_values[0],
                                         context=context, level=2, max_elements=50)
                self.logger.tensor_values(f"y_target_batch{batch_idx}", y_target[0],
                                         context=context, level=2, max_elements=50)

            debug_info = {'batch_idx': batch_idx, 'context': f'ç»´åº¦{dimension}'}

            # å‰å‘ä¼ æ’­
            forward_start = time.time()
            forward_results = self.forward_pass(model, condition_embeddings, z0_token_ids, z1_token_ids, debug_info)
            forward_time = time.time() - forward_start

            if self.accelerator.is_local_main_process and self.debug_mode:
                self.logger.log("FORWARD_PASS", f"å‰å‘ä¼ æ’­å®Œæˆ | è€—æ—¶={forward_time:.3f}s",
                                f"ç»´åº¦{dimension}_batch{batch_idx}", level=2)

            # NaNæ£€æŸ¥
            nan_check_start = time.time()
            if self.accelerator.distributed_type != "NO":
                pred_rates = forward_results['pred_rates']
                local_has_nan = torch.isnan(pred_rates).any().float()
                gathered_nan_results = self.accelerator.gather(local_has_nan)
                global_has_nan = gathered_nan_results.sum()

                if global_has_nan.item() > 0:
                    self.logger.error("FORWARD_NAN", f"ç»´åº¦{dimension} æ£€æµ‹åˆ°å‰å‘ä¼ æ’­NaN", f"batch_idx:{batch_idx}")

            nan_check_time = time.time() - nan_check_start

            # è®¡ç®—æŸå¤±
            loss_compute_start = time.time()
            loss = self.compute_loss(forward_results, criterion, debug_info)
            loss_compute_time = time.time() - loss_compute_start

            if self.accelerator.is_local_main_process and self.debug_mode:
                self.logger.log("LOSS_COMPUTED", f"loss={loss.item():.6f} | è€—æ—¶={loss_compute_time:.3f}s | NaNæ£€æŸ¥è€—æ—¶={nan_check_time:.3f}s",
                                f"ç»´åº¦{dimension}_batch{batch_idx}", level=2)

            # åå‘ä¼ æ’­
            if self.accelerator.is_local_main_process and self.debug_mode:
                self.logger.log("BACKWARD_START", f"å¼€å§‹åå‘ä¼ æ’­ | loss={loss.item():.6f} | timestamp={time.time():.2f}",
                                f"ç»´åº¦{dimension}_batch{batch_idx}", level=2)

            backward_start = time.time()
            try:
                self.accelerator.backward(loss)
                backward_time = time.time() - backward_start
                if self.accelerator.is_local_main_process and self.debug_mode:
                    self.logger.log("BACKWARD_SUCCESS", f"åå‘ä¼ æ’­æˆåŠŸ | è€—æ—¶={backward_time:.3f}s",
                                    f"ç»´åº¦{dimension}_batch{batch_idx}", level=2)
            except Exception as e:
                self.logger.log_crash(
                    step_name="BACKWARD", batch_idx=batch_idx, dimension=dimension,
                    error=e, extra_info=f"loss={loss.item():.6f}"
                )
                raise

            # æ¢¯åº¦è£å‰ªå’Œä¼˜åŒ–å™¨æ›´æ–°
            all_params = list(model.parameters()) + list(condition_encoder.parameters())
            self.accelerator.clip_grad_norm_(all_params, self.GRADIENT_CLIP_NORM)

            grad_norm = 0.0
            for param in all_params:
                if param.grad is not None:
                    grad_norm += float(param.grad.data.norm().item() ** 2)
            grad_norm = float(grad_norm ** 0.5)

            optimizer.step()
            optimizer.zero_grad()

            return loss.item(), grad_norm

    def evaluate(self, model, condition_encoder, criterion, test_dataloader, test_dataset):
        """æµ‹è¯•é›†è¯„ä¼°"""
        model.eval()
        condition_encoder.eval()

        # å…³é”®ä¿®å¤ï¼šå¯¹äºæµå¼æµ‹è¯•é›†ï¼Œä¹Ÿè°ƒç”¨ set_epoch é‡ç½®è¿­ä»£å™¨
        if hasattr(test_dataset, 'set_epoch'):
            test_dataset.set_epoch(0)  # æµ‹è¯•é›†ä½¿ç”¨å›ºå®š epoch
            if self.accelerator.is_local_main_process:
                self.logger.log(
                    "TEST_DATASET_RESET",
                    "æµ‹è¯•é›†è¿­ä»£å™¨å·²é‡ç½®",
                    "evaluation",
                    level=1
                )

        total_loss = 0.0
        num_batches = 0

        # è®¡ç®—æµ‹è¯•é›†ä¿¡æ¯
        test_size = len(test_dataset)
        test_num_batches_estimate = test_size // self.args.batch_size

        # è®°å½•æµ‹è¯•å¼€å§‹
        if self.accelerator.is_local_main_process:
            self.logger.log(
                "EVAL_START",
                f"å¼€å§‹æµ‹è¯•é›†è¯„ä¼° | æµ‹è¯•é›†å¤§å°={test_size} | "
                f"é¢„è®¡æ‰¹æ¬¡æ•°={test_num_batches_estimate} | æ‰¹æ¬¡å¤§å°={self.args.batch_size}",
                "evaluation",
                level=1
            )

        with torch.no_grad():
            for batch_idx, batch in enumerate(test_dataloader):
                # è®°å½•æµ‹è¯•æ•°æ®åŠ è½½è¿›åº¦
                if self.accelerator.is_local_main_process:
                    progress_pct = (batch_idx + 1) / test_num_batches_estimate * 100 if test_num_batches_estimate > 0 else 0
                    self.logger.log(
                        "TEST_BATCH_LOAD",
                        f"æµ‹è¯• Batch {batch_idx+1}/{test_num_batches_estimate} | "
                        f"è¿›åº¦={progress_pct:.1f}% | timestamp={time.time():.2f}",
                        f"evaluation_batch{batch_idx}",
                        level=1
                    )
                x_values = batch['x_values'].to(self.device)
                y_target = batch['y_target'].to(self.device)
                z0_token_ids = batch['z0_token_ids'].to(self.device)
                z1_token_ids = batch['z1_token_ids'].to(self.device)
                point_mask = batch['point_mask'].to(self.device)

                condition_embeddings = condition_encoder(x_values, y_target, point_mask)
                forward_results = self.forward_pass(model, condition_embeddings, z0_token_ids, z1_token_ids)
                loss = self.compute_loss(forward_results, criterion)

                total_loss += loss.item()
                num_batches += 1

        avg_loss = self._gather_average_loss(total_loss, num_batches, default_value=float('inf'))

        return avg_loss


    def save_checkpoint(self, model, condition_encoder, loss, epoch, is_final=False):
        self.accelerator.wait_for_everyone()

        checkpoint_dir = os.path.join(
            self.args.save_dir,
            "continuous_flow_final" if is_final else f"checkpoint_epoch_{epoch+1}"
        )
        os.makedirs(checkpoint_dir, exist_ok=True)

        # ä½¿ç”¨ Accelerate çš„ save_state æ–¹æ³•
        self.accelerator.save_state(checkpoint_dir)

        # ä¿å­˜æ¨¡å‹é…ç½®ä¿¡æ¯
        if self.accelerator.is_local_main_process:
            unwrapped_model = self.accelerator.unwrap_model(model)
            unwrapped_encoder = self.accelerator.unwrap_model(condition_encoder)

            model_config = {
                'vocab_size': unwrapped_model.vocab_size,
                'hidden_dim': unwrapped_model.hidden_dim,
                'n_layers': unwrapped_model.n_layers,
                'n_heads': unwrapped_model.n_heads,
                'condition_dim': unwrapped_model.condition_dim,
                'dropout': unwrapped_model.dropout,
                'max_seq_len': unwrapped_model.max_seq_len,
                'use_condition_injection': unwrapped_model.use_condition_injection,
            }

            config_data = {
                'epoch': epoch + 1,
                'model_state_dict': unwrapped_model.state_dict(),
                'condition_encoder_state_dict': unwrapped_encoder.state_dict(),
                'loss': loss,
                'model_config': model_config,
                'args': self.args,
                'accelerate_config': {
                    'distributed_type': str(self.accelerator.distributed_type),
                    'num_processes': self.accelerator.num_processes,
                    'mixed_precision': str(self.accelerator.mixed_precision),
                }
            }

            config_path = os.path.join(checkpoint_dir, "training_config.json")
            torch.save(config_data, config_path)

        return checkpoint_dir

    def train(self):
        checkpoint_path = find_latest_checkpoint(self.args)
        if self.accelerator.is_local_main_process:
            print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
            print(f"{'æ‰¾åˆ°æ£€æŸ¥ç‚¹' if checkpoint_path else 'æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹ï¼Œå°†ä»åŸºç¡€æ¨¡å‹å¼€å§‹è®­ç»ƒ'}: {checkpoint_path or ''}")

        model, condition_encoder, criterion, optimizer, scheduler, tokenizer = self.setup_models(checkpoint_path=checkpoint_path)
        train_dataloader, train_dataset, test_dataloader, test_dataset = self.prepare_data(tokenizer)

        model_params = sum(p.numel() for p in model.parameters())
        encoder_params = sum(p.numel() for p in condition_encoder.parameters() if p.requires_grad)
        if self.accelerator.is_local_main_process:
            print(f"æ¨¡å‹å‚æ•°æ•°é‡: {model_params:,}, æ¡ä»¶ç¼–ç å™¨å‚æ•°æ•°é‡: {encoder_params:,}")
            print(f"å¼€å§‹è¿ç»­æµè®­ç»ƒ ({self.args.num_epochs} epochs)...")
            self.logger.log("TRAINING_START", f"å¼€å§‹è®­ç»ƒ | num_epochs={self.args.num_epochs} | model_params={model_params:,} | encoder_params={encoder_params:,}", level=1)

            # åˆ†å¸ƒå¼è®­ç»ƒè¯´æ˜
            if self.accelerator.num_processes > 1:
                train_dataset_size = len(train_dataset)
                test_dataset_size = len(test_dataset)
                samples_per_process = train_dataset_size // self.accelerator.num_processes
                batches_per_process = samples_per_process // self.args.batch_size
                total_batches_all_processes = batches_per_process * self.accelerator.num_processes
                coverage_rate = (total_batches_all_processes * self.args.batch_size / train_dataset_size * 100) if train_dataset_size > 0 else 0.0

                print("\n" + "="*70)
                print("ğŸ“Š åˆ†å¸ƒå¼è®­ç»ƒé…ç½®è¯´æ˜")
                print("="*70)
                print(f"è¿›ç¨‹æ•° (GPUæ•°):        {self.accelerator.num_processes}")
                print(f"è®­ç»ƒé›†æ€»æ ·æœ¬æ•°:        {train_dataset_size}")
                print(f"æ¯ä¸ªè¿›ç¨‹åˆ†é…æ ·æœ¬æ•°:    {samples_per_process} (æ•´æ•°é™¤æ³•)")
                print(f"æ¯ä¸ªè¿›ç¨‹é¢„æœŸæ‰¹æ¬¡æ•°:    {batches_per_process}")
                print(f"æ‰€æœ‰è¿›ç¨‹æ€»æ‰¹æ¬¡æ•°:      {total_batches_all_processes}")
                print(f"æ ·æœ¬è¦†ç›–ç‡:            {coverage_rate:.1f}%")
                print(f"\næ³¨æ„ï¼šç”±äºæ•´æ•°é™¤æ³•ï¼Œçº¦ {train_dataset_size % self.accelerator.num_processes} ä¸ªæ ·æœ¬")
                print(f"      ({train_dataset_size - total_batches_all_processes * self.args.batch_size} ä¸ª) ä¸ä¼šè¢«è®­ç»ƒ")
                print("="*70 + "\n")

                self.logger.log(
                    "DISTRIBUTED_TRAINING_INFO",
                    f"åˆ†å¸ƒå¼è®­ç»ƒé…ç½® | è¿›ç¨‹æ•°={self.accelerator.num_processes} | "
                    f"è®­ç»ƒé›†å¤§å°={train_dataset_size} | æ¯è¿›ç¨‹æ ·æœ¬æ•°={samples_per_process} | "
                    f"æ¯è¿›ç¨‹æ‰¹æ¬¡æ•°={batches_per_process} | æ€»æ‰¹æ¬¡æ•°={total_batches_all_processes} | "
                    f"è¦†ç›–ç‡={coverage_rate:.1f}%",
                    "distributed_setup",
                    level=1
                )

        eval_every = self.args.eval_every

        for epoch in range(self.args.num_epochs):
            avg_loss, num_batches, total_loss, total_grad_norm = self.train_epoch(
                model, condition_encoder, criterion, optimizer,
                train_dataloader, train_dataset, epoch, "Mixed"
            )

            # åœ¨æ‰€æœ‰è¿›ç¨‹ä¸Šæ”¶é›†è®­ç»ƒæŒ‡æ ‡
            gathered_batches, gathered_total_losses, gathered_total_grad_norms = \
                self._gather_metrics_across_processes(num_batches, total_loss, total_grad_norm)

            # åªåœ¨ä¸»è¿›ç¨‹ä¸Šæ‰“å°å’Œè®°å½•æ—¥å¿—
            if self.accelerator.is_local_main_process:
                current_lr = optimizer.param_groups[0]['lr']

                if self.accelerator.num_processes > 1:
                    # ä½¿ç”¨è¾…åŠ©æ–¹æ³•æ ¼å¼åŒ–GPUæŒ‡æ ‡
                    gpu_details, global_total_batches, global_avg_loss, _ = \
                        self._format_gpu_metrics_summary(gathered_batches, gathered_total_losses, gathered_total_grad_norms)

                    # æ„å»ºå®Œæ•´çš„æ—¥å¿—æ¶ˆæ¯
                    gpu_summary = "\n" + "\n".join(gpu_details) + "\n--- å…¨å±€æ±‡æ€» --- | " + \
                                 f"total_batches={global_total_batches} | avg_train_loss={global_avg_loss:.6f} | " + \
                                 f"lr={current_lr:.2e}"

                    # æ§åˆ¶å°è¾“å‡ºï¼ˆç®€åŒ–ç‰ˆï¼‰
                    print(f"Epoch {epoch+1}/{self.args.num_epochs} å®Œæˆ | avg_train_loss={global_avg_loss:.4f} | total_batches={global_total_batches} | lr={current_lr:.2e}")

                    # æ—¥å¿—æ–‡ä»¶è®°å½•ï¼ˆåŒ…å«è¯¦ç»†çš„GPUä¿¡æ¯ï¼‰
                    self.logger.log(
                        "EPOCH_COMPLETE",
                        f"Epoch {epoch+1}/{self.args.num_epochs} [åˆ†å¸ƒå¼è®­ç»ƒè¯¦ç»†] |\n" + gpu_summary,
                        f"epoch{epoch+1}_complete",
                        level=1
                    )
                else:
                    # å•GPUè®­ç»ƒ
                    avg_grad_norm = total_grad_norm / num_batches if num_batches > 0 else 0.0
                    print(f"Epoch {epoch+1}/{self.args.num_epochs} å®Œæˆ, è®­ç»ƒæŸå¤±: {avg_loss:.4f}, æ¢¯åº¦èŒƒæ•°: {avg_grad_norm:.3f}, å­¦ä¹ ç‡: {current_lr:.2e}")
                    self.logger.log(
                        "EPOCH_COMPLETE",
                        f"Epoch {epoch+1}/{self.args.num_epochs} | train_loss={avg_loss:.4f} | "
                        f"avg_grad_norm={avg_grad_norm:.3f} | lr={current_lr:.2e} | batches={num_batches}",
                        level=1
                    )

            scheduler.step()

            if (epoch + 1) % eval_every == 0 or epoch == self.args.num_epochs - 1:
                test_loss = self.evaluate(model, condition_encoder, criterion, test_dataloader, test_dataset)
                if self.accelerator.is_local_main_process:
                    print(f"æµ‹è¯•é›†æŸå¤±: {test_loss:.4f}")
                    self.logger.log("EVALUATION", f"Epoch {epoch+1}/{self.args.num_epochs} | test_loss={test_loss:.4f}", level=1)

            if (epoch + 1) % self.args.save_every == 0:
                checkpoint_path = self.save_checkpoint(
                    model, condition_encoder, avg_loss, epoch
                )
                if self.accelerator.is_local_main_process:
                    print(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜åˆ°: {checkpoint_path}")
                    self.logger.log("CHECKPOINT_SAVED", f"Epoch {epoch+1}/{self.args.num_epochs} | path={checkpoint_path} | train_loss={avg_loss:.4f}", level=1)

        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_path = self.save_checkpoint(
            model, condition_encoder, avg_loss, self.args.num_epochs - 1, is_final=True
        )
        if self.accelerator.is_local_main_process:
            print(f"æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜åˆ°: {final_path}")
            self.logger.log("TRAINING_COMPLETE", f"è®­ç»ƒå®Œæˆ | final_path={final_path} | final_train_loss={avg_loss:.4f} | total_epochs={self.args.num_epochs}", level=1)

        return model, condition_encoder

    # ============= æ¨ç†è¾…åŠ©æ–¹æ³• =============
    def _load_inference_model(self, model_path):
        """åŠ è½½æ¨ç†æ¨¡å‹å¹¶å¤„ç†æ£€æŸ¥ç‚¹è­¦å‘Š"""
        checkpoint_path = model_path if model_path and os.path.exists(model_path) else None

        if checkpoint_path:
            self.logger.log("MODEL_LOAD", f"ä½¿ç”¨æ£€æŸ¥ç‚¹: {checkpoint_path}", "inference", level=3)
        else:
            if self.accelerator.is_local_main_process:
                print("\n" + "="*60)
                print("âš ï¸  è­¦å‘Šï¼šæœªæ‰¾åˆ°æ£€æŸ¥ç‚¹ï¼")
                print("="*60)
                print("æ¨¡å‹å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æƒé‡è¿›è¡Œæ¨ç†ã€‚")
                print("è¿™ä¼šå¯¼è‡´æ¨ç†è´¨é‡å¾ˆå·®ï¼Œå¯èƒ½é™·å…¥æ— é™å¾ªç¯ã€‚")
                print("\nå»ºè®®æ“ä½œï¼š")
                print("1. å…ˆè®­ç»ƒæ¨¡å‹ï¼špython train.py --num_epochs 30")
                print("2. æˆ–æŒ‡å®šå·²æœ‰æ£€æŸ¥ç‚¹ï¼špython example.py --model_path checkpoints/your_checkpoint")
                print("="*60 + "\n")
            self.logger.log("MODEL_LOAD", "âš ï¸ æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–æƒé‡ï¼ˆè­¦å‘Šï¼šæ¨ç†è´¨é‡ä¼šå¾ˆå·®ï¼‰", "inference", level=3)

        return self.setup_models(checkpoint_path=checkpoint_path)

    def _prepare_initial_expression(self, initial_expr, x_data, y_data_len):
        """å‡†å¤‡åˆå§‹è¡¨è¾¾å¼å’Œtokens"""
        import sympy as sp
        from ..symbolic.symbolic_utils import evaluate_expression_safe, expr_to_tree

        # å¤„ç†åˆå§‹è¡¨è¾¾å¼
        if isinstance(initial_expr, list):
            current_tokens = initial_expr
            initial_expr = None
        else:
            if initial_expr is None:
                initial_expr = sp.Symbol('x0')
            elif isinstance(initial_expr, str):
                initial_expr = sp.sympify(initial_expr)

            initial_expr_str = expr_to_tree(initial_expr)
            current_tokens = initial_expr_str.split(',') if initial_expr_str else ['x0']

        # è®¡ç®—åˆå§‹è¡¨è¾¾å¼çš„é¢„æµ‹å€¼
        if initial_expr is not None:
            success, y_pred = evaluate_expression_safe(initial_expr, x_data)
            if not success:
                self.logger.log("INITIAL_EXPR_WARN", "æ— æ³•è®¡ç®—åˆå§‹è¡¨è¾¾å¼çš„é¢„æµ‹å€¼ï¼Œä½¿ç”¨é›¶åˆå§‹åŒ–", "inference", level=3)
                y_pred = [0.0] * y_data_len
        else:
            y_pred = [0.0] * y_data_len

        return initial_expr, current_tokens, y_pred

    # ============= ä¸»æ¨ç†æ–¹æ³• =============
    def symbolic_regression(self, model_path, x_data, y_data, n_steps=100, input_dim=None, max_expr_length=None, initial_expr=None):
        """ç¬¦å·å›å½’ - ä½¿ç”¨ç®€å•æ¨ç†(è´ªå©ªæœç´¢)æ¥æ”¶æ•°æ®ç‚¹å¯¹ï¼Œè¾“å‡ºè¡¨è¾¾å¼

        Args:
            model_path: æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„
            x_data: è¾“å…¥xæ•°æ®
            y_data: ç›®æ ‡yæ•°æ®
            n_steps: æ¨ç†æ­¥æ•°
            input_dim: è¾“å…¥ç»´åº¦ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨æ¨æ–­
            max_expr_length: è¡¨è¾¾å¼æœ€å¤§tokené•¿åº¦ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨argsä¸­çš„å€¼
            initial_expr: åˆå§‹è¡¨è¾¾å¼ï¼ˆsympyè¡¨è¾¾å¼æˆ–å­—ç¬¦ä¸²ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨x0
        """
        self.logger.log("SYMBOLIC_REGRESSION_START",
                       f"è¾“å…¥æ•°æ®: xå½¢çŠ¶={x_data.shape}, yå½¢çŠ¶={y_data.shape} | n_steps={n_steps}",
                       "inference", level=3)

        model, condition_encoder, _, _, _, tokenizer = self._load_inference_model(model_path)
        device = self.device
        model.eval()
        condition_encoder.eval()

        # å‡†å¤‡è¾“å…¥æ•°æ®
        x_values = torch.FloatTensor(x_data).unsqueeze(0).to(device)
        y_values = torch.FloatTensor(y_data).unsqueeze(0).to(device)

        if input_dim is None:
            input_dim = x_data.shape[1] if len(x_data.shape) > 1 else 1

        # å‡†å¤‡åˆå§‹è¡¨è¾¾å¼
        initial_expr, current_tokens, y_pred = self._prepare_initial_expression(initial_expr, x_data, len(y_data))

        if self.accelerator.is_local_main_process:
            print(f"åˆå§‹è¡¨è¾¾å¼: {initial_expr}")
            print(f"åˆå§‹tokens: {current_tokens}")

        # è®¡ç®—æ®‹å·®å¹¶ç¼–ç æ¡ä»¶
        residuals = y_values - torch.FloatTensor(y_pred).unsqueeze(0).to(device)
        point_mask = torch.ones_like(y_values)
        condition = condition_encoder(x_values, y_values, point_mask)

        # è®°å½•æ¡ä»¶ä¿¡æ¯ï¼ˆå†…è”_encode_condition_and_logçš„é€»è¾‘ï¼‰
        self.logger.log("INITIAL_DATA",
                       f"x_values: shape={x_values.shape} range=[{x_values.min():.4f},{x_values.max():.4f}] | "
                       f"y_target: shape={y_values.shape} range=[{y_values.min():.4f},{y_values.max():.4f}] | "
                       f"residuals: shape={residuals.shape} range=[{residuals.min():.4f},{residuals.max():.4f}] | "
                       f"initial_expr: {initial_expr} | initial_tokens: {current_tokens}",
                       "inference", level=3)
        self.logger.log("ARCHITECTURE_INFO",
                       "ä½¿ç”¨ç›®æ ‡å€¼y_targetä½œä¸ºæ¡ä»¶ï¼ˆæ¶æ„æ”¹è¿›ï¼šåŒ—ææ˜Ÿæ¨¡å¼ï¼‰",
                       "inference", level=3)

        # æ‰“å°æ¡ä»¶åµŒå…¥çš„å‰10ä¸ªç»´åº¦
        condition_cpu = condition.cpu().squeeze(0)
        condition_values = condition_cpu.detach().numpy()
        condition_preview = condition_values.flatten()[:10] if condition_values.ndim == 2 else condition_values[:10]
        self.logger.log("INITIAL_CONDITION",
                       f"condition: shape={condition.shape} range=[{condition.min():.4f},{condition.max():.4f}] | "
                       f"å‰10ç»´: [{', '.join([f'{float(v):.6f}' for v in condition_preview])}]",
                       "inference", level=3)

        # åˆ›å»ºæœç´¢å™¨ï¼ˆå†…è”_create_searcherçš„é€»è¾‘ï¼‰
        self.logger.log("SIMPLE_SEARCH_INIT", f"åˆå§‹åŒ–ç®€å•æ¨ç†å™¨ | n_steps={n_steps}", "inference", level=3)
        searcher = SimpleSymbolicRegression(
            model=model, condition_encoder=condition_encoder, tokenizer=tokenizer,
            device=device, args=self.args, logger=self.logger,
            min_action_score=self.MIN_ACTION_SCORE,
            max_expression_length=self.MAX_EXPRESSION_LENGTH,
            numerical_clip_threshold=self.NUMERICAL_CLIP_THRESHOLD
        )

        # æ‰§è¡Œæ¨ç†ï¼ˆå†…è”_run_single_threshold_searchçš„é€»è¾‘ï¼‰
        if self.accelerator.is_local_main_process:
            print(f"\næ‰§è¡Œå•æœ€ä½³æ“ä½œæ¨ç†...")

        residuals_np = residuals.cpu().squeeze(0).numpy()
        best_candidate = searcher.greedy_search(
            initial_tokens=current_tokens, initial_condition=condition,
            initial_residuals=residuals_np, x_data=x_data, y_data=y_data,
            x_values=x_values, n_steps=n_steps
        )

        final_expression = ','.join(best_candidate.tokens) if best_candidate and best_candidate.tokens else ""

        if best_candidate and self.accelerator.is_local_main_process:
            mse_score = best_candidate.mse_score
            mse_str = f'{mse_score:.6f}' if mse_score is not None else 'N/A'
            self.logger.log("SIMPLE_SEARCH_RESULT",
                           f"MSEåˆ†æ•°: {mse_str} | "
                           f"æ“ä½œå†å²: {' -> '.join(best_candidate.history[-5:]) if best_candidate.history else 'N/A'}",
                           "inference", level=3)

        self.logger.log("INFERENCE_COMPLETE", f"æœ€ç»ˆè¡¨è¾¾å¼: {final_expression}", "inference", level=3)

        return {
            'final_expression': final_expression,
            'initial_tokens': current_tokens,
            'final_tokens': best_candidate.tokens if best_candidate else [],
            'history': best_candidate.history if best_candidate else [],
            'position_actions_history': best_candidate.position_actions_history if best_candidate else [],
            'mse_score': best_candidate.mse_score if best_candidate else None
        }

