"""
EditFlowè¿­ä»£ä¼˜åŒ–è®­ç»ƒå™¨ - å®ç°åŸºäºè¿­ä»£å¼ç¼–è¾‘æ“ä½œçš„ç¬¦å·å›å½’æ¨¡å‹è®­ç»ƒ
ä½¿ç”¨ Hugging Face Accelerate è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒåŠ é€Ÿ
"""

import os
import time
import sympy as sp

import torch
import numpy as np
from tqdm import tqdm
from accelerate import Accelerator
from accelerate.utils import set_seed

from ..symbolic.data_generator import generate_flow_samples
from .flow import (
    remove_gap_tokens, fill_gap_tokens_with_repeats,
    ContinuousFlowLoss, FlowDataset, custom_collate_fn
)
from ..modeling.condition_encoder import SetTransformerConditionEncoder
from ..modeling.llama_editflow import LlamaEditFlowBackbone
from ..utils.misc_utils import find_latest_checkpoint, load_checkpoint
from ..utils.logger import Logger
from .greedy_search import SimpleSymbolicRegression


class EditFlowManager:
    """EditFlowæ¨¡å‹ç®¡ç†å™¨ - æ”¯æŒè®­ç»ƒå’Œæ¨ç†åŠŸèƒ½

    æ¶æ„ç‰¹ç‚¹ï¼šè¿­ä»£ä¼˜åŒ–æ¨¡å¼
    - æ¨¡å‹ç›´æ¥é¢„æµ‹ä»z0åˆ°z1çš„ç¼–è¾‘æ“ä½œï¼ˆæ’å…¥ã€åˆ é™¤ã€æ›¿æ¢ï¼‰
    - æ—¶é—´æ­¥å›ºå®šä¸º0ï¼Œå­¦ä¹ ä»èµ·ç‚¹åˆ°ç›®æ ‡çš„ç›´æ¥ç¼–è¾‘è·¯å¾„
    - ä½¿ç”¨ç›®æ ‡å€¼y_targetä½œä¸ºæ¡ä»¶ï¼ˆè€Œéæ®‹å·®ï¼‰ï¼Œä¿æŒæ¡ä»¶æ’å®šä½œä¸º"åŒ—ææ˜Ÿ"
    """

    # ç±»å¸¸é‡ï¼šè®­ç»ƒå’Œæ¨ç†é…ç½®å‚æ•°
    GRADIENT_CLIP_NORM = 10.0  # æé«˜åˆ°10.0ï¼Œé¿å…è¿‡åº¦è£å‰ª
    NUMERICAL_CLIP_THRESHOLD = 1e6
    MAX_EXPRESSION_LENGTH = 50
    MIN_ACTION_SCORE = 0.01  # æœ€å°æ“ä½œåˆ†æ•°é˜ˆå€¼

    def __init__(self, args):
        self.args = args

        # åˆå§‹åŒ– Accelerate - è‡ªåŠ¨å¤„ç†åˆ†å¸ƒå¼è®­ç»ƒè®¾ç½®
        # æ³¨æ„ï¼šmixed_precision ç”± accelerate launch å‘½ä»¤è¡Œå‚æ•°æ§åˆ¶
        # ä¸è¦åœ¨ä»£ç ä¸­ç¡¬ç¼–ç ï¼Œå¦åˆ™ä¼šè¦†ç›–å‘½ä»¤è¡Œçš„ --mixed_precision=bf16 è®¾ç½®
        self.accelerator = Accelerator(
            gradient_accumulation_steps=args.gradient_accumulation_steps,
            log_with=args.log_with
        )

        # è®¾ç½®éšæœºç§å­
        set_seed(args.seed)

        # ä¿å­˜debugæ¨¡å¼æ ‡å¿—
        self.debug_mode = args.debug

        # åˆå§‹åŒ–ç»Ÿä¸€æ—¥å¿—ç®¡ç†å™¨ï¼Œä¼ å…¥debug_modeå‚æ•°
        self.logger = Logger(self.accelerator, enabled=True, debug_mode=self.debug_mode)

        # è®¾å¤‡ä¿¡æ¯
        self.device = self.accelerator.device
        if self.accelerator.is_local_main_process:
            print("=== EditFlowç¬¦å·å›å½’é¢„è®­ç»ƒ (ä½¿ç”¨ Accelerate åŠ é€Ÿ) ===")
            print(f"æ ·æœ¬æ•°: {self.args.num_samples}")
            print(f"æœ€å¤§ç»´åº¦: {self.args.max_dim}")
            print(f"è¡¨è¾¾å¼æœ€å¤§é•¿åº¦: {self.args.max_expr_length}")
            print(f"æ‰¹æ¬¡å¤§å°: {self.args.batch_size}")
            print(f"è®­ç»ƒè½®æ•°: {self.args.num_epochs}")
            print(f"å­¦ä¹ ç‡: {self.args.learning_rate}")
            print(f"æµ‹è¯•é›†æ¯”ä¾‹: {self.args.test_split}")
            print(f"è¯„ä¼°é¢‘ç‡: æ¯{self.args.eval_every}è½®")
            print(f"LLaMAæ¨¡å‹é…ç½®: hidden_dim={self.args.hidden_dim}, n_layers={self.args.n_layers}, n_heads={self.args.n_heads}")
            print(f"æ¡ä»¶åµŒå…¥æ¨¡å‹: {self.args.condition_model_name}")
            print(f"æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {self.args.gradient_accumulation_steps}")
            print(f"FP16æ··åˆç²¾åº¦: {self.args.use_fp16}")

            print(f"\nAccelerate åˆå§‹åŒ–å®Œæˆ")
            print(f"  è®¾å¤‡: {self.device}")
            print(f"  åˆ†å¸ƒå¼è®­ç»ƒ: {self.accelerator.distributed_type}")
            print(f"  è¿›ç¨‹æ•°: {self.accelerator.num_processes}")
            print(f"  æ··åˆç²¾åº¦: {self.accelerator.mixed_precision}")
            print(f"  è°ƒè¯•æ¨¡å¼: {'å¯ç”¨' if self.debug_mode else 'ç¦ç”¨'}")

        # è®°å½•è®­ç»ƒå¼€å§‹æ—¥å¿—
        self.logger.training_start(self.args)

    def _gather_average_loss(self, total_loss, num_batches, default_value=0.0):
        """è·¨è¿›ç¨‹æ”¶é›†å¹¶è®¡ç®—å¹³å‡æŸå¤±"""
        self.accelerator.wait_for_everyone()
        total_loss_tensor = torch.tensor(total_loss, device=self.device)
        num_batches_tensor = torch.tensor(num_batches, device=self.device)
        gathered_losses = self.accelerator.gather(total_loss_tensor)
        gathered_batches = self.accelerator.gather(num_batches_tensor)
        total_batches = gathered_batches.sum().item()
        return gathered_losses.sum().item() / total_batches if total_batches > 0 else default_value

    def set_seed(self, seed: int):
        """è®¾ç½®éšæœºç§å­ - ç°åœ¨ä½¿ç”¨ Accelerate çš„ set_seed"""
        set_seed(seed)

    def prepare_data(self, tokenizer):
        """å‡†å¤‡è®­ç»ƒæ•°æ®ï¼Œä½¿ç”¨ Hugging Face datasets åŠ è½½"""

        # 1. æ•°æ®ç”Ÿæˆé˜¶æ®µï¼šåªä½¿ç”¨ä¸»è¿›ç¨‹ï¼ˆå•è¿›ç¨‹ï¼‰
        cache_filename = f"data/flow_samples_{self.args.num_samples}_{self.args.max_dim}dim_{self.args.n_points}pts_{self.args.max_depth}depth_{self.args.max_expr_length}len.txt"

        # åªæœ‰ä¸»è¿›ç¨‹è´Ÿè´£æ•°æ®ç”Ÿæˆï¼Œé¿å…NCCLé€šä¿¡é—®é¢˜
        if self.accelerator.is_local_main_process:
            print(f"å‡†å¤‡è¿ç»­æµè®­ç»ƒæ•°æ® (å•è¿›ç¨‹ç”Ÿæˆæ¨¡å¼)...")

            # è·å–å¯¹é½æ–¹æ³•é…ç½®
            print(f"ä½¿ç”¨å¯¹é½æ–¹æ³•: {self.args.alignment_method}")

            # è°ƒç”¨æ•°æ®ç”Ÿæˆå‡½æ•°
            generate_flow_samples(
                num_samples=self.args.num_samples,
                max_dim=self.args.max_dim,
                n_points=self.args.n_points,
                max_depth=self.args.max_depth,
                max_expr_length=self.args.max_expr_length,
                verbose=True,  # æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—
                alignment_method=self.args.alignment_method,
            )
        else:
            # éä¸»è¿›ç¨‹è·³è¿‡æ•°æ®ç”Ÿæˆï¼Œç­‰å¾…ä¸»è¿›ç¨‹å®Œæˆ
            print(f"[Rank {self.accelerator.process_index}] è·³è¿‡æ•°æ®ç”Ÿæˆï¼Œç­‰å¾…ä¸»è¿›ç¨‹å®Œæˆ...")

        # 2. åŒæ­¥å±éšœï¼šç­‰å¾…ä¸»è¿›ç¨‹å®Œæˆæ•°æ®ç”Ÿæˆ
        self.accelerator.wait_for_everyone()

        if self.accelerator.is_local_main_process:
            print("[ä¸»è¿›ç¨‹] æ•°æ®ç”Ÿæˆå®Œæˆï¼Œå¼€å§‹åŠ è½½è®­ç»ƒæ•°æ®")

        # 3. åŒæ­¥å±éšœï¼šç¡®ä¿æ‰€æœ‰è¿›ç¨‹éƒ½èƒ½è®¿é—®åˆ°å®Œæ•´çš„æ•°æ®æ–‡ä»¶
        print(f"[Rank {self.accelerator.process_index}] å‡†å¤‡å¼€å§‹è®­ç»ƒé˜¶æ®µ...")
        self.accelerator.wait_for_everyone()

        # 4. ä½¿ç”¨ Hugging Face datasets åŠ è½½æ•°æ®
        # ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°æ§åˆ¶æ•°æ®åŠ è½½æ¨¡å¼
        use_stream = self.args.dataset_stream
        num_proc = self.args.dataset_num_proc

        if self.accelerator.is_local_main_process:
            print(f"ä½¿ç”¨ Hugging Face datasets åŠ è½½æ•°æ® (stream={use_stream})...")

        # åŠ è½½å®Œæ•´æ•°æ®é›†ï¼ˆtrainå’Œtestå°†é€šè¿‡train_test_splitåˆ†å‰²ï¼‰
        full_dataset = FlowDataset(
            data_file=cache_filename,
            tokenizer=tokenizer,
            max_dim=self.args.max_dim,
            max_expr_length=self.args.max_expr_length,
            stream=use_stream,
            num_proc=num_proc
        )

        # 5. åˆ†å‰²è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        # æ³¨æ„ï¼šæµå¼æ¨¡å¼ä¸‹æ— æ³•ç›´æ¥ä½¿ç”¨train_test_splitï¼Œéœ€è¦æ‰‹åŠ¨å¤„ç†
        if use_stream:
            # æµå¼æ¨¡å¼ï¼šä½¿ç”¨è¿­ä»£å™¨åˆ†å‰²ï¼ˆè¿‘ä¼¼ï¼‰
            # å…ˆè®¡ç®—åˆ†å‰²ç‚¹
            split_ratio = 1 - self.args.test_split
            train_size = int(self.args.num_samples * split_ratio)
            test_size = self.args.num_samples - train_size

            if self.accelerator.is_local_main_process:
                print(f"æµå¼æ¨¡å¼: è®­ç»ƒé›†çº¦ {train_size} æ ·æœ¬, æµ‹è¯•é›†çº¦ {test_size} æ ·æœ¬")

            # åˆ›å»ºä¸¤ä¸ªæ•°æ®é›†å®ä¾‹ï¼ˆé€šè¿‡è·³è¿‡ä¸åŒçš„è¡Œæ•°å®ç°ï¼‰
            # æ³¨æ„ï¼šè¿™ç§æ–¹å¼ä¸å¤Ÿç²¾ç¡®ï¼Œä½†æµå¼æ¨¡å¼ä¸‹æ— æ³•é¢„å…ˆçŸ¥é“ç¡®åˆ‡æ•°é‡
            train_dataset = full_dataset
            # å¯¹äºæµ‹è¯•é›†ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€ä¸ªæ–°çš„å®ä¾‹ï¼Œä½†éœ€è¦åœ¨è¿­ä»£æ—¶è·³è¿‡è®­ç»ƒæ ·æœ¬
            # ç®€åŒ–å¤„ç†ï¼šè¿™é‡Œæš‚æ—¶ä½¿ç”¨ç›¸åŒçš„æ•°æ®é›†ï¼Œå®é™…è®­ç»ƒæ—¶é€šè¿‡é‡‡æ ·æ§åˆ¶
            test_dataset = full_dataset  # ç®€åŒ–å¤„ç†

            train_size_estimate = train_size
            test_size_estimate = test_size
        else:
            # éæµå¼æ¨¡å¼ï¼šå¯ä»¥ç²¾ç¡®åˆ†å‰²
            total_size = len(full_dataset)
            train_size = int(total_size * (1 - self.args.test_split))

            # æ‰‹åŠ¨åˆ†å‰²åˆ—è¡¨
            from torch.utils.data import Subset

            # ç”Ÿæˆç´¢å¼•å¹¶æ‰“ä¹±
            indices = list(range(total_size))
            np.random.shuffle(indices)

            train_indices = indices[:train_size]
            test_indices = indices[train_size:]

            # å¦‚æœåªæœ‰1ä¸ªæ ·æœ¬ï¼Œè®©å®ƒæ—¢ç”¨äºè®­ç»ƒåˆç”¨äºæµ‹è¯•
            if total_size == 1:
                if self.accelerator.is_local_main_process:
                    print("è­¦å‘Š: åªæœ‰1ä¸ªæ ·æœ¬ï¼Œå°†åŒæ—¶ç”¨äºè®­ç»ƒå’Œæµ‹è¯•")
                train_dataset = full_dataset
                test_dataset = full_dataset
                train_size_estimate = 1
                test_size_estimate = 1
            else:
                train_dataset = Subset(full_dataset, train_indices)
                test_dataset = Subset(full_dataset, test_indices)
                train_size_estimate = len(train_indices)
                test_size_estimate = len(test_indices)

            if self.accelerator.is_local_main_process:
                print(f"éæµå¼æ¨¡å¼: è®­ç»ƒé›† {train_size_estimate} æ ·æœ¬, æµ‹è¯•é›† {test_size_estimate} æ ·æœ¬")

        # 6. åˆ›å»º DataLoader

        # æ£€æŸ¥æ˜¯å¦ä¸ºstreamæ¨¡å¼
        is_stream_mode = getattr(train_dataset, 'stream', False)

        # è·å–æ•°æ®é›†å¤§å°ï¼Œæ™ºèƒ½è°ƒæ•´drop_last
        train_size = len(train_dataset)
        test_size = len(test_dataset)

        # å¯¹äºå°æ•°æ®é›†ï¼Œç¦ç”¨drop_lastä»¥å…æ‰€æœ‰æ•°æ®éƒ½è¢«ä¸¢å¼ƒ
        train_drop_last = train_size >= self.args.batch_size
        test_drop_last = test_size >= self.args.batch_size

        if self.accelerator.is_local_main_process:
            if not train_drop_last:
                print(f"è­¦å‘Š: è®­ç»ƒé›†å¤§å°({train_size}) < batch_size({self.args.batch_size})ï¼Œç¦ç”¨drop_last")
            if not test_drop_last:
                print(f"è­¦å‘Š: æµ‹è¯•é›†å¤§å°({test_size}) < batch_size({self.args.batch_size})ï¼Œç¦ç”¨drop_last")

        # æ ¹æ®streamæ¨¡å¼ç¡®å®šDataLoaderå‚æ•°
        train_shuffle = not is_stream_mode
        num_workers = 0 if is_stream_mode else self.accelerator.num_processes

        # åˆ›å»ºDataLoader
        train_dataloader = torch.utils.data.DataLoader(
            train_dataset,
            batch_size=self.args.batch_size,
            shuffle=train_shuffle,
            num_workers=num_workers,
            collate_fn=custom_collate_fn,
            drop_last=train_drop_last,
            pin_memory=True
        )

        test_dataloader = torch.utils.data.DataLoader(
            test_dataset,
            batch_size=self.args.batch_size,
            shuffle=False,
            num_workers=num_workers,
            collate_fn=custom_collate_fn,
            drop_last=test_drop_last
        )

        # ä½¿ç”¨ Accelerate å‡†å¤‡
        train_dataloader, test_dataloader = self.accelerator.prepare(
            train_dataloader, test_dataloader
        )

        if self.accelerator.is_local_main_process:
            print(f"æ•°æ®å‡†å¤‡å®Œæˆ: è®­ç»ƒé›†çº¦ {train_size_estimate} æ ·æœ¬, æµ‹è¯•é›†çº¦ {test_size_estimate} æ ·æœ¬")

        return train_dataloader, train_dataset, test_dataloader, test_dataset

    def setup_models(self, checkpoint_path=None):
        """
        åˆå§‹åŒ–æ¨¡å‹å’Œtokenizerï¼Œæ”¯æŒä»æ£€æŸ¥ç‚¹åŠ è½½

        Args:
            checkpoint_path: æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™åˆ›å»ºæ–°æ¨¡å‹

        Returns:
            model, condition_encoder, criterion, optimizer, tokenizer
        """
        if self.accelerator.is_local_main_process:
            print("åˆå§‹åŒ–tokenizerå’Œæ¨¡å‹...")

        # ä½¿ç”¨ç¬¦å·å›å½’ä¸“å±çš„å°è¯æ±‡è¡¨åˆ†è¯å™¨
        # ä¸å†ä¾èµ–BERTçš„å¤§è¯æ±‡è¡¨ï¼Œä½¿ç”¨è‡ªå®šä¹‰çš„ç´§å‡‘è¯æ±‡è¡¨
        from ..utils.special_tokens import SymbolicRegressionTokenizer, SymbolicVocab

        tokenizer = SymbolicRegressionTokenizer(max_dim=self.args.max_dim)

        if self.accelerator.is_local_main_process:
            print(f"âœ“ ç¬¦å·å›å½’Tokenizeråˆå§‹åŒ–å®Œæˆ")
            print(f"  è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size} (ç¬¦å·å›å½’ä¸“å±å°è¯æ±‡è¡¨)")
            print(f"  æœ€å¤§ç»´åº¦: {self.args.max_dim}")
            print(f"  è¿ç®—ç¬¦: {len(SymbolicVocab.OPERATORS)}ä¸ª - {', '.join(SymbolicVocab.OPERATORS)}")
            print(f"  å‡½æ•°: {len(SymbolicVocab.FUNCTIONS)}ä¸ª - {', '.join(SymbolicVocab.FUNCTIONS)}")
            print(f"  ç‰¹æ®Štoken: {len(SymbolicVocab.SPECIAL_TOKENS)}ä¸ª")
            print(f"  å˜é‡token: x0 ~ x{self.args.max_dim-1} (å…±{self.args.max_dim}ä¸ª)")

        if self.accelerator.is_local_main_process:
            print("åˆå§‹åŒ–æ¡ä»¶ç¼–ç å™¨...")
        condition_encoder = SetTransformerConditionEncoder(
            max_input_dim=self.args.condition_max_input_dim,
            dim_hidden=self.args.condition_dim_hidden,
            num_heads=self.args.condition_num_heads,
            num_inds=self.args.condition_num_inds,
            num_layers=self.args.condition_num_layers,
            num_seeds=self.args.condition_num_seeds,
            dim_output=self.args.condition_dim_output,
            verbose=self.accelerator.is_local_main_process
        ).to(self.device)

        if self.accelerator.is_local_main_process:
            print("åˆå§‹åŒ–LLaMA EditFlowæ¨¡å‹ï¼ˆè‡ªå®šä¹‰æ¶æ„ï¼Œä¸åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼‰...")

        # è·å–æ¡ä»¶ç¼–ç å™¨çš„éšè—å±‚ç»´åº¦
        # ç°åœ¨æ¡ä»¶ç¼–ç å™¨è¾“å‡º (batch_size, num_seeds, dim_hidden) æ ¼å¼
        # æ‰€ä»¥ condition_dim åº”è¯¥ç­‰äº dim_hidden
        condition_hidden_dim = self.args.condition_dim_hidden

        # ç›´æ¥å®ä¾‹åŒ– LlamaEditFlowBackbone
        model = LlamaEditFlowBackbone(
            vocab_size=len(tokenizer.get_vocab()),  # ç¬¦å·å›å½’ä¸“ç”¨å°è¯è¡¨
            hidden_dim=self.args.hidden_dim,  # LLaMAéšè—å±‚ç»´åº¦
            n_layers=self.args.n_layers,  # Transformerå±‚æ•°
            n_heads=self.args.n_heads,  # æ³¨æ„åŠ›å¤´æ•°
            condition_dim=condition_hidden_dim,
            dropout=self.args.dropout,
            max_seq_len=self.args.max_expr_length,
            use_condition_injection=self.args.use_condition_injection,
            verbose=self.accelerator.is_local_main_process
        ).to(self.device)

        # åˆ›å»ºä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        criterion = ContinuousFlowLoss(debug_mode=self.debug_mode)
        optimizer = torch.optim.AdamW(
            list(model.parameters()) + list(condition_encoder.parameters()),
            lr=self.args.learning_rate,
            weight_decay=self.args.weight_decay,
            eps=1e-8,  # å¢åŠ æ•°å€¼ç¨³å®šæ€§
            betas=(0.9, 0.999)  # ä½¿ç”¨é»˜è®¤çš„betaå€¼
        )

        # æ·»åŠ å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆä½™å¼¦é€€ç«ï¼‰
        from torch.optim.lr_scheduler import CosineAnnealingLR
        scheduler = CosineAnnealingLR(
            optimizer,
            T_max=self.args.num_epochs,
            eta_min=1e-6  # æœ€å°å­¦ä¹ ç‡
        )

        # å¦‚æœæä¾›äº†æ£€æŸ¥ç‚¹è·¯å¾„ï¼ŒåŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        load_checkpoint(checkpoint_path, model, condition_encoder, self.device, optimizer, verbose=self.accelerator.is_local_main_process)

        # ä½¿ç”¨ Accelerate å‡†å¤‡æ¨¡å‹ã€ä¼˜åŒ–å™¨å’Œæ•°æ®åŠ è½½å™¨
        if self.accelerator.is_local_main_process:
            print(f"ä½¿ç”¨ Accelerate å‡†å¤‡æ¨¡å‹å’Œä¼˜åŒ–å™¨...")
            print(f"  è¿›ç¨‹æ•°: {self.accelerator.num_processes}")
            print(f"  è®¾å¤‡: {self.accelerator.device}")
            print(f"  æ··åˆç²¾åº¦: {self.accelerator.mixed_precision}")

        model, condition_encoder, optimizer = self.accelerator.prepare(
            model, condition_encoder, optimizer
        )

        # å¦‚æœæœ‰checkpointï¼Œä½¿ç”¨Accelerateçš„load_stateæ–¹æ³•åŠ è½½å®Œæ•´çŠ¶æ€
        if checkpoint_path:
            if self.accelerator.is_local_main_process:
                print(f"Loading complete training state from {checkpoint_path}")
            self.accelerator.load_state(checkpoint_path)

        total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        if self.accelerator.is_local_main_process:
            print(f"âœ“ LLaMA EditFlowæ¨¡å‹å‚æ•°æ•°é‡: {total_params:,}")

        # ä¿å­˜tokenizerå¼•ç”¨ä¾›åç»­ä½¿ç”¨
        self.tokenizer = tokenizer

        return model, condition_encoder, criterion, optimizer, scheduler, tokenizer

  
    def forward_pass(self, model, condition_embeddings, z0_token_ids, z1_token_ids, debug_info=None):
        """
        ä¿®æ”¹åçš„å‰å‘ä¼ æ’­ï¼šç§»é™¤ä¸­é—´çŠ¶æ€æ’å€¼ï¼Œç›´æ¥é¢„æµ‹ä»z0åˆ°z1çš„ç¼–è¾‘æ“ä½œ
        è¿™å°†æ¨¡å‹ä»"è¿ç»­æµåŒ¹é…"è½¬å˜ä¸º"è¿­ä»£ä¼˜åŒ–"æ¶æ„
        """
        batch_size = z0_token_ids.size(0)

        # è·å– vocab_sizeï¼ˆä½¿ç”¨self.tokenizerï¼Œé¿å…Subsetå¯¹è±¡æ²¡æœ‰tokenizerå±æ€§çš„é—®é¢˜ï¼‰
        vocab_size = self.tokenizer.vocab_size

        # è¿­ä»£ä¼˜åŒ–æ¨¡å¼ï¼šä½¿ç”¨z0ä½œä¸ºå½“å‰çŠ¶æ€çš„è¾“å…¥ï¼ˆz0 -> z1çš„ç¼–è¾‘æ“ä½œï¼‰
        batch_size, seq_len = z0_token_ids.shape
        z0_probs = torch.zeros(batch_size, seq_len, vocab_size, device=z0_token_ids.device)
        z0_probs.scatter_(2, z0_token_ids.unsqueeze(-1), 1.0)

        # z1 tokenåºåˆ—ç”¨äºè®¡ç®—ç›®æ ‡ç¼–è¾‘æ“ä½œ
        z1_probs = torch.zeros(batch_size, seq_len, vocab_size, device=z1_token_ids.device)
        z1_probs.scatter_(2, z1_token_ids.unsqueeze(-1), 1.0)

        # è®°å½•è¾“å…¥å˜é‡çš„å®Œæ•´å€¼ï¼ˆä»…åœ¨debugæ¨¡å¼ä¸‹è®°å½•ï¼‰
        if debug_info and self.accelerator.is_local_main_process and self.debug_mode:
            context = debug_info.get('context', '')
            batch_idx = debug_info.get('batch_idx', 0)
            sample_idx = 0

            # è®°å½•ç¬¬ä¸€ä¸ªæ ·æœ¬çš„tokenåºåˆ—ï¼ˆå®Œæ•´å€¼ï¼‰
            self.logger.tensor_values(f"z0_token_ids_batch{batch_idx}", z0_token_ids[sample_idx],
                                     context=context, level=2, max_elements=50)
            self.logger.tensor_values(f"z1_token_ids_batch{batch_idx}", z1_token_ids[sample_idx],
                                     context=context, level=2, max_elements=50)

            # è®°å½•condition_embeddingsï¼ˆæ˜¾ç¤ºå®Œæ•´å€¼ï¼‰
            self.logger.tensor_values(f"condition_embeddings_batch{batch_idx}", condition_embeddings[sample_idx],
                                     context=context, level=2, max_elements=100)

        # è¿­ä»£ä¼˜åŒ–æ¨¡å¼ï¼šç›´æ¥ä½¿ç”¨z0ä½œä¸ºå½“å‰çŠ¶æ€ï¼Œä¸å†è¿›è¡Œæ—¶é—´æ’å€¼
        # ç§»é™¤gap tokenå¾—åˆ°è¾“å…¥åºåˆ—x_tï¼ˆåŸå§‹åºåˆ—ç©ºé—´ï¼Œæ— gapé‡å¤ï¼‰
        x_t, x_pad_mask, z_gap_mask, z_pad_mask = remove_gap_tokens(
            z0_token_ids, self.tokenizer
        )

        # è®°å½•x_tçš„å®Œæ•´å€¼ï¼ˆä»…åœ¨debugæ¨¡å¼ä¸‹è®°å½•ï¼‰
        if debug_info and self.accelerator.is_local_main_process and self.debug_mode:
            context = debug_info.get('context', '')
            batch_idx = debug_info.get('batch_idx', 0)
            self.logger.tensor_values(f"x_t_batch{batch_idx}", x_t[0],
                                     context=context, level=2, max_elements=50)

        attention_mask = (~x_pad_mask).float()

        # è®°å½•attention_maskçš„å®Œæ•´å€¼ï¼ˆä»…åœ¨debugæ¨¡å¼ä¸‹è®°å½•ï¼‰
        if debug_info and self.accelerator.is_local_main_process and self.debug_mode:
            context = debug_info.get('context', '')
            batch_idx = debug_info.get('batch_idx', 0)
            self.logger.tensor_values(f"attention_mask_batch{batch_idx}", attention_mask[0],
                                     context=context, level=2, max_elements=50)

        # è°ƒç”¨ LlamaEditFlowBackboneï¼Œè¿”å›å­—å…¸æ ¼å¼
        output = model(
            input_ids=x_t, condition=condition_embeddings, attention_mask=attention_mask
        )

        # åˆå¹¶å››ä¸ªé€Ÿç‡ä¸ºä¸€ä¸ªtensorï¼ˆæ·»åŠ KEEPæ“ä½œï¼‰
        ins_rate, del_rate, sub_rate, keep_rate = output['rates']
        pred_rates = torch.cat([ins_rate, del_rate, sub_rate, keep_rate], dim=-1)

        # è®°å½•æ¨¡å‹è¾“å‡ºçš„å®Œæ•´å€¼ï¼ˆä»…åœ¨debugæ¨¡å¼ä¸‹è®°å½•ï¼‰
        if debug_info and self.accelerator.is_local_main_process and self.debug_mode:
            context = debug_info.get('context', '')
            batch_idx = debug_info.get('batch_idx', 0)
            sample_idx = 0

            # è®°å½•ç¬¬ä¸€ä¸ªæ ·æœ¬çš„pred_rateså®Œæ•´å€¼
            self.logger.tensor_values(f"pred_rates_batch{batch_idx}", pred_rates[sample_idx],
                                     context=context, level=2, max_elements=100)

            # è®°å½•ç¬¬ä¸€ä¸ªæ ·æœ¬çš„insert_logitså’Œsubstitute_logitså®Œæ•´å€¼
            self.logger.tensor_values(f"insert_logits_batch{batch_idx}", output['insert_logits'][sample_idx],
                                     context=context, level=2, max_elements=100)
            self.logger.tensor_values(f"substitute_logits_batch{batch_idx}", output['substitute_logits'][sample_idx],
                                     context=context, level=2, max_elements=100)

        return {
            'pred_rates': pred_rates,
            'pred_ins_logits': output['insert_logits'],  # è¿”å›logitsè€Œä¸æ˜¯æ¦‚ç‡
            'pred_sub_logits': output['substitute_logits'],  # è¿”å›logitsè€Œä¸æ˜¯æ¦‚ç‡
            'x_t': x_t,
            'z0': z0_token_ids,
            'z1_token_ids': z1_token_ids,
            'z_gap_mask': z_gap_mask,
            'z_pad_mask': z_pad_mask,
            'attention_mask': attention_mask,  # è¿”å›attention_maskç”¨äºå±è”½æ— æ•ˆä½ç½®
            'vocab_size': vocab_size,
        }

    def compute_loss(self, forward_results, criterion, debug_info=None):
        pred_rates = forward_results['pred_rates']
        x_t = forward_results['x_t']
        z0 = forward_results['z0']  # å½“å‰çŠ¶æ€ï¼ˆèµ·ç‚¹ï¼‰
        z1_token_ids = forward_results['z1_token_ids']  # ç›®æ ‡çŠ¶æ€ï¼ˆç»ˆç‚¹ï¼‰
        z_gap_mask = forward_results['z_gap_mask']
        z_pad_mask = forward_results['z_pad_mask']
        effective_vocab_size = forward_results['vocab_size']
        gap_token = self.tokenizer.convert_tokens_to_ids('<gap>')

        # ä¿®å¤ç´¢å¼•é”™ä½bugï¼šæ¨¡å‹è¾“å‡ºé¡ºåºæ˜¯ [ins_rate, del_rate, sub_rate, keep_rate]
        # å› æ­¤ç´¢å¼• 0=æ’å…¥, 1=åˆ é™¤, 2=æ›¿æ¢, 3=ä¿æŒ
        lambda_ins = pred_rates[:, :, 0:1]  # æ’å…¥é€Ÿç‡
        lambda_del = pred_rates[:, :, 1:2]  # åˆ é™¤é€Ÿç‡
        lambda_sub = pred_rates[:, :, 2:3]  # æ›¿æ¢é€Ÿç‡
        lambda_keep = pred_rates[:, :, 3:4]  # ä¿æŒé€Ÿç‡

        # å…³é”®ä¿®å¤ï¼šç›´æ¥ä½¿ç”¨logitsè€Œä¸æ˜¯æ¦‚ç‡
        # å°†é€Ÿç‡ä½œä¸ºlogç©ºé—´åŠ åˆ°logitsä¸Š
        ins_logits = forward_results['pred_ins_logits'] + torch.log(lambda_ins.clamp(min=1e-8))
        sub_logits = forward_results['pred_sub_logits'] + torch.log(lambda_sub.clamp(min=1e-8))
        del_logits = torch.log(lambda_del.clamp(min=1e-8))
        keep_logits = torch.log(lambda_keep.clamp(min=1e-8))  # KEEPæ“ä½œçš„logits

        # ğŸ”§ ä¿®å¤ï¼šä¸å†åœ¨è¿™é‡Œåº”ç”¨attention_mask
        # attention_maskä¼šåœ¨lossè®¡ç®—æ—¶é€šè¿‡u_maskè‡ªç„¶å¤„ç†
        # é¿å…åœ¨è¿™é‡Œè®¾ç½®-1e9å¯¼è‡´log_softmaxåå‡ºç°-infæ±¡æŸ“

        # u_cat_x ç°åœ¨æ˜¯logitsè€Œä¸æ˜¯æ¦‚ç‡
        # å½¢çŠ¶: [batch, x_seq_len, 2*vocab_size+2] (æ·»åŠ KEEPæ“ä½œ)
        # ğŸ”§ ä¿®å¤ï¼šè°ƒæ•´é¡ºåºä»¥åŒ¹é…u_maskçš„ç»´åº¦å®šä¹‰å’Œrates_headçš„è¾“å‡ºé¡ºåº
        # ç»Ÿä¸€é¡ºåºï¼š[INS(vocab_size) | DEL(1) | SUB(vocab_size) | KEEP(1)]
        #           ä½ç½®: 0~v-1          ä½ç½®: v   ä½ç½®: v+1~2v      ä½ç½®: 2v+1(-1)
        # å…¶ä¸­ v = vocab_size
        # æ‹¼æ¥é¡ºåºï¼šins | del | sub | keep (ä¸rates_headçš„[ins, del, sub, keep]ä¸€è‡´)
        u_cat_x = torch.cat([ins_logits, del_logits, sub_logits, keep_logits], dim=-1)

        # u_z æ˜¯ Z ç©ºé—´ï¼ˆæ‰©å±•ç©ºé—´ï¼Œå«gapé‡å¤ï¼‰çš„é¢„æµ‹é€Ÿç‡
        # å½¢çŠ¶: [batch, z_seq_len, 2*vocab_size+2] (æ·»åŠ KEEPæ“ä½œ)
        u_z = fill_gap_tokens_with_repeats(u_cat_x, z_gap_mask, z_pad_mask)

        # ç”Ÿæˆç¼–è¾‘æ“ä½œæ©ç ï¼šä½¿ç”¨åŒç´¢å¼•è¿½è¸ªé€»è¾‘
        # åœ¨Zç©ºé—´ï¼ˆz0ï¼‰éå†ï¼Œæ˜ å°„åˆ°Xç©ºé—´ï¼ˆx_tï¼‰çš„ç¼–è¾‘æ“ä½œ
        # u_maskåœ¨Xç©ºé—´ç”Ÿæˆ: [batch, x_seq_len, 2*vocab_size+1]
        u_mask_x = criterion.make_ut_mask_from_z(z0, z1_token_ids, effective_vocab_size, gap_token, self.tokenizer, x_t)

        # âš ï¸ å…³é”®ä¿®å¤ï¼šå°†u_maskæ‰©å±•åˆ°Zç©ºé—´ä»¥åŒ¹é…log_u_zçš„ç»´åº¦
        # ä½¿ç”¨fill_gap_tokens_with_repeatså°†Xç©ºé—´çš„maskæ‰©å±•åˆ°Zç©ºé—´
        u_mask = fill_gap_tokens_with_repeats(u_mask_x, z_gap_mask, z_pad_mask)

        # è®°å½•æŸå¤±è®¡ç®—ä¸­çš„å…³é”®å˜é‡å€¼ï¼ˆä»…åœ¨debugæ¨¡å¼ä¸‹è®°å½•ï¼‰
        if self.accelerator.is_local_main_process and self.debug_mode:
            context = debug_info.get('context', '') if debug_info else ''
            batch_idx = debug_info.get('batch_idx', 0) if debug_info else 0
            sample_idx = 0

            # è®°å½•æ ‡å‡†ç­”æ¡ˆï¼šz0ã€z1å’Œx_tçš„tokenåºåˆ—
            self.logger.tensor_values(f"GT_z0_batch{batch_idx}", z0[sample_idx],
                                     context=context, level=2, max_elements=50)
            self.logger.tensor_values(f"GT_z1_batch{batch_idx}", z1_token_ids[sample_idx],
                                     context=context, level=2, max_elements=50)
            self.logger.tensor_values(f"GT_x_t_batch{batch_idx}", x_t[sample_idx],
                                     context=context, level=2, max_elements=50)

            # è®°å½•åˆ†è§£åçš„é€Ÿç‡ï¼ˆæ¨¡å‹é¢„æµ‹ï¼‰
            self.logger.tensor_values(f"pred_lambda_ins_batch{batch_idx}", lambda_ins[sample_idx],
                                     context=context, level=2, max_elements=50)
            self.logger.tensor_values(f"pred_lambda_del_batch{batch_idx}", lambda_del[sample_idx],
                                     context=context, level=2, max_elements=50)
            self.logger.tensor_values(f"pred_lambda_sub_batch{batch_idx}", lambda_sub[sample_idx],
                                     context=context, level=2, max_elements=50)

            # è®°å½•æ ‡å‡†ç­”æ¡ˆï¼šu_maskï¼ˆçœŸå®ç¼–è¾‘æ“ä½œæ ‡ç­¾ï¼Œone-hotç¼–ç ï¼‰
            # ä½¿ç”¨ä¸“é—¨çš„æ—¥å¿—æ–¹æ³•æŒ‰è¯­ä¹‰æ‹†åˆ†è®°å½•
            self.logger.log_u_mask_split(f"GT_u_mask", u_mask[sample_idx:sample_idx+1], x_t[sample_idx:sample_idx+1],
                                        effective_vocab_size, context=context, level=2)

            # è§£ç å¹¶è®°å½•Ground Truthç¼–è¾‘æ“ä½œï¼ˆå¯è¯»æ ¼å¼ï¼Œä½¿ç”¨IDï¼‰
            self.logger.log_edit_operations(
                u_mask[sample_idx],
                x_t[sample_idx],
                effective_vocab_size,
                context=context,
                level=2,
                max_ops=20
            )

            # è®°å½•æ¨¡å‹é¢„æµ‹çš„u_cat_xï¼ˆç”¨äºå¯¹æ¯”ï¼‰
            self.logger.tensor_values(f"pred_u_cat_x_batch{batch_idx}_first5pos", u_cat_x[sample_idx, :5, :],
                                     context=context, level=2, max_elements=100)

        # å…³é”®ä¿®å¤ï¼šä¼ å…¥ u_cat_x (Xç©ºé—´) ç”¨äºæ­£ç¡®çš„ u_total è®¡ç®—
        # u_z ä»ç”¨äº cross_entropy è®¡ç®—ï¼ˆç›‘ç£å¸¦è·¯å¾„ç¼–è¾‘æ“ä½œï¼‰
        # ä¼ å…¥ logger ç”¨äºè®°å½•è¯¦ç»†çš„æŸå¤±ç»Ÿè®¡ä¿¡æ¯
        loss = criterion(u_cat_x, u_z, u_mask, effective_vocab_size,
                        accelerator=self.accelerator, logger=self.logger)

        return loss

    def train_epoch(self, model, condition_encoder, criterion, optimizer, dataloader, dataset, epoch, dimension):
        model.train()
        condition_encoder.train()

        total_loss = 0.0
        num_batches = 0

        # æ˜¾ç¤ºè¿›åº¦æ¡ - åªåœ¨ä¸»è¿›ç¨‹æ˜¾ç¤º
        progress_bar = tqdm(dataloader, desc=f"Epoch {epoch+1}/{self.args.num_epochs} - Dim {dimension}",
                          disable=not self.accelerator.is_local_main_process)

        # åªåœ¨ä¸»è¿›ç¨‹è®¾ç½®åˆå§‹è¿›åº¦æ¡æ˜¾ç¤º
        if self.accelerator.is_local_main_process:
            progress_bar.set_postfix({'loss': '0.0000', 'grad_norm': '0.000'})

        for batch_idx, batch in enumerate(progress_bar):
            batch_start_time = time.time()

            # ä½¿ç”¨ Accelerate çš„æ¢¯åº¦ç´¯ç§¯ä¸Šä¸‹æ–‡ç®¡ç†å™¨
            # è‡ªåŠ¨å¤„ç†æ¢¯åº¦åŒæ­¥ã€ç´¯ç§¯æ­¥æ•°åˆ¤æ–­ã€ä¼˜åŒ–å™¨æ›´æ–°
            if self.accelerator.is_local_main_process and self.debug_mode:
                self.logger.log("BATCH_START", f"å¼€å§‹å¤„ç† Batch {batch_idx} | timestamp={time.time():.2f}",
                                f"ç»´åº¦{dimension}_batch{batch_idx}", level=2)

            with self.accelerator.accumulate([model, condition_encoder]):
                data_load_start = time.time()
                x_values = batch['x_values'].to(self.device)
                y_target = batch['y_target'].to(self.device)  # ä¿®æ”¹ï¼šä½¿ç”¨y_targetè€Œéresiduals
                z0_token_ids = batch['z0_token_ids'].to(self.device)
                z1_token_ids = batch['z1_token_ids'].to(self.device)
                data_load_time = time.time() - data_load_start

                if self.accelerator.is_local_main_process and self.debug_mode:
                    self.logger.log("DATA_LOAD", f"æ•°æ®åŠ è½½å®Œæˆ | è€—æ—¶={data_load_time:.3f}s",
                                    f"ç»´åº¦{dimension}_batch{batch_idx}", level=2)

                # ç§»é™¤è¿‡åº¦çš„tokenè§£ç æ—¥å¿—ä»¥æé«˜æ€§èƒ½

                point_mask = batch['point_mask'].to(self.device) if 'point_mask' in batch else None

                condition_start = time.time()
                # å…³é”®ä¿®æ”¹ï¼šä½¿ç”¨y_targetä½œä¸ºæ¡ä»¶è€Œéresidualsï¼ˆæ¶æ„æ”¹è¿›ï¼‰
                condition_embeddings = condition_encoder(x_values, y_target, point_mask)
                condition_time = time.time() - condition_start

                if self.accelerator.is_local_main_process and self.debug_mode:
                    self.logger.log("CONDITION_ENCODE", f"æ¡ä»¶ç¼–ç å®Œæˆ | è€—æ—¶={condition_time:.3f}s",
                                    f"ç»´åº¦{dimension}_batch{batch_idx}", level=2)

                # è®°å½•è¾“å…¥æ•°æ®çš„å®Œæ•´å€¼ï¼ˆä»…åœ¨debugæ¨¡å¼ä¸‹è®°å½•ï¼‰
                if self.accelerator.is_local_main_process and self.debug_mode:
                    context = f'ç»´åº¦{dimension}'
                    self.logger.tensor_values(f"x_values_batch{batch_idx}", x_values[0],
                                             context=context, level=2, max_elements=50)
                    self.logger.tensor_values(f"y_target_batch{batch_idx}", y_target[0],
                                             context=context, level=2, max_elements=50)

                # å‡†å¤‡è°ƒè¯•ä¿¡æ¯ï¼ˆæ¯ä¸ªbatchéƒ½ä¼ é€’ï¼‰
                debug_info = {
                    'batch_idx': batch_idx,
                    'context': f'ç»´åº¦{dimension}'
                }

                forward_start = time.time()
                forward_results = self.forward_pass(model, condition_embeddings, z0_token_ids, z1_token_ids, debug_info)
                forward_time = time.time() - forward_start

                if self.accelerator.is_local_main_process and self.debug_mode:
                    self.logger.log("FORWARD_PASS", f"å‰å‘ä¼ æ’­å®Œæˆ | è€—æ—¶={forward_time:.3f}s",
                                    f"ç»´åº¦{dimension}_batch{batch_idx}", level=2)

                # åˆ†å¸ƒå¼å¥åº·æ£€æŸ¥ï¼šè®°å½•å‰å‘ä¼ æ’­ä¸­çš„NaNï¼ˆä»…ç”¨äºç›‘æ§ï¼‰
                nan_check_start = time.time()
                if self.accelerator.distributed_type != "NO":
                    pred_rates = forward_results['pred_rates']

                    # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•è¿›ç¨‹çš„æ¨¡å‹è¾“å‡ºåŒ…å«NaN
                    local_has_nan = torch.isnan(pred_rates).any().float()
                    gathered_nan_results = self.accelerator.gather(local_has_nan)
                    global_has_nan = gathered_nan_results.sum()

                    if global_has_nan.item() > 0:
                        if self.accelerator.is_local_main_process:
                            self.logger.error("FORWARD_NAN", f"ç»´åº¦{dimension} æ£€æµ‹åˆ°å‰å‘ä¼ æ’­NaN", f"batch_idx:{batch_idx}")
                nan_check_time = time.time() - nan_check_start

                loss_compute_start = time.time()
                # âœ… ä¸å†æ‰‹åŠ¨é™¤ä»¥ gradient_accumulation_stepsï¼Œaccelerator.accumulate ä¼šè‡ªåŠ¨å¤„ç†
                loss = self.compute_loss(forward_results, criterion, debug_info)
                loss_compute_time = time.time() - loss_compute_start

                # è®°å½•æŸå¤±å€¼ï¼ˆä»…åœ¨debugæ¨¡å¼ä¸‹è®°å½•è¯¦ç»†ä¿¡æ¯ï¼‰
                if self.accelerator.is_local_main_process and self.debug_mode:
                    self.logger.log("LOSS_COMPUTED", f"loss={loss.item():.6f} | è€—æ—¶={loss_compute_time:.3f}s | NaNæ£€æŸ¥è€—æ—¶={nan_check_time:.3f}s",
                                    f"ç»´åº¦{dimension}_batch{batch_idx}", level=2)

                grad_norm = 0.0
                # ä½¿ç”¨ Accelerate çš„ backward è€Œä¸æ˜¯ç›´æ¥è°ƒç”¨ loss.backward()
                if self.accelerator.is_local_main_process and self.debug_mode:
                    self.logger.log("BACKWARD_START", f"å¼€å§‹åå‘ä¼ æ’­ | loss={loss.item():.6f} | timestamp={time.time():.2f}",
                                    f"ç»´åº¦{dimension}_batch{batch_idx}", level=2)

                backward_start = time.time()
                try:
                    self.accelerator.backward(loss)
                    backward_time = time.time() - backward_start
                    if self.accelerator.is_local_main_process and self.debug_mode:
                        self.logger.log("BACKWARD_SUCCESS", f"åå‘ä¼ æ’­æˆåŠŸ | è€—æ—¶={backward_time:.3f}s",
                                        f"ç»´åº¦{dimension}_batch{batch_idx}", level=2)
                except Exception as e:
                    # è®°å½•åå‘ä¼ æ’­å´©æºƒä¿¡æ¯
                    self.logger.log_crash(
                        step_name="BACKWARD",
                        batch_idx=batch_idx,
                        dimension=dimension,
                        error=e,
                        extra_info=f"loss={loss.item():.6f}"
                    )
                    raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ä»¥ç»ˆæ­¢è®­ç»ƒ

                # ä¸å†éœ€è¦åˆ¤æ–­æ˜¯å¦æ˜¯æœ€åä¸€æ­¥ï¼Œå› ä¸º accumulate ä¼šè‡ªåŠ¨å¤„ç†
                all_params = list(model.parameters()) + list(condition_encoder.parameters())

                # ğŸ”§ ä¿®å¤ï¼šéµå¾ª Accelerate å®˜æ–¹æ–‡æ¡£ï¼Œåœ¨ accumulate å†…æ— æ¡ä»¶è°ƒç”¨ optimizer
                # å‚è€ƒ: https://huggingface.co/docs/accelerate/usage_guides/gradient_accumulation
                # Accelerate ä¼šè‡ªåŠ¨åœ¨æ­£ç¡®çš„æ—¶æœºï¼ˆåŸºäº gradient_accumulation_stepsï¼‰æ‰§è¡Œæ›´æ–°
                # æˆ‘ä»¬ä¸éœ€è¦æ‰‹åŠ¨æ£€æŸ¥ sync_gradients

                # åº”ç”¨æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸å’Œæ¶ˆå¤±ï¼‰
                # âš ï¸ å…³é”®ä¿®å¤ï¼šåœ¨optimizer.step()ä¹‹å‰è£å‰ªæ¢¯åº¦
                self.accelerator.clip_grad_norm_(all_params, self.GRADIENT_CLIP_NORM)

                # è®¡ç®—æ¢¯åº¦èŒƒæ•°ï¼ˆç”¨äºç›‘æ§ï¼Œä¸å½±å“è®­ç»ƒï¼‰
                grad_norm = 0.0
                for param in all_params:
                    if param.grad is not None:
                        grad_norm += float(param.grad.data.norm().item() ** 2)
                grad_norm = float(grad_norm ** 0.5)

                # æ— æ¡ä»¶æ‰§è¡Œä¼˜åŒ–å™¨æ›´æ–°ï¼ˆAccumulate ä¼šè‡ªåŠ¨å¤„ç†ï¼‰
                optimizer.step()
                optimizer.zero_grad()

                total_loss += loss.item()
                num_batches += 1

                batch_total_time = time.time() - batch_start_time

                # æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤ºï¼ˆæ¯ä¸ªbatchéƒ½æ›´æ–°ï¼‰
                if self.accelerator.is_local_main_process:
                    postfix_dict = {
                        'loss': f'{loss.item():.4f}',
                        'grad_norm': f'{grad_norm:.3f}' if isinstance(grad_norm, (int, float)) else f'{grad_norm:.3f}',
                        'time': f'{batch_total_time:.2f}s' if self.debug_mode else ''
                    }
                    progress_bar.set_postfix(postfix_dict)

                # accumulate ä¸Šä¸‹æ–‡ç®¡ç†å™¨å³å°†é€€å‡ºï¼Œè®°å½•batchå®Œæˆ
                if self.accelerator.is_local_main_process and self.debug_mode:
                    self.logger.log("BATCH_COMPLETE", f"Batch {batch_idx} å®Œæˆ | æ€»è€—æ—¶={batch_total_time:.3f}s | timestamp={time.time():.2f}",
                                    f"ç»´åº¦{dimension}_batch{batch_idx}", level=2)

        # ç­‰å¾…æ‰€æœ‰è¿›ç¨‹å®Œæˆ
        # è·¨è¿›ç¨‹æ”¶é›†å¹¶è®¡ç®—å¹³å‡æŸå¤±
        avg_loss = self._gather_average_loss(total_loss, num_batches, default_value=0.0)

        return avg_loss, num_batches

    def evaluate(self, model, condition_encoder, criterion, test_dataloader, test_dataset):
        """æµ‹è¯•é›†è¯„ä¼°"""
        model.eval()
        condition_encoder.eval()

        total_loss = 0.0
        num_batches = 0

        with torch.no_grad():
            # === ä¿®æ”¹ï¼šä¸å†å¾ªç¯ dimï¼Œç›´æ¥éå† dataloader ===
            for batch in test_dataloader:
                x_values = batch['x_values'].to(self.device)
                y_target = batch['y_target'].to(self.device)  # ä¿®æ”¹ï¼šä½¿ç”¨y_target
                z0_token_ids = batch['z0_token_ids'].to(self.device)
                z1_token_ids = batch['z1_token_ids'].to(self.device)
                point_mask = batch['point_mask'].to(self.device)

                # ä¿®æ”¹ï¼šä½¿ç”¨y_targetè€Œéresiduals
                condition_embeddings = condition_encoder(x_values, y_target, point_mask)
                forward_results = self.forward_pass(model, condition_embeddings, z0_token_ids, z1_token_ids)

                # è®¡ç®—æŸå¤±
                loss = self.compute_loss(forward_results, criterion)
                total_loss += loss.item()
                num_batches += 1

        # ç­‰å¾…æ‰€æœ‰è¿›ç¨‹å®Œæˆ
        # è·¨è¿›ç¨‹æ”¶é›†å¹¶è®¡ç®—å¹³å‡æŸå¤±
        avg_loss = self._gather_average_loss(total_loss, num_batches, default_value=float('inf'))

        return avg_loss


    def save_checkpoint(self, model, condition_encoder, loss, epoch, is_final=False):
        # ç­‰å¾…æ‰€æœ‰è¿›ç¨‹åŒæ­¥
        self.accelerator.wait_for_everyone()

        # åˆ›å»ºcheckpointç›®å½•
        checkpoint_dir = os.path.join(
            self.args.save_dir,
            "continuous_flow_final" if is_final else f"checkpoint_epoch_{epoch+1}"
        )
        os.makedirs(checkpoint_dir, exist_ok=True)

        # ä½¿ç”¨ Accelerate çš„ save_state æ–¹æ³•ï¼ˆæ¨èçš„æ­£ç¡®æ–¹å¼ï¼‰
        self.accelerator.save_state(checkpoint_dir)

        # å¦å¤–ä¿å­˜æ¨¡å‹é…ç½®ä¿¡æ¯
        if self.accelerator.is_local_main_process:
            unwrapped_model = self.accelerator.unwrap_model(model)
            unwrapped_encoder = self.accelerator.unwrap_model(condition_encoder)

            # ä» model ä¸­æå–é…ç½®ä¿¡æ¯
            model_config = {
                'vocab_size': unwrapped_model.vocab_size,
                'hidden_dim': unwrapped_model.hidden_dim,
                'n_layers': unwrapped_model.n_layers,
                'n_heads': unwrapped_model.n_heads,
                'condition_dim': unwrapped_model.condition_dim,
                'dropout': unwrapped_model.dropout,
                'max_seq_len': unwrapped_model.max_seq_len,
                'use_condition_injection': unwrapped_model.use_condition_injection,
            }

            config_data = {
                'epoch': epoch + 1,
                'model_state_dict': unwrapped_model.state_dict(),
                'condition_encoder_state_dict': unwrapped_encoder.state_dict(),
                'loss': loss,
                'model_config': model_config,
                'args': self.args,
                'accelerate_config': {
                    'distributed_type': str(self.accelerator.distributed_type),
                    'num_processes': self.accelerator.num_processes,
                    'mixed_precision': str(self.accelerator.mixed_precision),
                }
            }

            # ä¿å­˜é…ç½®ä¿¡æ¯
            config_path = os.path.join(checkpoint_dir, "training_config.json")
            torch.save(config_data, config_path)

        return checkpoint_dir

    def train(self):
        # æ£€æŸ¥æ£€æŸ¥ç‚¹å¹¶åŠ è½½æ¨¡å‹
        checkpoint_path = find_latest_checkpoint(self.args)
        if self.accelerator.is_local_main_process:
            print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
            print(f"{'æ‰¾åˆ°æ£€æŸ¥ç‚¹' if checkpoint_path else 'æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹ï¼Œå°†ä»åŸºç¡€æ¨¡å‹å¼€å§‹è®­ç»ƒ'}: {checkpoint_path or ''}")

        model, condition_encoder, criterion, optimizer, scheduler, tokenizer = self.setup_models(checkpoint_path=checkpoint_path)

        # æ³¨æ„è¿™é‡Œæ¥æ”¶è¿”å›å€¼çš„å˜åŒ–
        train_dataloader, train_dataset, test_dataloader, test_dataset = self.prepare_data(tokenizer)

        model_params = sum(p.numel() for p in model.parameters())
        encoder_params = sum(p.numel() for p in condition_encoder.parameters() if p.requires_grad)
        if self.accelerator.is_local_main_process:
            print(f"æ¨¡å‹å‚æ•°æ•°é‡: {model_params:,}, æ¡ä»¶ç¼–ç å™¨å‚æ•°æ•°é‡: {encoder_params:,}")
            print(f"å¼€å§‹è¿ç»­æµè®­ç»ƒ ({self.args.num_epochs} epochs)...")
            # è®°å½•è®­ç»ƒå¼€å§‹åˆ° training.log
            self.logger.log("TRAINING_START", f"å¼€å§‹è®­ç»ƒ | num_epochs={self.args.num_epochs} | model_params={model_params:,} | encoder_params={encoder_params:,}", level=1)

        eval_every = self.args.eval_every

        for epoch in range(self.args.num_epochs):
            # === ä¿®æ”¹å¼€å§‹ï¼šä¸å†å¾ªç¯ dimï¼Œç›´æ¥ä¼ æ•´ä¸ª dataloader ===
            # è¿™é‡Œä¼ å…¥ "Mixed" ä½œä¸ºç»´åº¦åç§°ä»…ç”¨äºæ˜¾ç¤º
            avg_loss, num_batches = self.train_epoch(
                model, condition_encoder, criterion, optimizer,
                train_dataloader, train_dataset, epoch, "Mixed"
            )

            if self.accelerator.is_local_main_process:
                # è·å–å½“å‰å­¦ä¹ ç‡
                current_lr = optimizer.param_groups[0]['lr']
                print(f"Epoch {epoch+1}/{self.args.num_epochs} å®Œæˆ, è®­ç»ƒæŸå¤±: {avg_loss:.4f}, å­¦ä¹ ç‡: {current_lr:.2e}")
                # è®°å½•è®­ç»ƒæˆæœåˆ° training.log
                self.logger.log("EPOCH_COMPLETE", f"Epoch {epoch+1}/{self.args.num_epochs} | train_loss={avg_loss:.4f} | lr={current_lr:.2e} | batches={num_batches}", level=1)

            # æ›´æ–°å­¦ä¹ ç‡
            scheduler.step()

            # ä¿®æ”¹ evaluate è°ƒç”¨ï¼Œä¼ å…¥å•ä¸ª dataloader
            if (epoch + 1) % eval_every == 0 or epoch == self.args.num_epochs - 1:
                test_loss = self.evaluate(model, condition_encoder, criterion, test_dataloader, test_dataset)
                if self.accelerator.is_local_main_process:
                    print(f"æµ‹è¯•é›†æŸå¤±: {test_loss:.4f}")
                    # è®°å½•è¯„ä¼°ç»“æœåˆ° training.log
                    self.logger.log("EVALUATION", f"Epoch {epoch+1}/{self.args.num_epochs} | test_loss={test_loss:.4f}", level=1)

            # ä¿å­˜æ£€æŸ¥ç‚¹
            if (epoch + 1) % self.args.save_every == 0:
                checkpoint_path = self.save_checkpoint(
                    model, condition_encoder, avg_loss, epoch
                )
                if self.accelerator.is_local_main_process:
                    print(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜åˆ°: {checkpoint_path}")
                    # è®°å½•æ£€æŸ¥ç‚¹ä¿å­˜åˆ° training.log
                    self.logger.log("CHECKPOINT_SAVED", f"Epoch {epoch+1}/{self.args.num_epochs} | path={checkpoint_path} | train_loss={avg_loss:.4f}", level=1)

        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_path = self.save_checkpoint(
            model, condition_encoder, avg_loss, self.args.num_epochs - 1, is_final=True
        )
        if self.accelerator.is_local_main_process:
            print(f"æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜åˆ°: {final_path}")
            # è®°å½•è®­ç»ƒå®Œæˆåˆ° training.log
            self.logger.log("TRAINING_COMPLETE", f"è®­ç»ƒå®Œæˆ | final_path={final_path} | final_train_loss={avg_loss:.4f} | total_epochs={self.args.num_epochs}", level=1)

        return model, condition_encoder

    def symbolic_regression(self, model_path, x_data, y_data, n_steps=100, input_dim=None, max_expr_length=None, initial_expr=None):
        """ç¬¦å·å›å½’ - ä½¿ç”¨ç®€å•æ¨ç†(è´ªå©ªæœç´¢)æ¥æ”¶æ•°æ®ç‚¹å¯¹ï¼Œè¾“å‡ºè¡¨è¾¾å¼

        Args:
            model_path: æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„
            x_data: è¾“å…¥xæ•°æ®
            y_data: ç›®æ ‡yæ•°æ®
            n_steps: æ¨ç†æ­¥æ•°
            input_dim: è¾“å…¥ç»´åº¦ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨æ¨æ–­
            max_expr_length: è¡¨è¾¾å¼æœ€å¤§tokené•¿åº¦ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨argsä¸­çš„å€¼
            initial_expr: åˆå§‹è¡¨è¾¾å¼ï¼ˆsympyè¡¨è¾¾å¼æˆ–å­—ç¬¦ä¸²ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨x0
        """
        # è®°å½•å¼€å§‹
        self.logger.log("SYMBOLIC_REGRESSION_START",
                       f"è¾“å…¥æ•°æ®: xå½¢çŠ¶={x_data.shape}, yå½¢çŠ¶={y_data.shape} | n_steps={n_steps}",
                       "inference", level=1)

        # åŠ è½½æ¨¡å‹
        checkpoint_path = model_path if model_path and os.path.exists(model_path) else None
        if checkpoint_path:
            self.logger.log("MODEL_LOAD", f"ä½¿ç”¨æ£€æŸ¥ç‚¹: {checkpoint_path}", "inference", level=3)
        else:
            if self.accelerator.is_local_main_process:
                print("\n" + "="*60)
                print("âš ï¸  è­¦å‘Šï¼šæœªæ‰¾åˆ°æ£€æŸ¥ç‚¹ï¼")
                print("="*60)
                print("æ¨¡å‹å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æƒé‡è¿›è¡Œæ¨ç†ã€‚")
                print("è¿™ä¼šå¯¼è‡´æ¨ç†è´¨é‡å¾ˆå·®ï¼Œå¯èƒ½é™·å…¥æ— é™å¾ªç¯ã€‚")
                print("\nå»ºè®®æ“ä½œï¼š")
                print("1. å…ˆè®­ç»ƒæ¨¡å‹ï¼špython train.py --num_epochs 30")
                print("2. æˆ–æŒ‡å®šå·²æœ‰æ£€æŸ¥ç‚¹ï¼špython example.py --model_path checkpoints/your_checkpoint")
                print("="*60 + "\n")
            self.logger.log("MODEL_LOAD", "âš ï¸ æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–æƒé‡ï¼ˆè­¦å‘Šï¼šæ¨ç†è´¨é‡ä¼šå¾ˆå·®ï¼‰", "inference", level=3)

        model, condition_encoder, _, _, _, tokenizer = self.setup_models(checkpoint_path=checkpoint_path)

        # è®¾ç½®è®¾å¤‡å’Œæ¨¡å¼
        device = self.device
        model.eval()
        condition_encoder.eval()

        # å‡†å¤‡è¾“å…¥æ•°æ®
        x_values = torch.FloatTensor(x_data).unsqueeze(0).to(device)
        y_values = torch.FloatTensor(y_data).unsqueeze(0).to(device)  # è¿™æ˜¯ç›®æ ‡å€¼

        # æ¨æ–­è¾“å…¥ç»´åº¦å¹¶ç”Ÿæˆåˆå§‹è¡¨è¾¾å¼
        if input_dim is None:
            input_dim = x_data.shape[1] if len(x_data.shape) > 1 else 1

        # è®¡ç®—åˆå§‹æ®‹å·® (çœŸå®å€¼ - åˆå§‹è¡¨è¾¾å¼çš„é¢„æµ‹å€¼)
        import sympy as sp
        from ..symbolic.symbolic_utils import evaluate_expression_safe, expr_to_tree

        # å¤„ç†åˆå§‹è¡¨è¾¾å¼
        if initial_expr is None:
            initial_expr = sp.Symbol('x0')
            # å°†sympyè¡¨è¾¾å¼è½¬æ¢ä¸ºå‰ç¼€è¡¨è¾¾å¼tokens
            initial_expr_str = expr_to_tree(initial_expr)
            current_tokens = initial_expr_str.split(',') if initial_expr_str else ['x0']
        elif isinstance(initial_expr, str):
            # å°†å­—ç¬¦ä¸²è¡¨è¾¾å¼è½¬æ¢ä¸ºsympyè¡¨è¾¾å¼
            initial_expr = sp.sympify(initial_expr)
            # å°†sympyè¡¨è¾¾å¼è½¬æ¢ä¸ºå‰ç¼€è¡¨è¾¾å¼tokens
            initial_expr_str = expr_to_tree(initial_expr)
            current_tokens = initial_expr_str.split(',') if initial_expr_str else ['x0']
        elif isinstance(initial_expr, list):
            # ç›´æ¥ä¼ å…¥tokenåˆ—è¡¨ï¼ˆç”¨äºä»è®­ç»ƒæ•°æ®ä¸­æ¢å¤ï¼‰
            current_tokens = initial_expr
            # å°è¯•å°†tokensè½¬æ¢å›sympyè¡¨è¾¾å¼ï¼ˆç”¨äºè®¡ç®—é¢„æµ‹å€¼ï¼‰
            # æ³¨æ„ï¼šè¿™ä¸ªè½¬æ¢å¯èƒ½å¤±è´¥ï¼Œå› ä¸ºtokenizerä¸æ”¯æŒå®Œå…¨åŒå‘è½¬æ¢
            # æˆ‘ä»¬å…ˆå°è¯•è¯„ä¼°ï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨é»˜è®¤å€¼
            initial_expr = None  # æ ‡è®°ä¸ºéœ€è¦ç‰¹æ®Šå¤„ç†

        if self.accelerator.is_local_main_process:
            print(f"åˆå§‹è¡¨è¾¾å¼: {initial_expr}")
            print(f"åˆå§‹tokens: {current_tokens}")

        # è®¡ç®—åˆå§‹è¡¨è¾¾å¼åœ¨x_dataä¸Šçš„é¢„æµ‹å€¼
        if initial_expr is not None:
            success, y_pred = evaluate_expression_safe(initial_expr, x_data)
        else:
            # å¦‚æœæ— æ³•ä»tokensæ¢å¤è¡¨è¾¾å¼ï¼Œä½¿ç”¨å¸¸é‡0ä½œä¸ºåˆå§‹é¢„æµ‹
            # è¿™ä¸æ˜¯æœ€ä¼˜è§£ï¼Œä½†å¯ä»¥é¿å…ç¨‹åºå´©æºƒ
            success = False
            y_pred = [0.0] * len(y_data)

        if not success:
            self.logger.log("INITIAL_EXPR_WARN", f"æ— æ³•è®¡ç®—åˆå§‹è¡¨è¾¾å¼çš„é¢„æµ‹å€¼ï¼Œä½¿ç”¨é›¶åˆå§‹åŒ–", "inference", level=1)
            # ç»§ç»­æ‰§è¡Œï¼Œä¸è¿”å›ï¼Œå› ä¸ºæ¡ä»¶ç¼–ç å™¨ä½¿ç”¨çš„æ˜¯y_targetè€Œéæ®‹å·®

        # è®¡ç®—æ®‹å·®ï¼šçœŸå®å€¼ - é¢„æµ‹å€¼ï¼ˆä»…ç”¨äºè¯„ä¼°ï¼Œä¸ä½œä¸ºæ¡ä»¶ï¼‰
        residuals = y_values - torch.FloatTensor(y_pred).unsqueeze(0).to(device)

        # å…³é”®ä¿®æ”¹ï¼šä½¿ç”¨ç›®æ ‡å€¼y_valuesä½œä¸ºæ¡ä»¶ï¼Œè€Œéæ®‹å·®
        # è¿™æ ·æ¡ä»¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¿æŒæ’å®šï¼Œä½œä¸º"åŒ—ææ˜Ÿ"æŒ‡å¼•æ–¹å‘
        point_mask = torch.ones_like(y_values)
        condition = condition_encoder(x_values, y_values, point_mask)

        # è®°å½•åˆå§‹æ•°æ®
        self.logger.log("INITIAL_DATA",
                       f"x_values: shape={x_values.shape} range=[{x_values.min():.4f},{x_values.max():.4f}] | "
                       f"y_target: shape={y_values.shape} range=[{y_values.min():.4f},{y_values.max():.4f}] | "
                       f"residuals: shape={residuals.shape} range=[{residuals.min():.4f},{residuals.max():.4f}] | "
                       f"initial_expr: {initial_expr} | initial_tokens: {current_tokens}",
                       "inference", level=1)
        self.logger.log("ARCHITECTURE_INFO",
                       "ä½¿ç”¨ç›®æ ‡å€¼y_targetä½œä¸ºæ¡ä»¶ï¼ˆæ¶æ„æ”¹è¿›ï¼šåŒ—ææ˜Ÿæ¨¡å¼ï¼‰",
                       "inference", level=1)
        self.logger.log("INITIAL_CONDITION",
                       f"condition: shape={condition.shape} range=[{condition.min():.4f},{condition.max():.4f}]",
                       "inference", level=1)

        # æ‰“å°æ¡ä»¶åµŒå…¥çš„å‰10ä¸ªç»´åº¦çš„å…·ä½“å€¼
        condition_cpu = condition.cpu().squeeze(0)
        condition_values = condition_cpu.detach().numpy()
        # å¤„ç†åºåˆ—æ ¼å¼ (num_seeds, dim_hidden) æˆ–å‘é‡æ ¼å¼ (dim_hidden,)
        if condition_values.ndim == 2:
            condition_preview = condition_values.flatten()[:10]  # å±•å¹³åå–å‰10ä¸ª
        else:
            condition_preview = condition_values[:10]
        self.logger.log("INITIAL_CONDITION_VALUES",
                       f"conditionå‰10ç»´: [{', '.join([f'{float(v):.6f}' for v in condition_preview])}]",
                       "inference", level=1)

        # åˆ›å»ºç®€å•æ¨ç†å™¨
        self.logger.log("SIMPLE_SEARCH_INIT", f"åˆå§‹åŒ–ç®€å•æ¨ç†å™¨ | n_steps={n_steps}", "inference", level=3)

        # è§£æaction_thresholdså‚æ•°
        action_thresholds = getattr(self.args, 'action_thresholds', None)
        if action_thresholds:
            try:
                action_thresholds = [float(x.strip()) for x in action_thresholds.split(',')]
                self.logger.log("ACTION_THRESHOLDS_CONFIG",
                               f"ä½¿ç”¨å¤šé˜ˆå€¼æ¨ç†æ¨¡å¼ | thresholds={action_thresholds}",
                               "inference", level=1)
                if self.accelerator.is_local_main_process:
                    print(f"\nä½¿ç”¨å¤šé˜ˆå€¼æ¨ç†æ¨¡å¼ï¼Œé˜ˆå€¼: {action_thresholds}")
            except ValueError as e:
                self.logger.log("ACTION_THRESHOLDS_PARSE_ERROR",
                               f"æ— æ³•è§£æaction_thresholdså‚æ•°: {action_thresholds} | error={e}",
                               "inference", level=1)
                if self.accelerator.is_local_main_process:
                    print(f"\nâš ï¸ è­¦å‘Š: æ— æ³•è§£æaction_thresholdså‚æ•° '{action_thresholds}'ï¼Œå›é€€åˆ°å•æœ€ä½³æ“ä½œæ¨¡å¼")
                action_thresholds = None

        simple_searcher = SimpleSymbolicRegression(
            model=model,
            condition_encoder=condition_encoder,
            tokenizer=tokenizer,
            device=device,
            args=self.args,
            logger=self.logger,
            min_action_score=self.MIN_ACTION_SCORE,
            max_expression_length=self.MAX_EXPRESSION_LENGTH,
            numerical_clip_threshold=self.NUMERICAL_CLIP_THRESHOLD,
            action_thresholds=action_thresholds
        )

        # æ‰§è¡Œæ¨ç†ï¼ˆå•é˜ˆå€¼æˆ–å¤šé˜ˆå€¼æ¨¡å¼ï¼‰
        initial_residuals_np = residuals.cpu().squeeze(0).numpy()

        # æ ¹æ®æ˜¯å¦å¯ç”¨å¤šé˜ˆå€¼æ¨¡å¼é€‰æ‹©æ¨ç†æ–¹æ³•
        if simple_searcher.use_multi_threshold:
            # å¤šé˜ˆå€¼æ¨ç†æ¨¡å¼
            if self.accelerator.is_local_main_process:
                print(f"\næ‰§è¡Œå¤šé˜ˆå€¼æ¨ç†...")

            results_dict = simple_searcher.multi_threshold_search(
                initial_tokens=current_tokens,
                initial_condition=condition,
                initial_residuals=initial_residuals_np,
                x_data=x_data,
                y_data=y_data,
                x_values=x_values,
                n_steps=n_steps
            )

            # è¿”å›æ‰€æœ‰ç»“æœçš„è¡¨è¾¾å¼å­—å…¸
            if self.accelerator.is_local_main_process:
                print(f"\nå¤šé˜ˆå€¼æ¨ç†å®Œæˆï¼Œè¿”å› {len(results_dict)} ä¸ªå€™é€‰ç»“æœ")

                # è®°å½•æ‰€æœ‰ç»“æœåˆ°æ—¥å¿—
                for threshold, candidate in results_dict.items():
                    expr_str = ','.join(candidate.tokens) if candidate and candidate.tokens else ""
                    mse_str = f'{candidate.mse_score:.6f}' if candidate.mse_score is not None else 'N/A'
                    self.logger.log("MULTI_THRESHOLD_RESULT",
                                   f"threshold={threshold} | expression={expr_str} | MSE={mse_str}",
                                   "inference", level=1)

            # è¿”å›å­—å…¸æ ¼å¼çš„ç»“æœï¼š{threshold: expression}
            return {threshold: ','.join(candidate.tokens) if candidate and candidate.tokens else ""
                    for threshold, candidate in results_dict.items()}

        else:
            # å•æœ€ä½³æ“ä½œæ¨¡å¼ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
            if self.accelerator.is_local_main_process:
                print(f"\næ‰§è¡Œå•æœ€ä½³æ“ä½œæ¨ç†...")

            best_candidate = simple_searcher.greedy_search(
                initial_tokens=current_tokens,
                initial_condition=condition,
                initial_residuals=initial_residuals_np,
                x_data=x_data,
                y_data=y_data,
                x_values=x_values,
                n_steps=n_steps
            )

            # è¿”å›æœ€ä½³å€™é€‰çš„è¡¨è¾¾å¼
            final_expression = ','.join(best_candidate.tokens) if best_candidate and best_candidate.tokens else ""

            if best_candidate and self.accelerator.is_local_main_process:
                # è®°å½•MSEåˆ†æ•°
                mse_score = best_candidate.mse_score
                mse_str = f'{mse_score:.6f}' if mse_score is not None else 'N/A'
                self.logger.log("SIMPLE_SEARCH_RESULT",
                               f"MSEåˆ†æ•°: {mse_str} | "
                               f"æ“ä½œå†å²: {' -> '.join(best_candidate.history[-5:]) if best_candidate.history else 'N/A'}",
                               "inference", level=3)

            self.logger.log("INFERENCE_COMPLETE", f"æœ€ç»ˆè¡¨è¾¾å¼: {final_expression}", "inference", level=3)
            return final_expression

