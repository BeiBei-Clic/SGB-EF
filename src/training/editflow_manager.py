"""
EditFlowè¿­ä»£ä¼˜åŒ–è®­ç»ƒå™¨ - å®ç°åŸºäºè¿­ä»£å¼ç¼–è¾‘æ“ä½œçš„ç¬¦å·å›å½’æ¨¡å‹è®­ç»ƒ
ä½¿ç”¨ Hugging Face Accelerate è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒåŠ é€Ÿ

é‡æ„è¯´æ˜ï¼šEditFlowManager ç°åœ¨ä½œä¸ºåè°ƒè€…ï¼Œå§”æ‰˜å…·ä½“ä»»åŠ¡ç»™ï¼š
- EditFlowTrainer: è´Ÿè´£è®­ç»ƒå¾ªç¯å’Œè¯„ä¼°
- InferenceEngine: è´Ÿè´£ç¬¦å·å›å½’æ¨ç†
"""

import os
import time

import torch
import torch.nn.functional as F
import numpy as np
from tqdm import tqdm
from accelerate import Accelerator
from accelerate.utils import set_seed

from ..symbolic.data_generator import generate_flow_samples
from .flow import (
    remove_gap_tokens, fill_gap_tokens_with_repeats,
    ContinuousFlowLoss, prepare_dataset_hf, custom_collate_fn
)
from ..modeling.condition_encoder import SetTransformerConditionEncoder
from ..modeling.llama_editflow import LlamaEditFlowBackbone
from ..utils.misc_utils import find_latest_checkpoint, load_checkpoint
from ..utils.logger import Logger
from .greedy_search import SimpleSymbolicRegression

# æ–°å¯¼å…¥ï¼šè®­ç»ƒå™¨å’Œæ¨ç†å¼•æ“
from .trainers.editflow_trainer import EditFlowTrainer
from .inference.inference_engine import InferenceEngine


class EditFlowManager:
    """EditFlowæ¨¡å‹ç®¡ç†å™¨ - åè°ƒè®­ç»ƒå’Œæ¨ç†åŠŸèƒ½

    é‡æ„åçš„èŒè´£ï¼š
    - æ•°æ®å‡†å¤‡å’Œç®¡ç†
    - æ¨¡å‹è®¾ç½®å’Œæ£€æŸ¥ç‚¹ç®¡ç†
    - è®­ç»ƒæµç¨‹åè°ƒï¼ˆå§”æ‰˜ç»™ EditFlowTrainerï¼‰
    - æ¨ç†æµç¨‹åè°ƒï¼ˆå§”æ‰˜ç»™ InferenceEngineï¼‰

    æ¶æ„ç‰¹ç‚¹ï¼šè¿­ä»£ä¼˜åŒ–æ¨¡å¼
    - æ¨¡å‹ç›´æ¥é¢„æµ‹ä»z0åˆ°z1çš„ç¼–è¾‘æ“ä½œï¼ˆæ’å…¥ã€åˆ é™¤ã€æ›¿æ¢ï¼‰
    - æ—¶é—´æ­¥å›ºå®šä¸º0ï¼Œå­¦ä¹ ä»èµ·ç‚¹åˆ°ç›®æ ‡çš„ç›´æ¥ç¼–è¾‘è·¯å¾„
    - ä½¿ç”¨ç›®æ ‡å€¼y_targetä½œä¸ºæ¡ä»¶ï¼ˆè€Œéæ®‹å·®ï¼‰ï¼Œä¿æŒæ¡ä»¶æ’å®šä½œä¸º"åŒ—ææ˜Ÿ"
    """

    def __init__(self, args):
        self.args = args

        # åˆå§‹åŒ– Accelerate - è‡ªåŠ¨å¤„ç†åˆ†å¸ƒå¼è®­ç»ƒè®¾ç½®
        # æ³¨æ„ï¼šmixed_precision ç”± accelerate launch å‘½ä»¤è¡Œå‚æ•°æ§åˆ¶
        self.accelerator = Accelerator(
            gradient_accumulation_steps=args.gradient_accumulation_steps,
            log_with=args.log_with
        )

        set_seed(args.seed)
        self.debug_mode = args.debug
        self.logger = Logger(self.accelerator, enabled=True, debug_mode=self.debug_mode)
        self.device = self.accelerator.device

        if self.accelerator.is_local_main_process:
            print("=== EditFlowç¬¦å·å›å½’é¢„è®­ç»ƒ (ä½¿ç”¨ Accelerate åŠ é€Ÿ) ===")
            print(f"æ ·æœ¬æ•°: {self.args.num_samples}")
            print(f"æœ€å¤§ç»´åº¦: {self.args.max_dim}")
            print(f"è¡¨è¾¾å¼æœ€å¤§é•¿åº¦: {self.args.max_expr_length}")
            print(f"æ‰¹æ¬¡å¤§å°: {self.args.batch_size}")
            print(f"è®­ç»ƒè½®æ•°: {self.args.num_epochs}")
            print(f"å­¦ä¹ ç‡: {self.args.learning_rate}")
            print(f"æµ‹è¯•é›†æ¯”ä¾‹: {self.args.test_split}")
            print(f"è¯„ä¼°é¢‘ç‡: æ¯{self.args.eval_every}è½®")
            print(f"LLaMAæ¨¡å‹é…ç½®: hidden_dim={self.args.hidden_dim}, n_layers={self.args.n_layers}, n_heads={self.args.n_heads}")
            print(f"æ¡ä»¶åµŒå…¥æ¨¡å‹: {self.args.condition_model_name}")
            print(f"æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {self.args.gradient_accumulation_steps}")
            print(f"FP16æ··åˆç²¾åº¦: {self.args.use_fp16}")
            print(f"\nAccelerate åˆå§‹åŒ–å®Œæˆ")
            print(f"  è®¾å¤‡: {self.device}")
            print(f"  åˆ†å¸ƒå¼è®­ç»ƒ: {self.accelerator.distributed_type}")
            print(f"  è¿›ç¨‹æ•°: {self.accelerator.num_processes}")
            print(f"  æ··åˆç²¾åº¦: {self.accelerator.mixed_precision}")
            print(f"  è°ƒè¯•æ¨¡å¼: {'å¯ç”¨' if self.debug_mode else 'ç¦ç”¨'}")

        self.logger.training_start(self.args)

    # ============= æ•°æ®ç®¡ç†æ–¹æ³• =============
    def prepare_data(self, tokenizer):
        """å‡†å¤‡è®­ç»ƒæ•°æ®ï¼Œä½¿ç”¨ Hugging Face datasets åŠ è½½"""
        cache_filename = f"data/flow_samples_{self.args.num_samples}_{self.args.max_dim}dim_{self.args.n_points}pts_{self.args.max_depth}depth_{self.args.max_expr_length}len.parquet"

        # ä¸»è¿›ç¨‹è´Ÿè´£æ•°æ®ç”Ÿæˆ
        if self.accelerator.is_local_main_process:
            print(f"å‡†å¤‡è¿ç»­æµè®­ç»ƒæ•°æ® (å•è¿›ç¨‹ç”Ÿæˆæ¨¡å¼)...")
            print(f"ä½¿ç”¨å¯¹é½æ–¹æ³•: {self.args.alignment_method}")
            generate_flow_samples(
                num_samples=self.args.num_samples,
                max_dim=self.args.max_dim,
                n_points=self.args.n_points,
                max_depth=self.args.max_depth,
                max_expr_length=self.args.max_expr_length,
                verbose=True,
                alignment_method=self.args.alignment_method,
            )
        else:
            print(f"[Rank {self.accelerator.process_index}] è·³è¿‡æ•°æ®ç”Ÿæˆï¼Œç­‰å¾…ä¸»è¿›ç¨‹å®Œæˆ...")

        self.accelerator.wait_for_everyone()

        if self.accelerator.is_local_main_process:
            print("[ä¸»è¿›ç¨‹] æ•°æ®ç”Ÿæˆå®Œæˆï¼Œå¼€å§‹åŠ è½½è®­ç»ƒæ•°æ®")

        print(f"[Rank {self.accelerator.process_index}] å‡†å¤‡å¼€å§‹è®­ç»ƒé˜¶æ®µ...")
        self.accelerator.wait_for_everyone()

        # åŠ è½½æ•°æ®
        use_stream = self.args.dataset_stream
        num_proc = self.args.dataset_num_proc

        if self.accelerator.is_local_main_process:
            print(f"ä½¿ç”¨ Hugging Face datasets åŠ è½½æ•°æ® (stream={use_stream})...")

        # åˆ†å‰²è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        train_dataset, test_dataset, train_size_estimate, test_size_estimate = self._split_train_test(
            cache_filename, tokenizer, use_stream, num_proc
        )

        # åˆ›å»ºDataLoader
        train_dataloader, test_dataloader = self._create_dataloaders(
            train_dataset, test_dataset
        )

        # å‡†å¤‡åˆ†å¸ƒå¼è®­ç»ƒ
        if self.accelerator.is_local_main_process:
            print(f"æ­£åœ¨å‡†å¤‡åˆ†å¸ƒå¼è®­ç»ƒ (accelerator.prepare)...")

        import time
        prepare_start = time.time()
        train_dataloader, test_dataloader = self.accelerator.prepare(
            train_dataloader, test_dataloader
        )
        prepare_time = time.time() - prepare_start

        if self.accelerator.is_local_main_process:
            print(f"âœ“ åˆ†å¸ƒå¼è®­ç»ƒå‡†å¤‡å®Œæˆ (è€—æ—¶: {prepare_time:.2f}ç§’)")

        if self.accelerator.is_local_main_process:
            is_stream_mode = getattr(train_dataset, 'stream', False)
            train_shuffle = not is_stream_mode
            num_workers = 0 if is_stream_mode else self.accelerator.num_processes
            expected_train_batches = train_size_estimate // self.args.batch_size
            expected_test_batches = test_size_estimate // self.args.batch_size

            print(f"âœ“ åˆ†å¸ƒå¼è®­ç»ƒå‡†å¤‡å®Œæˆ")
            print(f"æ•°æ®å‡†å¤‡å®Œæˆ: è®­ç»ƒé›†çº¦ {train_size_estimate} æ ·æœ¬, æµ‹è¯•é›†çº¦ {test_size_estimate} æ ·æœ¬")

            self.logger.log(
                "DATALOADER_VERIFY",
                f"DataLoaderåˆ›å»ºå®Œæˆ | é¢„æœŸè®­ç»ƒæ‰¹æ¬¡æ•°={expected_train_batches} | "
                f"é¢„æœŸæµ‹è¯•æ‰¹æ¬¡æ•°={expected_test_batches} | "
                f"num_workers={num_workers} | is_stream_mode={is_stream_mode} | "
                f"train_shuffle={train_shuffle} | "
                f"æ”¯æŒset_epoch={hasattr(train_dataset, 'set_epoch')}",
                "data_loading",
                level=1
            )

        return train_dataloader, train_dataset, test_dataloader, test_dataset

    def _split_train_test(self, cache_filename, tokenizer, use_stream, num_proc):
        """åˆ†å‰²è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼ˆç»Ÿä¸€æ–¹æ³•ï¼‰"""
        import time

        # å½“æ ·æœ¬æ•°å¾ˆå°‘æ—¶ï¼Œè®©æ‰€æœ‰æ ·æœ¬åŒæ—¶ç”¨äºè®­ç»ƒå’Œæµ‹è¯•
        if self.args.num_samples <= self.args.batch_size:
            if self.accelerator.is_local_main_process:
                mode_str = "æµå¼" if use_stream else "éæµå¼"
                print(f"{mode_str}æ¨¡å¼: æ ·æœ¬æ•°({self.args.num_samples}) â‰¤ batch_size({self.args.batch_size})")
                print(f"        æ‰€æœ‰æ ·æœ¬å°†åŒæ—¶ç”¨äºè®­ç»ƒå’Œæµ‹è¯•")

            full_dataset = prepare_dataset_hf(
                data_file=cache_filename, tokenizer=tokenizer,
                max_expr_length=self.args.max_expr_length,
                stream=use_stream, num_proc=num_proc,
                logger=self.logger
            )
            return full_dataset, full_dataset, self.args.num_samples, self.args.num_samples

        # æ­£å¸¸åˆ†å‰²é€»è¾‘
        split_ratio = 1 - self.args.test_split
        train_size = int(self.args.num_samples * split_ratio)
        test_size = self.args.num_samples - train_size

        if use_stream:
            # æµå¼æ¨¡å¼ï¼šä½¿ç”¨skipå’Œtake
            if self.accelerator.is_local_main_process:
                print(f"æµå¼æ¨¡å¼: è®­ç»ƒé›†çº¦ {train_size} æ ·æœ¬, æµ‹è¯•é›†çº¦ {test_size} æ ·æœ¬")

            train_dataset = prepare_dataset_hf(
                data_file=cache_filename, tokenizer=tokenizer,
                max_expr_length=self.args.max_expr_length,
                stream=True, num_proc=num_proc,
                skip=0, take=train_size,
                logger=self.logger
            )
            test_dataset = prepare_dataset_hf(
                data_file=cache_filename, tokenizer=tokenizer,
                max_expr_length=self.args.max_expr_length,
                stream=True, num_proc=num_proc,
                skip=train_size, take=test_size,
                logger=self.logger
            )
            return train_dataset, test_dataset, train_size, test_size
        else:
            # éæµå¼æ¨¡å¼ï¼šä½¿ç”¨Subsetç´¢å¼•
            if self.accelerator.is_local_main_process:
                print(f"[æ€§èƒ½] å¼€å§‹åˆ›å»ºå®Œæ•´æ•°æ®é›†...")

            dataset_start = time.time()
            full_dataset = prepare_dataset_hf(
                data_file=cache_filename, tokenizer=tokenizer,
                max_expr_length=self.args.max_expr_length,
                stream=False, num_proc=num_proc,
                logger=self.logger
            )
            dataset_time = time.time() - dataset_start

            if self.accelerator.is_local_main_process:
                print(f"[æ€§èƒ½] Dataset åˆ›å»ºè€—æ—¶: {dataset_time:.2f}ç§’")
                print(f"[æ€§èƒ½] å¼€å§‹åˆ›å»ºè®­ç»ƒ/æµ‹è¯•é›†ç´¢å¼• (total_size={self.args.num_samples})...")

            shuffle_start = time.time()
            from torch.utils.data import Subset
            indices = list(range(self.args.num_samples))
            np.random.shuffle(indices)
            shuffle_time = time.time() - shuffle_start

            train_indices = indices[:train_size]
            test_indices = indices[train_size:]

            if self.accelerator.is_local_main_process:
                print(f"[æ€§èƒ½] åˆ›å»ºå’Œæ‰“ä¹±ç´¢å¼•è€—æ—¶: {shuffle_time:.2f}ç§’")
                print(f"éæµå¼æ¨¡å¼: è®­ç»ƒé›† {len(train_indices)} æ ·æœ¬, æµ‹è¯•é›† {len(test_indices)} æ ·æœ¬")

            train_dataset = Subset(full_dataset, train_indices)
            test_dataset = Subset(full_dataset, test_indices)
            return train_dataset, test_dataset, len(train_indices), len(test_indices)

    def _create_dataloaders(self, train_dataset, test_dataset):
        """åˆ›å»ºè®­ç»ƒå’Œæµ‹è¯•DataLoader"""
        import time

        is_stream_mode = getattr(train_dataset, 'stream', False)
        train_size = len(train_dataset)
        test_size = len(test_dataset)
        train_drop_last = train_size >= self.args.batch_size
        test_drop_last = test_size >= self.args.batch_size

        if self.accelerator.is_local_main_process:
            if not train_drop_last:
                print(f"è­¦å‘Š: è®­ç»ƒé›†å¤§å°({train_size}) < batch_size({self.args.batch_size})ï¼Œç¦ç”¨drop_last")
            if not test_drop_last:
                print(f"è­¦å‘Š: æµ‹è¯•é›†å¤§å°({test_size}) < batch_size({self.args.batch_size})ï¼Œç¦ç”¨drop_last")

        train_shuffle = not is_stream_mode
        num_workers = 0 if is_stream_mode else self.accelerator.num_processes

        if self.accelerator.is_local_main_process:
            print(f"æ­£åœ¨åˆ›å»º DataLoader (batch_size={self.args.batch_size}, num_workers={num_workers}, shuffle={train_shuffle})...")

        dataloader_start = time.time()
        train_dataloader = torch.utils.data.DataLoader(
            train_dataset, batch_size=self.args.batch_size, shuffle=train_shuffle,
            num_workers=num_workers, collate_fn=custom_collate_fn,
            drop_last=train_drop_last, pin_memory=True
        )

        test_dataloader = torch.utils.data.DataLoader(
            test_dataset, batch_size=self.args.batch_size, shuffle=False,
            num_workers=num_workers, collate_fn=custom_collate_fn, drop_last=test_drop_last
        )
        dataloader_time = time.time() - dataloader_start

        if self.accelerator.is_local_main_process:
            print(f"âœ“ DataLoader åˆ›å»ºå®Œæˆ (è€—æ—¶: {dataloader_time:.2f}ç§’)")

        return train_dataloader, test_dataloader

    def setup_models(self, checkpoint_path=None):
        """åˆå§‹åŒ–æ¨¡å‹å’Œtokenizerï¼Œæ”¯æŒä»æ£€æŸ¥ç‚¹åŠ è½½"""
        if self.accelerator.is_local_main_process:
            print("åˆå§‹åŒ–tokenizerå’Œæ¨¡å‹...")

        # åˆå§‹åŒ–tokenizer
        from ..utils.special_tokens import SymbolicRegressionTokenizer, SymbolicVocab
        tokenizer = SymbolicRegressionTokenizer(max_dim=self.args.max_dim)

        if self.accelerator.is_local_main_process:
            print(f"âœ“ ç¬¦å·å›å½’Tokenizeråˆå§‹åŒ–å®Œæˆ")
            print(f"  è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size} (ç¬¦å·å›å½’ä¸“å±å°è¯æ±‡è¡¨)")
            print(f"  æœ€å¤§ç»´åº¦: {self.args.max_dim}")
            print(f"  è¿ç®—ç¬¦: {len(SymbolicVocab.OPERATORS)}ä¸ª - {', '.join(SymbolicVocab.OPERATORS)}")
            print(f"  å‡½æ•°: {len(SymbolicVocab.FUNCTIONS)}ä¸ª - {', '.join(SymbolicVocab.FUNCTIONS)}")
            print(f"  ç‰¹æ®Štoken: {len(SymbolicVocab.SPECIAL_TOKENS)}ä¸ª")
            print(f"  å˜é‡token: x0 ~ x{self.args.max_dim-1} (å…±{self.args.max_dim}ä¸ª)")

        # åˆå§‹åŒ–æ¡ä»¶ç¼–ç å™¨
        if self.accelerator.is_local_main_process:
            print("åˆå§‹åŒ–æ¡ä»¶ç¼–ç å™¨...")
        condition_encoder = SetTransformerConditionEncoder(
            max_input_dim=self.args.condition_max_input_dim,
            dim_hidden=self.args.condition_dim_hidden,
            num_heads=self.args.condition_num_heads,
            num_inds=self.args.condition_num_inds,
            num_layers=self.args.condition_num_layers,
            num_seeds=self.args.condition_num_seeds,
            dim_output=self.args.condition_dim_output,
            verbose=self.accelerator.is_local_main_process
        ).to(self.device)

        # åˆå§‹åŒ–LLaMA EditFlowæ¨¡å‹
        if self.accelerator.is_local_main_process:
            print("åˆå§‹åŒ–LLaMA EditFlowæ¨¡å‹ï¼ˆè‡ªå®šä¹‰æ¶æ„ï¼Œä¸åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼‰...")

        model = LlamaEditFlowBackbone(
            vocab_size=len(tokenizer.get_vocab()),
            hidden_dim=self.args.hidden_dim,
            n_layers=self.args.n_layers,
            n_heads=self.args.n_heads,
            condition_dim=self.args.condition_dim_hidden,
            dropout=self.args.dropout,
            max_seq_len=self.args.max_expr_length,
            use_condition_injection=self.args.use_condition_injection,
            verbose=self.accelerator.is_local_main_process
        ).to(self.device)

        # åˆ›å»ºä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        criterion = ContinuousFlowLoss(debug_mode=self.debug_mode)
        optimizer = torch.optim.AdamW(
            list(model.parameters()) + list(condition_encoder.parameters()),
            lr=self.args.learning_rate,
            weight_decay=self.args.weight_decay,
            eps=1e-8,
            betas=(0.9, 0.999)
        )

        # æ·»åŠ å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆä½™å¼¦é€€ç«ï¼‰
        from torch.optim.lr_scheduler import CosineAnnealingLR
        scheduler = CosineAnnealingLR(optimizer, T_max=self.args.num_epochs, eta_min=1e-6)

        # åŠ è½½æ£€æŸ¥ç‚¹
        load_checkpoint(checkpoint_path, model, condition_encoder, self.device, optimizer, verbose=self.accelerator.is_local_main_process)

        # ä½¿ç”¨ Accelerate å‡†å¤‡æ¨¡å‹å’Œä¼˜åŒ–å™¨
        if self.accelerator.is_local_main_process:
            print(f"ä½¿ç”¨ Accelerate å‡†å¤‡æ¨¡å‹å’Œä¼˜åŒ–å™¨...")
            print(f"  è¿›ç¨‹æ•°: {self.accelerator.num_processes}")
            print(f"  è®¾å¤‡: {self.accelerator.device}")
            print(f"  æ··åˆç²¾åº¦: {self.accelerator.mixed_precision}")

        model, condition_encoder, optimizer = self.accelerator.prepare(model, condition_encoder, optimizer)

        # å¦‚æœæœ‰checkpointï¼Œä½¿ç”¨Accelerateçš„load_stateæ–¹æ³•åŠ è½½å®Œæ•´çŠ¶æ€
        if checkpoint_path:
            if self.accelerator.is_local_main_process:
                print(f"Loading complete training state from {checkpoint_path}")
            self.accelerator.load_state(checkpoint_path)

        total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        if self.accelerator.is_local_main_process:
            print(f"âœ“ LLaMA EditFlowæ¨¡å‹å‚æ•°æ•°é‡: {total_params:,}")

        self.tokenizer = tokenizer

        return model, condition_encoder, criterion, optimizer, scheduler, tokenizer

    # ============= æ£€æŸ¥ç‚¹ç®¡ç† =============
    def save_checkpoint(self, model, condition_encoder, loss, epoch, is_final=False):
        self.accelerator.wait_for_everyone()

        checkpoint_dir = os.path.join(
            self.args.save_dir,
            "continuous_flow_final" if is_final else f"checkpoint_epoch_{epoch+1}"
        )
        os.makedirs(checkpoint_dir, exist_ok=True)

        # ä½¿ç”¨ Accelerate çš„ save_state æ–¹æ³•
        self.accelerator.save_state(checkpoint_dir)

        # ä¿å­˜æ¨¡å‹é…ç½®ä¿¡æ¯
        if self.accelerator.is_local_main_process:
            unwrapped_model = self.accelerator.unwrap_model(model)
            unwrapped_encoder = self.accelerator.unwrap_model(condition_encoder)

            model_config = {
                'vocab_size': unwrapped_model.vocab_size,
                'hidden_dim': unwrapped_model.hidden_dim,
                'n_layers': unwrapped_model.n_layers,
                'n_heads': unwrapped_model.n_heads,
                'condition_dim': unwrapped_model.condition_dim,
                'dropout': unwrapped_model.dropout,
                'max_seq_len': unwrapped_model.max_seq_len,
                'use_condition_injection': unwrapped_model.use_condition_injection,
            }

            config_data = {
                'epoch': epoch + 1,
                'model_state_dict': unwrapped_model.state_dict(),
                'condition_encoder_state_dict': unwrapped_encoder.state_dict(),
                'loss': loss,
                'model_config': model_config,
                'args': self.args,
                'accelerate_config': {
                    'distributed_type': str(self.accelerator.distributed_type),
                    'num_processes': self.accelerator.num_processes,
                    'mixed_precision': str(self.accelerator.mixed_precision),
                }
            }

            config_path = os.path.join(checkpoint_dir, "training_config.json")
            torch.save(config_data, config_path)

        return checkpoint_dir

    # ============= è®­ç»ƒæ¥å£ï¼ˆå§”æ‰˜ç»™ EditFlowTrainerï¼‰============
    def train(self):
        """è®­ç»ƒæ¨¡å‹ - å§”æ‰˜ç»™ EditFlowTrainer"""
        checkpoint_path = find_latest_checkpoint(self.args)
        if self.accelerator.is_local_main_process:
            print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
            print(f"{'æ‰¾åˆ°æ£€æŸ¥ç‚¹' if checkpoint_path else 'æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹ï¼Œå°†ä»åŸºç¡€æ¨¡å‹å¼€å§‹è®­ç»ƒ'}: {checkpoint_path or ''}")

        # å‡†å¤‡æ•°æ®å’Œæ¨¡å‹
        model, condition_encoder, criterion, optimizer, scheduler, tokenizer = self.setup_models(checkpoint_path=checkpoint_path)
        train_dataloader, train_dataset, test_dataloader, test_dataset = self.prepare_data(tokenizer)

        model_params = sum(p.numel() for p in model.parameters())
        encoder_params = sum(p.numel() for p in condition_encoder.parameters() if p.requires_grad)
        if self.accelerator.is_local_main_process:
            print(f"æ¨¡å‹å‚æ•°æ•°é‡: {model_params:,}, æ¡ä»¶ç¼–ç å™¨å‚æ•°æ•°é‡: {encoder_params:,}")
            print(f"å¼€å§‹è¿ç»­æµè®­ç»ƒ ({self.args.num_epochs} epochs)...")
            self.logger.log("TRAINING_START", f"å¼€å§‹è®­ç»ƒ | num_epochs={self.args.num_epochs} | model_params={model_params:,} | encoder_params={encoder_params:,}", level=1)

            # åˆ†å¸ƒå¼è®­ç»ƒè¯´æ˜
            if self.accelerator.num_processes > 1:
                train_dataset_size = len(train_dataset)
                test_dataset_size = len(test_dataset)
                samples_per_process = train_dataset_size // self.accelerator.num_processes
                batches_per_process = samples_per_process // self.args.batch_size
                total_batches_all_processes = batches_per_process * self.accelerator.num_processes
                coverage_rate = (total_batches_all_processes * self.args.batch_size / train_dataset_size * 100) if train_dataset_size > 0 else 0.0

                print("\n" + "="*70)
                print("ğŸ“Š åˆ†å¸ƒå¼è®­ç»ƒé…ç½®è¯´æ˜")
                print("="*70)
                print(f"è¿›ç¨‹æ•° (GPUæ•°):        {self.accelerator.num_processes}")
                print(f"è®­ç»ƒé›†æ€»æ ·æœ¬æ•°:        {train_dataset_size}")
                print(f"æ¯ä¸ªè¿›ç¨‹åˆ†é…æ ·æœ¬æ•°:    {samples_per_process} (æ•´æ•°é™¤æ³•)")
                print(f"æ¯ä¸ªè¿›ç¨‹é¢„æœŸæ‰¹æ¬¡æ•°:    {batches_per_process}")
                print(f"æ‰€æœ‰è¿›ç¨‹æ€»æ‰¹æ¬¡æ•°:      {total_batches_all_processes}")
                print(f"æ ·æœ¬è¦†ç›–ç‡:            {coverage_rate:.1f}%")
                print(f"\næ³¨æ„ï¼šç”±äºæ•´æ•°é™¤æ³•ï¼Œçº¦ {train_dataset_size % self.accelerator.num_processes} ä¸ªæ ·æœ¬")
                print(f"      ({train_dataset_size - total_batches_all_processes * self.args.batch_size} ä¸ª) ä¸ä¼šè¢«è®­ç»ƒ")
                print("="*70 + "\n")

                self.logger.log(
                    "DISTRIBUTED_TRAINING_INFO",
                    f"åˆ†å¸ƒå¼è®­ç»ƒé…ç½® | è¿›ç¨‹æ•°={self.accelerator.num_processes} | "
                    f"è®­ç»ƒé›†å¤§å°={train_dataset_size} | æ¯è¿›ç¨‹æ ·æœ¬æ•°={samples_per_process} | "
                    f"æ¯è¿›ç¨‹æ‰¹æ¬¡æ•°={batches_per_process} | æ€»æ‰¹æ¬¡æ•°={total_batches_all_processes} | "
                    f"è¦†ç›–ç‡={coverage_rate:.1f}%",
                    "distributed_setup",
                    level=1
                )

        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = EditFlowTrainer(
            model=model,
            condition_encoder=condition_encoder,
            criterion=criterion,
            optimizer=optimizer,
            scheduler=scheduler,
            tokenizer=tokenizer,
            args=self.args,
            logger=self.logger,
            accelerator=self.accelerator
        )

        eval_every = self.args.eval_every

        # è®­ç»ƒå¾ªç¯
        for epoch in range(self.args.num_epochs):
            avg_loss, num_batches, total_loss, total_grad_norm = trainer.train_epoch(
                train_dataloader, train_dataset, epoch, "Mixed"
            )

            # åœ¨æ‰€æœ‰è¿›ç¨‹ä¸Šæ”¶é›†è®­ç»ƒæŒ‡æ ‡ï¼ˆä½¿ç”¨åˆå¹¶åçš„æ–¹æ³•ï¼‰
            metrics = trainer.gather_and_format_metrics(num_batches, total_loss, total_grad_norm)

            # åªåœ¨ä¸»è¿›ç¨‹ä¸Šæ‰“å°å’Œè®°å½•æ—¥å¿—
            if self.accelerator.is_local_main_process:
                current_lr = optimizer.param_groups[0]['lr']

                if self.accelerator.num_processes > 1:
                    # ä½¿ç”¨åˆå¹¶åçš„æ–¹æ³•è¿”å›çš„æŒ‡æ ‡
                    gpu_details = metrics['gpu_metrics']
                    global_total_batches = metrics['global_total_batches']
                    global_avg_loss = metrics['global_avg_loss']

                    # æ„å»ºå®Œæ•´çš„æ—¥å¿—æ¶ˆæ¯
                    gpu_summary = "\n" + "\n".join(gpu_details) + "\n--- å…¨å±€æ±‡æ€» --- | " + \
                                 f"total_batches={global_total_batches} | avg_train_loss={global_avg_loss:.6f} | " + \
                                 f"lr={current_lr:.2e}"

                    # æ§åˆ¶å°è¾“å‡ºï¼ˆç®€åŒ–ç‰ˆï¼‰
                    print(f"Epoch {epoch+1}/{self.args.num_epochs} å®Œæˆ | avg_train_loss={global_avg_loss:.4f} | total_batches={global_total_batches} | lr={current_lr:.2e}")

                    # æ—¥å¿—æ–‡ä»¶è®°å½•ï¼ˆåŒ…å«è¯¦ç»†çš„GPUä¿¡æ¯ï¼‰
                    self.logger.log(
                        "EPOCH_COMPLETE",
                        f"Epoch {epoch+1}/{self.args.num_epochs} [åˆ†å¸ƒå¼è®­ç»ƒè¯¦ç»†] |\n" + gpu_summary,
                        f"epoch{epoch+1}_complete",
                        level=1
                    )
                else:
                    # å•GPUè®­ç»ƒ
                    avg_grad_norm = total_grad_norm / num_batches if num_batches > 0 else 0.0
                    print(f"Epoch {epoch+1}/{self.args.num_epochs} å®Œæˆ, è®­ç»ƒæŸå¤±: {avg_loss:.4f}, æ¢¯åº¦èŒƒæ•°: {avg_grad_norm:.3f}, å­¦ä¹ ç‡: {current_lr:.2e}")
                    self.logger.log(
                        "EPOCH_COMPLETE",
                        f"Epoch {epoch+1}/{self.args.num_epochs} | train_loss={avg_loss:.4f} | "
                        f"avg_grad_norm={avg_grad_norm:.3f} | lr={current_lr:.2e} | batches={num_batches}",
                        level=1
                    )

            scheduler.step()

            # è¯„ä¼°
            if (epoch + 1) % eval_every == 0 or epoch == self.args.num_epochs - 1:
                test_loss = trainer.evaluate(test_dataloader, test_dataset)
                if self.accelerator.is_local_main_process:
                    print(f"æµ‹è¯•é›†æŸå¤±: {test_loss:.4f}")
                    self.logger.log("EVALUATION", f"Epoch {epoch+1}/{self.args.num_epochs} | test_loss={test_loss:.4f}", level=1)

            # ä¿å­˜æ£€æŸ¥ç‚¹
            if (epoch + 1) % self.args.save_every == 0:
                checkpoint_path = self.save_checkpoint(
                    model, condition_encoder, avg_loss, epoch
                )
                if self.accelerator.is_local_main_process:
                    print(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜åˆ°: {checkpoint_path}")
                    self.logger.log("CHECKPOINT_SAVED", f"Epoch {epoch+1}/{self.args.num_epochs} | path={checkpoint_path} | train_loss={avg_loss:.4f}", level=1)

        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_path = self.save_checkpoint(
            model, condition_encoder, avg_loss, self.args.num_epochs - 1, is_final=True
        )
        if self.accelerator.is_local_main_process:
            print(f"æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜åˆ°: {final_path}")
            self.logger.log("TRAINING_COMPLETE", f"è®­ç»ƒå®Œæˆ | final_path={final_path} | final_train_loss={avg_loss:.4f} | total_epochs={self.args.num_epochs}", level=1)

        return model, condition_encoder

    # ============= æ¨ç†æ¥å£ï¼ˆå§”æ‰˜ç»™ InferenceEngineï¼‰============
    def symbolic_regression(self, model_path, x_data, y_data, n_steps=100, input_dim=None, max_expr_length=None, initial_expr=None):
        """ç¬¦å·å›å½’ - å§”æ‰˜ç»™ InferenceEngine

        Args:
            model_path: æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„
            x_data: è¾“å…¥xæ•°æ®
            y_data: ç›®æ ‡yæ•°æ®
            n_steps: æ¨ç†æ­¥æ•°
            input_dim: è¾“å…¥ç»´åº¦ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨æ¨æ–­
            max_expr_length: è¡¨è¾¾å¼æœ€å¤§tokené•¿åº¦ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨argsä¸­çš„å€¼
            initial_expr: åˆå§‹è¡¨è¾¾å¼ï¼ˆsympyè¡¨è¾¾å¼æˆ–å­—ç¬¦ä¸²ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨x0
        """
        self.logger.log("SYMBOLIC_REGRESSION_START",
                       f"è¾“å…¥æ•°æ®: xå½¢çŠ¶={x_data.shape}, yå½¢çŠ¶={y_data.shape} | n_steps={n_steps}",
                       "inference", level=3)

        # åŠ è½½æ¨¡å‹
        model, condition_encoder, _, _, _, tokenizer = self.setup_models(checkpoint_path=model_path)
        device = self.device

        # å‡†å¤‡è¾“å…¥æ•°æ®
        x_values = torch.FloatTensor(x_data).unsqueeze(0).to(device)
        y_values = torch.FloatTensor(y_data).unsqueeze(0).to(device)

        if input_dim is None:
            input_dim = x_data.shape[1] if len(x_data.shape) > 1 else 1

        # ç¼–ç æ¡ä»¶
        point_mask = torch.ones_like(y_values)
        condition = condition_encoder(x_values, y_values, point_mask)

        # åˆ›å»ºæ¨ç†å¼•æ“
        inference_engine = InferenceEngine(
            model=model,
            condition_encoder=condition_encoder,
            tokenizer=tokenizer,
            args=self.args,
            logger=self.logger,
            device=device
        )

        # æ‰§è¡Œæ¨ç†
        return inference_engine.symbolic_regression(
            x_data=x_data,
            y_data=y_data,
            condition=condition,
            x_values=x_values,
            y_values=y_values,
            n_steps=n_steps,
            initial_expr=initial_expr
        )

