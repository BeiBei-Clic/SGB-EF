"""
EditFlowè¿ç»­æµåŒ¹é…çš„æ ¸å¿ƒç»„ä»¶
"""

import torch
import json
import os
import time
from typing import List, Dict, Tuple, Optional
from datasets import load_dataset
from ..symbolic.data_generator import generate_flow_samples


def remove_gap_tokens(z_t: torch.Tensor, tokenizer) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
    """ç§»é™¤gap tokenå¹¶è¿”å›å¤„ç†åçš„åºåˆ—

    é‡è¦ï¼šæ­¤å‡½æ•°åªç§»é™¤ gap_tokenï¼Œä¿ç•™ BOS token å’Œæ‰€æœ‰å…¶ä»– tokens
    ç¡®ä¿è¿”å›çš„åºåˆ—æ ¼å¼ä¸º [BOS] + [non_gap_tokens] + [PAD...]
    """
    pad_token_id = tokenizer.convert_tokens_to_ids('<pad>')
    gap_token_id = tokenizer.convert_tokens_to_ids('<gap>')
    bos_token_id = tokenizer.convert_tokens_to_ids('<s>')
    batch_size, z_seq_len = z_t.shape
    device = z_t.device

    z_gap_mask = (z_t == gap_token_id)
    z_pad_mask = (z_t == pad_token_id)

    # ä½¿ç”¨æ©ç æ“ä½œç§»é™¤gap tokensï¼ˆåªç§»é™¤gapï¼Œä¿ç•™BOSå’Œå…¶ä»–tokensï¼‰
    x_t_list = []
    for i in range(batch_size):
        non_gap_mask = ~z_gap_mask[i]
        x_row = z_t[i][non_gap_mask]
        x_t_list.append(x_row)

        # éªŒè¯ï¼šç¡®ä¿BOS tokenè¢«ä¿ç•™ï¼ˆå¦‚æœè¾“å…¥ä¸­å­˜åœ¨ï¼‰
        if len(x_row) > 0 and z_t[i, 0] == bos_token_id:
            assert x_row[0] == bos_token_id, f"BOS tokenå¿…é¡»è¢«ä¿ç•™åœ¨ä½ç½®0"

    max_len = max(len(x) for x in x_t_list)
    x_t_padded = torch.full((batch_size, max_len), pad_token_id, dtype=torch.long, device=device)
    x_pad_mask_padded = torch.zeros((batch_size, max_len), dtype=torch.bool, device=device)

    for i, x_row in enumerate(x_t_list):
        x_t_padded[i, :len(x_row)] = x_row
        x_pad_mask_padded[i, :len(x_row)] = (x_row == pad_token_id)

    return x_t_padded, x_pad_mask_padded, z_gap_mask, z_pad_mask


def fill_gap_tokens_with_repeats(x_ut: torch.Tensor, z_gap_mask: torch.Tensor, z_pad_mask: torch.Tensor) -> torch.Tensor:
    """ç”¨é‡å¤å€¼å¡«å……gap tokenä½ç½®"""
    batch_size, z_seq_len = z_gap_mask.shape
    _, x_seq_len, vocab_size = x_ut.shape


    # è®¡ç®—æ¯ä¸ªä½ç½®å¯¹åº”çš„égapä½ç½®

    non_gap_mask = ~z_gap_mask


    indices = non_gap_mask.cumsum(dim=1) - 1


    indices = indices.clamp(min=0, max=x_seq_len-1)


    # æ”¶é›†å¯¹åº”çš„ç‰¹å¾
    batch_indices = torch.arange(batch_size, device=x_ut.device).unsqueeze(1)

    result = x_ut[batch_indices, indices]

    result[z_pad_mask] = 0


    return result


class ContinuousFlowLoss:
    """è¿ç»­æ—¶é—´æµåŒ¹é…æŸå¤±å‡½æ•°ï¼ˆæ¶æ„v2.0 - å›ºå®št=0ï¼Œä¸å†éœ€è¦è°ƒåº¦å™¨ï¼‰"""

    def __init__(self, debug_mode=False):
        self.debug_mode = debug_mode

    def make_ut_mask_from_z(self, z_t: torch.Tensor, z_1: torch.Tensor, vocab_size: int,
                           gap_token: int, tokenizer, x_t: torch.Tensor) -> torch.Tensor:
        """
        æ ¹æ®è®ºæ–‡Fig. 13çš„åŒç´¢å¼•è¿½è¸ªé€»è¾‘ï¼Œç”Ÿæˆæ­£ç¡®çš„ç¼–è¾‘æ“ä½œæ©ç 

        æ ¸å¿ƒæ€æƒ³ï¼šåœ¨Zç©ºé—´ï¼ˆå«gapï¼‰éå†ï¼ŒåŠ¨æ€ç»´æŠ¤Xç©ºé—´ï¼ˆæ— gapï¼‰çš„ç´¢å¼•æŒ‡é’ˆ

        Args:
            z_t: å½“å‰çŠ¶æ€ï¼ˆZç©ºé—´ï¼Œå«gapï¼‰[batch, z_seq_len]
            z_1: ç›®æ ‡çŠ¶æ€ï¼ˆZç©ºé—´ï¼Œå«gapï¼‰[batch, z_seq_len]
            vocab_size: è¯æ±‡è¡¨å¤§å°
            gap_token: gap tokençš„ID
            tokenizer: åˆ†è¯å™¨
            x_t: å½“å‰çŠ¶æ€ï¼ˆXç©ºé—´ï¼Œæ— gapï¼‰[batch, x_seq_len] - ç”¨äºåŒç´¢å¼•æ˜ å°„

        Returns:
            u_mask: ç¼–è¾‘æ“ä½œæ©ç  [batch, x_seq_len, 2*vocab_size+1]
                    ä½¿ç”¨one-hotç¼–ç ï¼šu_mask[b, pos, op_id] = 1 è¡¨ç¤ºåœ¨ä½ç½®posæ‰§è¡Œæ“ä½œop_id
                    æ¯ä¸ªä½ç½®å¯¹åº”ï¼š[vocab_sizeä¸ªæ’å…¥æ“ä½œ, vocab_sizeä¸ªæ›¿æ¢æ“ä½œ, 1ä¸ªåˆ é™¤æ“ä½œ]
        """
        batch_size, z_seq_len = z_t.shape
        x_seq_len = x_t.shape[1]
        n_ops = 2 * vocab_size + 1  # æ’å…¥(vocab_size) + æ›¿æ¢(vocab_size) + åˆ é™¤(1)

        pad_token = tokenizer.convert_tokens_to_ids('<pad>')

        # åˆå§‹åŒ–è¾“å‡ºæ©ç ï¼ˆåœ¨Xç©ºé—´ï¼‰
        # âœ… ä½¿ç”¨one-hotç¼–ç ï¼šu_mask[b, pos, op_id] = 1 è¡¨ç¤ºåœ¨ä½ç½®posæ‰§è¡Œæ“ä½œop_id
        # å³ä½¿éœ€è¦å¤šæ¬¡æ’å…¥åŒä¸€tokenï¼Œä¹Ÿåªæ ‡è®°ä¸€æ¬¡ï¼ˆå› ä¸ºè¿™æ˜¯æ“ä½œç±»å‹ï¼Œä¸æ˜¯æ•°é‡ï¼‰
        u_mask = torch.zeros((batch_size, x_seq_len, n_ops), dtype=torch.int, device=z_t.device)

        # å¯¹æ¯ä¸ªæ ·æœ¬è¿›è¡ŒåŒç´¢å¼•éå†ï¼ˆè®ºæ–‡Fig. 13çš„æ ¸å¿ƒé€»è¾‘ï¼‰
        for b in range(batch_size):
            x_t_index = -1  # Xç©ºé—´æŒ‡é’ˆåˆå§‹åŒ–ä¸º-1ï¼ˆæŒ‡å‘x_tçš„å‰ä¸€ä¸ªä½ç½®ï¼‰
            first_valid_index = 0  # è®°å½•ç¬¬ä¸€ä¸ªæœ‰æ•ˆtokençš„ä½ç½®ï¼Œç”¨äºå¤„ç†å¼€å¤´çš„gap

            for i in range(z_seq_len):
                token_t = z_t[b, i].item()
                token_1 = z_1[b, i].item()

                # è·³è¿‡z_tå’Œz_1çš„padä½ç½®
                if token_t == pad_token or token_1 == pad_token:
                    continue

                # === å…³é”®æ­¥éª¤1ï¼šç»´æŠ¤Xç©ºé—´æŒ‡é’ˆ ===
                # å¦‚æœz_tå½“å‰ä½ç½®ä¸æ˜¯gapï¼Œè¯´æ˜å®ƒåœ¨x_tä¸­å æ®ä¸€ä¸ªä½ç½®
                # å› æ­¤éœ€è¦å°†x_t_indexå‘å‰ç§»åŠ¨ä¸€ä½
                if token_t != gap_token:
                    x_t_index += 1  # ç§»åŠ¨åˆ°x_tä¸­çš„ä¸‹ä¸€ä¸ªä½ç½®

                    # æ›´æ–°ç¬¬ä¸€ä¸ªæœ‰æ•ˆtokençš„ä½ç½®
                    if first_valid_index == 0:
                        first_valid_index = x_t_index

                    # === å…³é”®ä¿®å¤ï¼šæ£€æŸ¥x_tå½“å‰ä½ç½®æ˜¯å¦æ˜¯pad ===
                    # å¦‚æœx_tå½“å‰ä½ç½®æ˜¯padï¼Œè¯´æ˜å·²ç»è¶…å‡ºæœ‰æ•ˆé•¿åº¦ï¼Œåœæ­¢éå†
                    if x_t_index >= x_seq_len:
                        break  # è¶…å‡ºx_tçš„æœ‰æ•ˆé•¿åº¦ï¼Œåœæ­¢

                    if x_t[b, x_t_index].item() == pad_token:
                        break  # x_tå½“å‰ä½ç½®æ˜¯padï¼Œè¯´æ˜å·²ç»æ˜¯å¡«å……åŒºåŸŸï¼Œåœæ­¢

                # === å…³é”®æ­¥éª¤2ï¼šåˆ¤æ–­ç¼–è¾‘ç±»å‹å¹¶æ ‡è®° ===
                # æ ¹æ®z_t[i]å’Œz_1[i]çš„å…³ç³»ï¼Œå†³å®šåœ¨x_t[x_t_index]ä½ç½®æ‰§è¡Œä»€ä¹ˆæ“ä½œ

                if token_t == gap_token and token_1 != gap_token:
                    # æ’å…¥æ“ä½œï¼š
                    # z_t[i]æ˜¯gapï¼Œz_1[i]æ˜¯æœ‰æ•ˆtoken
                    # æ„å‘³ç€éœ€è¦åœ¨gapä½ç½®æ’å…¥token_1
                    #
                    # âœ… ä¿®å¤ï¼šä½¿ç”¨one-hotç¼–ç è€Œéç´¯åŠ è®¡æ•°
                    # å³ä½¿éœ€è¦å¤šæ¬¡æ’å…¥ç›¸åŒtokenï¼Œä¹Ÿåªæ ‡è®°ä¸€æ¬¡
                    # å› ä¸ºæ¨¡å‹åªéœ€è¦çŸ¥é“"åœ¨è¿™ä¸ªä½ç½®æ‰§è¡Œæ’å…¥sinæ“ä½œ"
                    # è€Œä¸æ˜¯"æ’å…¥2æ¬¡sin"ï¼ˆåè€…æ˜¯ç¼–è¾‘æ“ä½œçš„å®šä¹‰ï¼Œä¸æ˜¯æŸå¤±çš„ä¸€éƒ¨åˆ†ï¼‰
                    insert_pos = max(x_t_index, first_valid_index)
                    if insert_pos >= 0 and insert_pos < x_seq_len:
                        u_mask[b, insert_pos, token_1] = 1  # one-hotç¼–ç ï¼ˆåªæ ‡è®°æ“ä½œç±»å‹ï¼‰

                elif token_t != gap_token and token_1 == gap_token:
                    # åˆ é™¤æ“ä½œï¼š
                    # z_t[i]æ˜¯æœ‰æ•ˆtokenï¼Œz_1[i]æ˜¯gap
                    # æ„å‘³ç€éœ€è¦åˆ é™¤å½“å‰token
                    # åˆ é™¤æ“ä½œç›´æ¥æ ‡è®°åœ¨x_t_indexä½ç½®ï¼ˆå½“å‰tokençš„ä½ç½®ï¼‰
                    if x_t_index >= 0 and x_t_index < x_seq_len:
                        u_mask[b, x_t_index, -1] = 1  # one-hotç¼–ç 

                elif token_t != gap_token and token_1 != gap_token and token_t != token_1:
                    # æ›¿æ¢æ“ä½œï¼š
                    # z_t[i]å’Œz_1[i]éƒ½æ˜¯æœ‰æ•ˆtokenä½†ä¸åŒ
                    # æ„å‘³ç€éœ€è¦å°†token_tæ›¿æ¢ä¸ºtoken_1
                    # æ›¿æ¢æ“ä½œæ ‡è®°åœ¨x_t_indexä½ç½®ï¼ˆåç§»vocab_sizeä»¥åŒºåˆ†æ’å…¥å’Œæ›¿æ¢ï¼‰
                    if x_t_index >= 0 and x_t_index < x_seq_len:
                        u_mask[b, x_t_index, token_1 + vocab_size] = 1  # one-hotç¼–ç 

        return u_mask

    def __call__(self, u_cat_x: torch.Tensor, u_z: torch.Tensor, u_mask: torch.Tensor,
                 vocab_size: int, accelerator=None, logger=None) -> torch.Tensor:
        """
        è¿ç»­æµæŸå¤±è®¡ç®—

        Args:
            u_cat_x: Xç©ºé—´çš„é¢„æµ‹é€Ÿç‡ [batch, x_seq_len, 2*vocab_size+1]ï¼ˆåŸå§‹ç©ºé—´ï¼Œä¸å«gapï¼‰
            u_z: Zç©ºé—´çš„é¢„æµ‹é€Ÿç‡ [batch, z_seq_len, 2*vocab_size+1]ï¼ˆæ‰©å±•ç©ºé—´ï¼Œå«gapé‡å¤ï¼‰
            u_mask: æ“ä½œæ©ç  [batch, z_seq_len, 2*vocab_size+1]
            vocab_size: è¯æ±‡è¡¨å¤§å°
            accelerator: AccelerateåŠ é€Ÿå™¨
            logger: æ—¥å¿—è®°å½•å™¨ï¼ˆå¯é€‰ï¼‰

        Returns:
            loss: æ ‡é‡æŸå¤±å€¼
        """
        # NaN/Infæ£€æµ‹ - æ£€æµ‹ä¸¤ä¸ªè¾“å…¥
        nan_count_x = torch.isnan(u_cat_x).sum().item()
        inf_count_x = torch.isinf(u_cat_x).sum().item()
        nan_count_z = torch.isnan(u_z).sum().item()
        inf_count_z = torch.isinf(u_z).sum().item()

        # é¢å¤–æ£€æŸ¥ï¼šu_z æ˜¯å¦åŒ…å«æ¥è¿‘0çš„å€¼ï¼ˆå¯èƒ½å¯¼è‡´ log(0)ï¼‰
        u_z_min = float(u_z.min().item())
        u_z_max = float(u_z.max().item())
        u_z_mean = float(u_z.mean().item())
        u_z_std = float(u_z.std().item())
        u_z_has_near_zero = bool((u_z < 1e-10).any().item())
        u_z_has_negative = bool((u_z < 0).any().item())
        u_z_num_zeros = int((u_z == 0).sum().item())

        # åˆ†å¸ƒå¼NaNæ£€æµ‹ï¼šå¦‚æœä½¿ç”¨acceleratorï¼Œç¡®ä¿æ‰€æœ‰è¿›ç¨‹åŒæ­¥æ£€æµ‹ç»“æœ
        has_nan_or_inf = torch.tensor(1 if ((nan_count_x > 0 or inf_count_x > 0) or (nan_count_z > 0 or inf_count_z > 0)) else 0,
                                    device=u_cat_x.device)

        if accelerator is not None and accelerator.distributed_type != "NO":
            # åŒæ­¥æ‰€æœ‰è¿›ç¨‹çš„NaNæ£€æµ‹ç»“æœ
            # ä½¿ç”¨accelerator.gatheræ”¶é›†æ‰€æœ‰è¿›ç¨‹çš„NaNæ£€æµ‹ç»“æœ
            gathered_results = accelerator.gather(has_nan_or_inf)
            has_nan_or_inf = gathered_results.sum()
        else:
            has_nan_or_inf = has_nan_or_inf.item()

        # è®°å½•NaN/Infæ£€æµ‹ç»“æœï¼ˆä»…ç”¨äºç›‘æ§ï¼Œä¸å†æŠ›å‡ºå¼‚å¸¸è·³è¿‡ï¼‰
        if has_nan_or_inf > 0 and logger is not None and accelerator.is_local_main_process:
            logger.error("INPUT_NAN_INF",
                        f"æ£€æµ‹åˆ°å¼‚å¸¸å€¼: Xç©ºé—´NaN={nan_count_x}, Inf={inf_count_x}, "
                        f"Zç©ºé—´NaN={nan_count_z}, Inf={inf_count_z}, åˆ†å¸ƒå¼æ£€æµ‹={has_nan_or_inf}",
                        "compute_loss", level=1)

        # å…³é”®ä¿®å¤ï¼šu_total éœ€è¦åœ¨æ¦‚ç‡ç©ºé—´è®¡ç®—
        # ä»logitsè½¬æ¢ä¸ºæ¦‚ç‡
        u_cat_x_probs = torch.exp(u_cat_x.clamp(max=10))  # é˜²æ­¢æº¢å‡º
        u_total = u_cat_x_probs.sum(dim=(1, 2))

        # ğŸ”§ å…³é”®ä¿®å¤ï¼šé¿å…-infæ±¡æŸ“lossè®¡ç®—
        # é—®é¢˜ï¼šå¦‚æœç›´æ¥å¯¹æ‰€æœ‰ä½ç½®è®¡ç®—log_softmaxï¼Œpaddingä½ç½®çš„-infä¼šå½±å“æ•°å€¼ç¨³å®šæ€§
        # è§£å†³ï¼šå¯¹æ¯ä¸ªæ ·æœ¬ï¼Œåªæå–æœ‰æ•ˆä½ç½®ï¼ˆu_mask=1ï¼‰çš„logitsï¼Œå•ç‹¬è®¡ç®—log_softmax

        batch_size, z_seq_len, n_ops = u_z.shape

        # åˆå§‹åŒ–è¾“å‡ºå¼ é‡ï¼Œå¡«å……ä¸€ä¸ªè¾ƒå°çš„è´Ÿæ•°ï¼ˆä¸æ˜¯-infï¼Œé¿å…æ±¡æŸ“ï¼‰
        log_u_z = torch.zeros_like(u_z) - 10.0  # åˆå§‹åŒ–ä¸ºlog(å¾ˆå°çš„æ¦‚ç‡)

        # å¯¹æ¯ä¸ªæ ·æœ¬å•ç‹¬å¤„ç†
        for b in range(batch_size):
            # æ‰¾åˆ°è¯¥æ ·æœ¬æ‰€æœ‰æœ‰æ•ˆä½ç½®çš„mask
            mask_b = u_mask[b].bool()  # [z_seq_len, n_ops]

            if mask_b.any():
                # æå–æœ‰æ•ˆä½ç½®çš„logits
                valid_logits = u_z[b][mask_b]  # [num_valid]

                # åªåœ¨æœ‰æ•ˆä½ç½®è®¡ç®—log_softmax
                valid_log_probs = torch.log_softmax(valid_logits, dim=-1)

                # å¡«å……å›åŸä½ç½®
                log_u_z[b][mask_b] = valid_log_probs
            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆä½ç½®ï¼Œä¿æŒåˆå§‹å€¼-10.0ï¼ˆlog(exp(-10)) = -10ï¼‰

        # ç»Ÿè®¡ä¿¡æ¯ï¼šlog_u_zï¼ˆåº”è¯¥åœ¨è´Ÿæ— ç©·åˆ°0ä¹‹é—´ï¼‰
        log_u_z_min = float(log_u_z.min().item())
        log_u_z_max = float(log_u_z.max().item())
        log_u_z_mean = float(log_u_z.mean().item())
        log_u_z_has_inf = bool(torch.isinf(log_u_z).any().item())
        log_u_z_has_nan = bool(torch.isnan(log_u_z).any().item())

        # ç»Ÿè®¡ä¿¡æ¯ï¼šu_maskï¼ˆæ ‡è®°éœ€è¦é¢„æµ‹çš„ä½ç½®ï¼‰
        u_mask_num_true = int(u_mask.sum().item())
        u_mask_total = int(u_mask.numel())
        u_mask_sparsity = float(1.0 - (u_mask_num_true / u_mask_total))

        # cross_entropy åœ¨ Z ç©ºé—´è®¡ç®—ï¼ˆè´Ÿå¯¹æ•°ä¼¼ç„¶ï¼‰
        # u_mask æ ‡è®°äº†æ­£ç¡®çš„æ“ä½œä½ç½®ï¼ˆone-hotç¼–ç ï¼‰
        # åªåœ¨ u_mask=True çš„ä½ç½®ç´¯åŠ ï¼ˆå…¶ä»–ä½ç½®ä¸å½±å“æŸå¤±ï¼‰
        masked_log_u_z = log_u_z * u_mask.float()

        # ğŸ”§ ä¿®å¤ï¼šè®¡ç®—æ¯ä¸ªæ ·æœ¬çš„æœ‰æ•ˆæ“ä½œæ•°é‡ï¼ˆç”¨äºå½’ä¸€åŒ–ï¼‰
        # è¿™ç¡®ä¿ä¸åŒé•¿åº¦çš„åºåˆ—å¯¹æŸå¤±çš„è´¡çŒ®æ˜¯å…¬å¹³çš„
        valid_ops_per_sample = u_mask.sum(dim=(1, 2)).clamp(min=1)  # è‡³å°‘ä¸º1é¿å…é™¤0

        # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„å¹³å‡è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼ˆå¯¹åºåˆ—é•¿åº¦å½’ä¸€åŒ–ï¼‰
        cross_entropy = masked_log_u_z.sum(dim=(1, 2)) / valid_ops_per_sample

        # ç»Ÿè®¡ä¿¡æ¯ï¼šcross_entropyï¼ˆè´Ÿå¯¹æ•°ä¼¼ç„¶ï¼Œåº”è¯¥æ˜¯è´Ÿæ•°ï¼‰
        cross_entropy_min = float(cross_entropy.min().item())
        cross_entropy_max = float(cross_entropy.max().item())
        cross_entropy_mean = float(cross_entropy.mean().item())
        cross_entropy_std = float(cross_entropy.std().item() if cross_entropy.numel() > 1 else 0.0)

        # æœ€ç»ˆæŸå¤±ï¼šè´Ÿå¯¹æ•°ä¼¼ç„¶ + u_totalæ­£åˆ™åŒ–
        # cross_entropy: æƒ©ç½š"åœ¨éœ€è¦ç¼–è¾‘çš„ä½ç½®é¢„æµ‹é”™è¯¯"
        # u_total: æƒ©ç½š"é¢„æµ‹è¿‡é«˜çš„æ€»æ“ä½œé€Ÿç‡"ï¼ˆé¼“åŠ±ç¨€ç–æ€§ï¼‰
        # è¿™æ ·æ¨¡å‹åŒæ—¶å­¦ä¹ ä¸¤ä¸ªç›®æ ‡ï¼š
        # 1. åœ¨éœ€è¦ç¼–è¾‘çš„ä½ç½®é¢„æµ‹æ­£ç¡®çš„ç¼–è¾‘æ“ä½œ
        # 2. åœ¨æ‰€æœ‰ä½ç½®ä¿æŒä½æ“ä½œé€Ÿç‡ï¼ˆç¨€ç–é¢„æµ‹ï¼‰

        # u_totalå·²ç»åœ¨å‰é¢è®¡ç®—ï¼šu_total = u_cat_x.sum(dim=(1, 2))
        # shape: [batch]ï¼Œæ¯ä¸ªæ ·æœ¬çš„æ€»æ“ä½œé€Ÿç‡

        # ğŸ”§ æµ‹è¯•ï¼šå®Œå…¨ç§»é™¤æ­£åˆ™åŒ–ï¼Œçœ‹èƒ½å¦è¿‡æ‹Ÿåˆ
        reg_coeff = 0.0  # ç§»é™¤æ‰€æœ‰æ­£åˆ™åŒ–

        # æœ€ç»ˆæŸå¤± = è´Ÿå¯¹æ•°ä¼¼ç„¶ + u_totalæ­£åˆ™åŒ–
        ce_loss = -cross_entropy.mean()
        u_total_loss = u_total.mean()
        loss = ce_loss + reg_coeff * u_total_loss

        # ç»Ÿè®¡ä¿¡æ¯ï¼šloss
        loss_value = float(loss.item())
        loss_is_nan = bool(torch.isnan(loss).item())
        loss_is_inf = bool(torch.isinf(loss).item())

        # é¢å¤–ç»Ÿè®¡ï¼šåˆ†è§£æŸå¤±é¡¹
        ce_loss_value = float(ce_loss.item())
        u_total_loss_value = float(u_total_loss.item())

        # è®°å½•æ‰€æœ‰ç»Ÿè®¡ä¿¡æ¯åˆ°æ—¥å¿—ï¼ˆä»…åœ¨debugæ¨¡å¼ä¸‹ï¼‰
        if logger is not None and self.debug_mode:
            # æ·»åŠ å½’ä¸€åŒ–ç›¸å…³çš„ç»Ÿè®¡ä¿¡æ¯
            valid_ops_min = int(valid_ops_per_sample.min().item())
            valid_ops_max = int(valid_ops_per_sample.max().item())
            valid_ops_mean = float(valid_ops_per_sample.float().mean().item())

            logger.log(f"LOSS_STATS",
                      f"u_z: min={u_z_min:.6f}, max={u_z_max:.6f}, mean={u_z_mean:.6f}, std={u_z_std:.6f} | "
                      f"zeros={u_z_num_zeros}, near_zero={u_z_has_near_zero}, negative={u_z_has_negative} | "
                      f"log_u_z: min={log_u_z_min:.6f}, max={log_u_z_max:.6f}, mean={log_u_z_mean:.6f} | "
                      f"has_inf={log_u_z_has_inf}, has_nan={log_u_z_has_nan} | "
                      f"u_mask: {u_mask_num_true}/{u_mask_total} ({(1-u_mask_sparsity)*100:.2f}%) | "
                      f"valid_ops_per_sample: min={valid_ops_min}, max={valid_ops_max}, mean={valid_ops_mean:.2f} | "
                      f"cross_entropy: min={cross_entropy_min:.6f}, max={cross_entropy_max:.6f}, "
                      f"mean={cross_entropy_mean:.6f}, std={cross_entropy_std:.6f} | "
                      f"loss: {loss_value:.6f}, is_nan={loss_is_nan}, is_inf={loss_is_inf}",
                      level=2)

        return loss


def prepare_dataset_hf(data_file: str, tokenizer, max_dim: int = 10,
                       max_expr_length: int = 128, stream: bool = True,
                       num_proc: Optional[int] = None):
    """
    ä½¿ç”¨ Hugging Face datasets åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®

    Args:
        data_file: æ•°æ®æ–‡ä»¶è·¯å¾„ (.txtæ ¼å¼ï¼Œæ¯è¡Œä¸€ä¸ªJSONæ ·æœ¬)
        tokenizer: åˆ†è¯å™¨
        max_dim: æœ€å¤§ç»´åº¦
        max_expr_length: è¡¨è¾¾å¼æœ€å¤§é•¿åº¦
        stream: æ˜¯å¦ä½¿ç”¨æµå¼åŠ è½½ï¼ˆé»˜è®¤Trueï¼Œé€‚åˆå¤§æ–‡ä»¶ï¼‰
        num_proc: é¢„å¤„ç†æ—¶çš„è¿›ç¨‹æ•°ï¼ŒNoneè¡¨ç¤ºè‡ªåŠ¨é€‰æ‹©

    Returns:
        dataset: Hugging Face Dataset å¯¹è±¡
    """
    data_files = {"train": data_file}

    # åŠ è½½åŸå§‹æ–‡æœ¬æ•°æ®
    if stream:
        # æµå¼åŠ è½½ï¼šé€‚åˆå¤§æ–‡ä»¶ï¼Œä¸ä¸€æ¬¡æ€§åŠ è½½åˆ°å†…å­˜
        raw_dataset = load_dataset("text", data_files=data_files, split="train", streaming=True)
    else:
        # ä¸€æ¬¡æ€§åŠ è½½ï¼šé€‚åˆå°æ–‡ä»¶ï¼Œåç»­å¤„ç†æ›´å¿«
        raw_dataset = load_dataset("text", data_files=data_files, split="train")

    # è·å–tokenç›¸å…³ä¿¡æ¯
    pad_token_id = tokenizer.convert_tokens_to_ids('<pad>')
    bos_token_id = tokenizer.convert_tokens_to_ids('<s>')
    gap_token_id = tokenizer.convert_tokens_to_ids('<gap>')
    vocab_size = len(tokenizer.get_vocab())

    def process_function(examples):
        """
        é¢„å¤„ç†å‡½æ•°ï¼šå°†JSONå­—ç¬¦ä¸²è½¬æ¢ä¸ºæ¨¡å‹æ‰€éœ€çš„æ ¼å¼
        """
        # å¤„ç†å•ä¸ªæ ·æœ¬ï¼ˆstreamingæ¨¡å¼ï¼Œbatch_size=1æ—¶ï¼‰
        if isinstance(examples, dict) and 'text' in examples:
            text = examples['text']
            # æ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯listï¼ˆstreamingæ¨¡å¼å¯èƒ½æ‰¹å¤„ç†ï¼‰
            if isinstance(text, list):
                lines = text
            else:
                lines = [text]
        else:
            # å¤„ç†batchï¼ˆéstreamingæ¨¡å¼ï¼‰
            lines = examples['text']

        batch_size = len(lines)

        # é¢„åˆ†é…åˆ—è¡¨
        outputs = {
            'x_values': [],
            'y_target': [],
            'residuals': [],
            'z0_token_ids': [],
            'z1_token_ids': [],
            'gap_token': []
        }

        for line in lines:
            sample = json.loads(line)

            # æ·»åŠ æ•°å€¼æ•°æ®
            outputs['x_values'].append(sample['x_values'])
            outputs['y_target'].append(sample['y_target'])
            outputs['residuals'].append(sample['residuals'])

            # Tokenå¤„ç†
            def pad_z_sequence(tokens):
                # è¿‡æ»¤æ‰Noneå€¼ï¼Œå¹¶ç¡®ä¿æ‰€æœ‰tokenéƒ½æ˜¯æ•´æ•°
                filtered_tokens = [t for t in tokens if t is not None and isinstance(t, int)]
                if len(filtered_tokens) != len(tokens):
                    print(f"è­¦å‘Š: è¿‡æ»¤äº† {len(tokens) - len(filtered_tokens)} ä¸ªæ— æ•ˆtoken")

                # æ·»åŠ BOS tokenå¹¶æˆªæ–­
                tokens = [bos_token_id] + filtered_tokens[:max_expr_length-1]
                # Paddingåˆ°å›ºå®šé•¿åº¦
                tokens.extend([pad_token_id] * (max_expr_length - len(tokens)))
                return tokens

            # è½¬æ¢token
            z0_tokens = tokenizer.convert_tokens_to_ids(sample['z0_tokens'])
            z1_tokens = tokenizer.convert_tokens_to_ids(sample['z1_tokens'])

            outputs['z0_token_ids'].append(pad_z_sequence(z0_tokens))
            outputs['z1_token_ids'].append(pad_z_sequence(z1_tokens))
            outputs['gap_token'].append(gap_token_id)

        return outputs

    # åº”ç”¨é¢„å¤„ç†
    if stream:
        # æµå¼æ¨¡å¼ï¼šä½¿ç”¨map (IterableDatasetä¸æ”¯æŒdescå‚æ•°)
        tokenized_dataset = raw_dataset.map(
            process_function,
            batched=True,
            remove_columns=["text"]
        )
    else:
        # éæµå¼æ¨¡å¼ï¼šä½¿ç”¨å¤šè¿›ç¨‹åŠ é€Ÿ
        tokenized_dataset = raw_dataset.map(
            process_function,
            batched=True,
            num_proc=num_proc,
            remove_columns=["text"],
            desc="Preprocessing dataset"
        )

    # è®¾ç½®æ ¼å¼ä¸ºtorchï¼Œè¿™æ ·DataLoaderæ‹¿åˆ°çš„ç›´æ¥æ˜¯Tensor
    if stream:
        # IterableDatasetä½¿ç”¨with_format (ä¸æ”¯æŒcolumnså‚æ•°)
        tokenized_dataset = tokenized_dataset.with_format(type='torch')
    else:
        # æ™®é€šDatasetä½¿ç”¨set_format
        tokenized_dataset.set_format(type='torch', columns=[
            'x_values', 'y_target', 'residuals',
            'z0_token_ids', 'z1_token_ids', 'gap_token'
        ])

    # ä¿å­˜tokenizerå¼•ç”¨ä¾›åç»­ä½¿ç”¨
    tokenized_dataset.tokenizer = tokenizer

    return tokenized_dataset


class FlowDataset(torch.utils.data.Dataset):
    """è¿ç»­æµæ•°æ®é›†åŒ…è£…å™¨ - å…¼å®¹æ—§æ¥å£ï¼Œå†…éƒ¨ä½¿ç”¨ Hugging Face datasets"""

    def __init__(self, data_file: str, tokenizer, max_dim: int = 10,
                 max_expr_length: int = 128, stream: bool = True,
                 num_proc: Optional[int] = None):
        """
        ä½¿ç”¨ Hugging Face datasets çš„æ•°æ®é›†åŒ…è£…å™¨

        Args:
            data_file: æ•°æ®æ–‡ä»¶è·¯å¾„
            tokenizer: åˆ†è¯å™¨
            max_dim: æœ€å¤§ç»´åº¦
            max_expr_length: è¡¨è¾¾å¼æœ€å¤§é•¿åº¦
            stream: æ˜¯å¦ä½¿ç”¨æµå¼åŠ è½½ï¼ˆé»˜è®¤Trueï¼‰
            num_proc: é¢„å¤„ç†æ—¶çš„è¿›ç¨‹æ•°
        """
        self.tokenizer = tokenizer
        self.max_dim = max_dim
        self.max_expr_length = max_expr_length
        self.vocab_size = len(tokenizer.get_vocab())
        self.pad_token = tokenizer.convert_tokens_to_ids('<pad>')
        self.bos_token = tokenizer.convert_tokens_to_ids('<s>')
        self.gap_token = tokenizer.convert_tokens_to_ids('<gap>')
        self.stream = stream  # ä¿å­˜streamæ¨¡å¼æ ‡å¿—

        # ä½¿ç”¨ Hugging Face datasets åŠ è½½æ•°æ®
        self._hf_dataset = prepare_dataset_hf(
            data_file=data_file,
            tokenizer=tokenizer,
            max_dim=max_dim,
            max_expr_length=max_expr_length,
            stream=stream,
            num_proc=num_proc
        )

        # å¦‚æœæ˜¯éæµå¼æ¨¡å¼ï¼Œç¼“å­˜æ•°æ®åˆ—è¡¨ä»¥ä¾¿å¿«é€Ÿè®¿é—®
        if not stream:
            self._data_list = list(self._hf_dataset)
            self._dataset_length = len(self._data_list)
        else:
            self._data_list = None
            # æµå¼æ¨¡å¼ï¼šç›´æ¥ç»Ÿè®¡æ–‡ä»¶è¡Œæ•°ï¼ˆæ¯”éå†datasetå¿«å¾—å¤šï¼‰
            try:
                with open(data_file, 'r', encoding='utf-8') as f:
                    self._dataset_length = sum(1 for _ in f)
            except Exception as e:
                # å¦‚æœæ–‡ä»¶ç»Ÿè®¡å¤±è´¥ï¼Œä½¿ç”¨ä¸€ä¸ªé»˜è®¤çš„å¤§æ•°
                print(f"è­¦å‘Š: æ— æ³•ç»Ÿè®¡æ–‡ä»¶è¡Œæ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼ã€‚é”™è¯¯: {e}")
                self._dataset_length = 1000000  # é»˜è®¤å€¼

    def __len__(self):
        """è¿”å›æ•°æ®é›†å¤§å°"""
        return self._dataset_length

    def __iter__(self):
        """æµå¼æ¨¡å¼ä¸‹çš„è¿­ä»£å™¨"""
        if self._data_list is not None:
            # éæµå¼æ¨¡å¼ï¼šè¿­ä»£ç¼“å­˜åˆ—è¡¨
            return iter(self._data_list)
        else:
            # æµå¼æ¨¡å¼ï¼šç›´æ¥è¿­ä»£Hugging Face dataset
            return self._hf_dataset.__iter__()

    def __getitem__(self, idx):
        """è·å–å•ä¸ªæ ·æœ¬ï¼ˆä»…éæµå¼æ¨¡å¼ï¼‰"""
        if self._data_list is not None:
            # éæµå¼æ¨¡å¼ï¼šç›´æ¥ä»ç¼“å­˜åˆ—è¡¨è·å–
            sample = self._data_list[idx]
        else:
            # æµå¼æ¨¡å¼ï¼šä½¿ç”¨isliceè·³è½¬åˆ°æŒ‡å®šä½ç½®
            # æ³¨æ„ï¼šåœ¨DataLoaderä¸­ä½¿ç”¨IterableDatasetæ—¶ï¼Œ__getitem__ä¸åº”è¯¥è¢«è°ƒç”¨
            from itertools import islice
            sample = next(islice(self._hf_dataset, idx, None))

        # è½¬æ¢ä¸ºTensorï¼ˆå¦‚æœæ˜¯numpyçš„è¯ï¼‰
        result = {
            'x_values': torch.FloatTensor(sample['x_values'])
                if not isinstance(sample['x_values'], torch.Tensor) else sample['x_values'],
            'y_target': torch.FloatTensor(sample['y_target'])
                if not isinstance(sample['y_target'], torch.Tensor) else sample['y_target'],
            'residuals': torch.FloatTensor(sample['residuals'])
                if not isinstance(sample['residuals'], torch.Tensor) else sample['residuals'],
            'z0_token_ids': sample['z0_token_ids'].long()
                if not isinstance(sample['z0_token_ids'], torch.Tensor) else sample['z0_token_ids'],
            'z1_token_ids': sample['z1_token_ids'].long()
                if not isinstance(sample['z1_token_ids'], torch.Tensor) else sample['z1_token_ids'],
            'gap_token': sample['gap_token'].item()
                if isinstance(sample['gap_token'], torch.Tensor) else sample['gap_token']
        }

        return result


def custom_collate_fn(batch):
    """å¤„ç†ä¸åŒç»´åº¦æ•°æ®çš„collateå‡½æ•°ï¼Œä½¿ç”¨padding + maskæ–¹æ¡ˆ

    ä¿®æ”¹ï¼šæ·»åŠ y_targetçš„å¤„ç†ï¼ˆæ¶æ„æ”¹è¿›ï¼šä½¿ç”¨ç›®æ ‡å€¼è€Œéæ®‹å·®ä½œä¸ºæ¡ä»¶ï¼‰
    """
    if len(batch) == 0:
        return {
            'x_values': torch.empty(0, 0, 0),
            'y_target': torch.empty(0, 0),  # ä¿®æ”¹ï¼šæ·»åŠ y_target
            'residuals': torch.empty(0, 0),
            'dim_mask': torch.empty(0, 0),
            'z0_token_ids': torch.empty(0, 0),
            'z1_token_ids': torch.empty(0, 0),
            'gap_token': None
        }

    # æ‰¾åˆ°æœ€å¤§ç»´åº¦å’Œn_points
    max_dim = 0
    max_n_points = 0
    original_dims = []

    for i, item in enumerate(batch):
        x_val = item['x_values']  # [n_points, current_dim]
        y_tgt = item['y_target']  # [n_points]  # æ–°å¢ï¼šè·å–y_target

        # ç¡®ä¿x_valuesè‡³å°‘æ˜¯2ç»´çš„
        if x_val.dim() == 1:
            x_val = x_val.unsqueeze(1)  # [n_points, 1]

        # ç¡®ä¿y_targetæ˜¯1ç»´çš„
        if y_tgt.dim() > 1:
            y_tgt = y_tgt.squeeze()

        current_dim = x_val.shape[1]
        current_n_points = x_val.shape[0]

        max_dim = max(max_dim, current_dim)
        max_n_points = max(max_n_points, current_n_points)
        original_dims.append(current_dim)

    # Paddingæ‰€æœ‰æ•°æ®åˆ°æœ€å¤§å½¢çŠ¶
    x_values_padded = []
    y_target_padded = []  # æ–°å¢ï¼šy_targetçš„padding
    residuals_padded = []
    dim_masks = []
    point_masks = []

    for i, item in enumerate(batch):
        x_val = item['x_values'].clone()  # [n_points, current_dim]
        y_tgt = item['y_target'].clone()  # [n_points]  # æ–°å¢ï¼šå…‹éš†y_target
        resid = item['residuals'].clone()  # [n_points]  # ä¿ç•™ï¼šç”¨äºå‘åå…¼å®¹

        # ç¡®ä¿x_valuesè‡³å°‘æ˜¯2ç»´çš„
        if x_val.dim() == 1:
            x_val = x_val.unsqueeze(1)

        # ç¡®ä¿y_targetå’Œresidualsæ˜¯1ç»´çš„
        if y_tgt.dim() > 1:
            y_tgt = y_tgt.squeeze()
        if resid.dim() > 1:
            resid = resid.squeeze()

        current_n_points = x_val.shape[0]
        current_dim = x_val.shape[1]

        # Padding n_pointsç»´åº¦ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if current_n_points < max_n_points:
            padding_points = torch.zeros(max_n_points - current_n_points, x_val.shape[1], dtype=x_val.dtype)
            x_val = torch.cat([x_val, padding_points], dim=0)

            padding_y_tgt = torch.zeros(max_n_points - current_n_points, dtype=y_tgt.dtype)
            y_tgt = torch.cat([y_tgt, padding_y_tgt], dim=0)

            padding_resid = torch.zeros(max_n_points - current_n_points, dtype=resid.dtype)
            resid = torch.cat([resid, padding_resid], dim=0)

        # Padding dimç»´åº¦
        if current_dim < max_dim:
            padding_dim = torch.zeros(x_val.shape[0], max_dim - current_dim, dtype=x_val.dtype)
            x_val = torch.cat([x_val, padding_dim], dim=1)

        # åˆ›å»ºç»´åº¦maskï¼š1è¡¨ç¤ºæœ‰æ•ˆç»´åº¦ï¼Œ0è¡¨ç¤ºpadding
        dim_mask = torch.zeros(max_dim, dtype=torch.float32)
        dim_mask[:current_dim] = 1.0

        # åˆ›å»ºç‚¹maskï¼š1è¡¨ç¤ºçœŸå®ç‚¹ï¼Œ0è¡¨ç¤ºå¡«å……ç‚¹
        point_mask = torch.zeros(max_n_points, dtype=torch.float32)
        point_mask[:current_n_points] = 1.0

        x_values_padded.append(x_val)          # [max_n_points, max_dim]
        y_target_padded.append(y_tgt)          # [max_n_points]  # æ–°å¢
        residuals_padded.append(resid)         # [max_n_points]
        dim_masks.append(dim_mask)             # [max_dim]
        point_masks.append(point_mask)         # [max_n_points]

    result = {
        'x_values': torch.stack(x_values_padded),       # [batch_size, max_n_points, max_dim]
        'y_target': torch.stack(y_target_padded),       # [batch_size, max_n_points]  # æ–°å¢
        'residuals': torch.stack(residuals_padded),     # [batch_size, max_n_points]  # ä¿ç•™
        'dim_mask': torch.stack(dim_masks),             # [batch_size, max_dim]
        'point_mask': torch.stack(point_masks),         # [batch_size, max_n_points]
        'z0_token_ids': torch.stack([item['z0_token_ids'] for item in batch]),
        'z1_token_ids': torch.stack([item['z1_token_ids'] for item in batch]),
        'gap_token': batch[0]['gap_token'],
        'original_dims': original_dims  # è®°å½•åŸå§‹ç»´åº¦ï¼Œä¾›è°ƒè¯•ä½¿ç”¨
    }

    return result


