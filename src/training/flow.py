"""
EditFlowè¿ç»­æµåŒ¹é…çš„æ ¸å¿ƒç»„ä»¶
"""

import torch
import torch.nn.functional as F
from typing import Tuple, Optional
from datasets import load_dataset


def remove_gap_tokens(z_t: torch.Tensor, tokenizer) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
    """ç§»é™¤gap tokenå¹¶è¿”å›å¤„ç†åçš„åºåˆ—

    é‡è¦ï¼šæ­¤å‡½æ•°åªç§»é™¤ gap_tokenï¼Œä¿ç•™ BOS token å’Œæ‰€æœ‰å…¶ä»– tokens
    ç¡®ä¿è¿”å›çš„åºåˆ—æ ¼å¼ä¸º [BOS] + [non_gap_tokens] + [PAD...]
    """
    pad_token_id = tokenizer.convert_tokens_to_ids('<pad>')
    gap_token_id = tokenizer.convert_tokens_to_ids('<gap>')
    bos_token_id = tokenizer.convert_tokens_to_ids('<s>')
    batch_size = z_t.shape[0]
    device = z_t.device

    z_gap_mask = (z_t == gap_token_id)
    z_pad_mask = (z_t == pad_token_id)

    # ä½¿ç”¨æ©ç æ“ä½œç§»é™¤gap tokensï¼ˆåªç§»é™¤gapï¼Œä¿ç•™BOSå’Œå…¶ä»–tokensï¼‰
    x_t_list = []
    for i in range(batch_size):
        non_gap_mask = ~z_gap_mask[i]
        x_row = z_t[i][non_gap_mask]
        x_t_list.append(x_row)

        # éªŒè¯ï¼šç¡®ä¿BOS tokenè¢«ä¿ç•™ï¼ˆå¦‚æœè¾“å…¥ä¸­å­˜åœ¨ï¼‰
        if len(x_row) > 0 and z_t[i, 0] == bos_token_id:
            assert x_row[0] == bos_token_id, f"BOS tokenå¿…é¡»è¢«ä¿ç•™åœ¨ä½ç½®0"

    max_len = max(len(x) for x in x_t_list)
    x_t_padded = torch.full((batch_size, max_len), pad_token_id, dtype=torch.long, device=device)
    x_pad_mask_padded = torch.zeros((batch_size, max_len), dtype=torch.bool, device=device)

    for i, x_row in enumerate(x_t_list):
        x_t_padded[i, :len(x_row)] = x_row
        x_pad_mask_padded[i, :len(x_row)] = (x_row == pad_token_id)

    return x_t_padded, x_pad_mask_padded, z_gap_mask, z_pad_mask


def fill_gap_tokens_with_repeats(x_ut: torch.Tensor, z_gap_mask: torch.Tensor, z_pad_mask: torch.Tensor) -> torch.Tensor:
    """ç”¨é‡å¤å€¼å¡«å……gap tokenä½ç½®"""
    batch_size = z_gap_mask.shape[0]
    x_seq_len = x_ut.shape[1]

    # è®¡ç®—æ¯ä¸ªä½ç½®å¯¹åº”çš„égapä½ç½®
    non_gap_mask = ~z_gap_mask
    indices = non_gap_mask.cumsum(dim=1) - 1
    indices = indices.clamp(min=0, max=x_seq_len-1)

    # æ”¶é›†å¯¹åº”çš„ç‰¹å¾
    batch_indices = torch.arange(batch_size, device=x_ut.device).unsqueeze(1)
    result = x_ut[batch_indices, indices]
    result[z_pad_mask] = 0

    return result


class ContinuousFlowLoss:
    """è¿ç»­æ—¶é—´æµåŒ¹é…æŸå¤±å‡½æ•°ï¼ˆæ¶æ„v2.0 - å›ºå®št=0ï¼Œä¸å†éœ€è¦è°ƒåº¦å™¨ï¼‰"""

    def __init__(self, debug_mode=False):
        self.debug_mode = debug_mode

    def make_ut_mask_from_z(self, z_t: torch.Tensor, z_1: torch.Tensor, vocab_size: int,
                           gap_token: int, tokenizer, x_t: torch.Tensor) -> torch.Tensor:
        """
        ä½¿ç”¨æ˜ç¡®çš„ä½ç½®æ˜ å°„è¡¨ç”Ÿæˆæ­£ç¡®çš„ç¼–è¾‘æ“ä½œæ©ç 

        æ ¸å¿ƒæ”¹è¿›ï¼šæ„å»ºZç©ºé—´åˆ°Xç©ºé—´çš„æ˜ç¡®æ˜ å°„è¡¨ï¼Œé¿å…åŒç´¢å¼•éå†å¯¼è‡´çš„ä½ç½®é”™ä½é—®é¢˜

        Args:
            z_t: å½“å‰çŠ¶æ€ï¼ˆZç©ºé—´ï¼Œå«gapï¼‰[batch, z_seq_len]
            z_1: ç›®æ ‡çŠ¶æ€ï¼ˆZç©ºé—´ï¼Œå«gapï¼‰[batch, z_seq_len]
            vocab_size: è¯æ±‡è¡¨å¤§å°
            gap_token: gap tokençš„ID
            tokenizer: åˆ†è¯å™¨
            x_t: å½“å‰çŠ¶æ€ï¼ˆXç©ºé—´ï¼Œæ— gapï¼‰[batch, x_seq_len]

        Returns:
            u_mask: ç¼–è¾‘æ“ä½œæ©ç  [batch, x_seq_len, 2*vocab_size+2]
                    ç»´åº¦å¸ƒå±€ï¼š[INS(vocab_size) | DEL(1) | SUB(vocab_size) | KEEP(1)]
        """
        batch_size, z_seq_len = z_t.shape
        x_seq_len = x_t.shape[1]
        n_ops = 2 * vocab_size + 2  # æ’å…¥(vocab_size) + åˆ é™¤(1) + æ›¿æ¢(vocab_size) + KEEP(1)

        pad_token = tokenizer.convert_tokens_to_ids('<pad>')

        # åˆå§‹åŒ–è¾“å‡ºæ©ç ï¼ˆåœ¨Xç©ºé—´ï¼‰
        u_mask = torch.zeros((batch_size, x_seq_len, n_ops), dtype=torch.int, device=z_t.device)

        # å¯¹æ¯ä¸ªæ ·æœ¬è¿›è¡Œå¤„ç†
        for b in range(batch_size):
            # === æ­¥éª¤1ï¼šæ„å»ºZç©ºé—´åˆ°Xç©ºé—´çš„ä½ç½®æ˜ å°„è¡¨ ===
            # ä½ç½®è¯´æ˜ï¼šXç©ºé—´ä½ç½®0=BOS tokenï¼Œä½ç½®1,2,3...=åºåˆ—ä¸­çš„å®é™…token
            z_to_x_map = {}  # {z_pos: x_pos or None}
            insert_positions = []  # è®°å½•æ‰€æœ‰gapä½ç½®ï¼Œç”¨äºåç»­INSERTæ“ä½œæ˜ å°„

            # ğŸ”§ ä¿®å¤ï¼šBOSä½ç½®ä¹Ÿåº”è¯¥å‚ä¸æ˜ å°„ï¼Œè¿™æ ·gapä½ç½®æ‰èƒ½æ‰¾åˆ°ä¹‹å‰çš„égapä½ç½®
            # ä¿®æ”¹å‰ï¼šè·³è¿‡BOSä½ç½®ï¼Œå¯¼è‡´gapæ‰¾ä¸åˆ°ä¹‹å‰çš„égapï¼ŒINSERTæ ‡è®°é”™è¯¯
            # ä¿®æ”¹åï¼šBOSæ˜ å°„åˆ°Xç©ºé—´ä½ç½®0ï¼ˆä½ç½®0=BOSï¼‰ï¼Œgapå¯ä»¥æ­£ç¡®æ‰¾åˆ°BOSä½œä¸ºæ’å…¥ç‚¹
            bos_token = tokenizer.convert_tokens_to_ids('<s>')

            # ä»Xç©ºé—´ä½ç½®0å¼€å§‹æ˜ å°„ï¼ˆä½ç½®0æ˜¯BOS tokenï¼‰
            x_index = 0

            for z_pos in range(z_seq_len):
                token_t = z_t[b, z_pos].item()

                # è·³è¿‡padä½ç½®
                if token_t == pad_token:
                    z_to_x_map[z_pos] = None
                    continue

                # ğŸ”§ ä¿®å¤ï¼šBOS tokenä¹Ÿè¦æ˜ å°„åˆ°Xç©ºé—´ï¼Œè¿™æ ·gapä½ç½®æ‰èƒ½æ‰¾åˆ°BOSä½œä¸ºæ’å…¥ç‚¹
                # ä¿®æ”¹å‰ï¼šBOSä¸æ˜ å°„ï¼Œå¯¼è‡´gapæ‰¾ä¸åˆ°ä¹‹å‰çš„égapä½ç½®
                # ä¿®æ”¹åï¼šBOSæ˜ å°„åˆ°Xç©ºé—´ä½ç½®0ï¼ˆä½ç½®0=BOSï¼Œè¿™æ˜¯åºåˆ—çš„å¼€å§‹ä½ç½®ï¼‰
                if token_t == bos_token:
                    z_to_x_map[z_pos] = x_index  # BOSæ˜ å°„åˆ°Xç©ºé—´ä½ç½®0
                    x_index += 1
                    continue

                if token_t != gap_token:
                    # égapä½ç½®ï¼šæ˜ å°„åˆ°Xç©ºé—´ä½ç½®
                    z_to_x_map[z_pos] = x_index
                    x_index += 1
                else:
                    # gapä½ç½®ï¼šä¸å ç”¨Xç©ºé—´ä½ç½®ï¼Œä½†è®°å½•ä¸ºæ’å…¥ç‚¹
                    z_to_x_map[z_pos] = None
                    insert_positions.append(z_pos)

            # === æ­¥éª¤2ï¼šç¬¬ä¸€éå¤„ç† - å¤„ç†æ‰€æœ‰égapä½ç½®çš„æ“ä½œ ===
            for z_pos in range(z_seq_len):
                token_t = z_t[b, z_pos].item()
                token_1 = z_1[b, z_pos].item()

                # è·³è¿‡padä½ç½®
                if token_t == pad_token or token_1 == pad_token:
                    continue

                # åªå¤„ç†égapä½ç½®ï¼ˆSUBSTITUTE/DELETE/KEEPï¼‰
                if token_t == gap_token:
                    continue  # gapä½ç½®çš„INSERTæ“ä½œåœ¨ç¬¬äºŒéå¤„ç†

                x_pos = z_to_x_map[z_pos]
                if x_pos is None or x_pos >= x_seq_len:
                    continue

                # åˆ¤æ–­æ“ä½œç±»å‹
                if token_1 == gap_token:
                    # DELETEæ“ä½œ
                    u_mask[b, x_pos, vocab_size] = 1  # DELETEåœ¨ä½ç½®vocab_size
                elif token_t != token_1:
                    # SUBSTITUTEæ“ä½œ
                    u_mask[b, x_pos, token_1 + vocab_size + 1] = 1  # SUBSTITUTEåœ¨vocab_size+1ä¹‹å
                else:
                    # KEEPæ“ä½œ
                    u_mask[b, x_pos, -1] = 1  # KEEPåœ¨æœ€åä¸€ä½

            # === æ­¥éª¤3ï¼šç¬¬äºŒéå¤„ç† - å¤„ç†æ‰€æœ‰gapä½ç½®çš„INSERTæ“ä½œ ===
            for gap_z_pos in insert_positions:
                token_t = z_t[b, gap_z_pos].item()
                token_1 = z_1[b, gap_z_pos].item()

                # è·³è¿‡padä½ç½®
                if token_t == pad_token or token_1 == pad_token:
                    continue

                # åªå¤„ç† gap â†’ égap çš„INSERTæ“ä½œ
                if token_t == gap_token and token_1 != gap_token:
                    # INSERTæ“ä½œè¯­ä¹‰ï¼šåœ¨æŸä¸ªä½ç½®ä¹‹åæ’å…¥token
                    # ä¾‹å¦‚ï¼šgapåœ¨ä½ç½®1ï¼Œè¡¨ç¤ºåœ¨ä½ç½®0çš„å…ƒç´ ä¹‹åæ’å…¥
                    # Zç©ºé—´: [..., token_A, <gap>, token_B, ...]
                    #      â†’ [..., token_A, NEW_TOKEN, token_B, ...]
                    # Xç©ºé—´: [..., token_A, token_B, ...]
                    #   INSERTåœ¨ä½ç½®0 â†’ [..., token_A, NEW_TOKEN, token_B, ...]

                    # ç¡®å®šINSERTæ“ä½œçš„ç›®æ ‡Xç©ºé—´ä½ç½®
                    # ç­–ç•¥ï¼šINSERTæ“ä½œæ˜ å°„åˆ°gapä¹‹å‰çš„ç¬¬ä¸€ä¸ªégapä½ç½®
                    # è¡¨ç¤º"åœ¨è¯¥ä½ç½®ä¹‹åæ’å…¥"

                    # æ‰¾åˆ°gapä¹‹å‰çš„ç¬¬ä¸€ä¸ªégapä½ç½®çš„Xç©ºé—´ç´¢å¼•
                    insert_x_pos = None
                    for prev_z_pos in range(gap_z_pos - 1, -1, -1):
                        if z_to_x_map[prev_z_pos] is not None:
                            insert_x_pos = z_to_x_map[prev_z_pos]
                            break

                    # å¦‚æœgapä¹‹å‰æ²¡æœ‰égapä½ç½®ï¼Œæ’å…¥åˆ°x_tçš„å¼€å¤´ï¼ˆä½ç½®0ï¼‰
                    # è¿™è¡¨ç¤ºåœ¨åºåˆ—æœ€å‰é¢æ’å…¥ï¼ˆä½ç½®0æ˜¯BOSï¼Œæ‰€ä»¥å®é™…æ˜¯åœ¨BOSä¹‹åæ’å…¥ï¼‰
                    if insert_x_pos is None:
                        insert_x_pos = 0

                    # æ ‡è®°INSERTæ“ä½œ
                    if 0 <= insert_x_pos < x_seq_len:
                        # æ£€æŸ¥è¯¥ä½ç½®æ˜¯å¦å·²æœ‰KEEPæ“ä½œ
                        if u_mask[b, insert_x_pos, -1].item() == 1:
                            # å¦‚æœæœ‰KEEPæ“ä½œï¼Œç§»é™¤KEEPï¼Œå› ä¸ºINSERTä¼˜å…ˆçº§æ›´é«˜
                            u_mask[b, insert_x_pos, -1] = 0

                        # æ ‡è®°INSERTæ“ä½œï¼šu_mask[b, insert_x_pos, token_1] = 1
                        # è¯­ä¹‰ï¼šåœ¨ä½ç½®insert_x_posçš„å…ƒç´ ä¹‹åæ’å…¥token_1
                        u_mask[b, insert_x_pos, token_1] = 1  # INSERTåœ¨0~vocab_size-1

        return u_mask

    def __call__(self, u_cat_x: torch.Tensor, u_z: torch.Tensor, u_mask: torch.Tensor,
                 vocab_size: int, accelerator=None, logger=None) -> torch.Tensor:
        """
        è¿ç»­æµæŸå¤±è®¡ç®—

        Args:
            u_cat_x: Xç©ºé—´çš„é¢„æµ‹é€Ÿç‡ [batch, x_seq_len, 2*vocab_size+1]ï¼ˆåŸå§‹ç©ºé—´ï¼Œä¸å«gapï¼‰
            u_z: Zç©ºé—´çš„é¢„æµ‹é€Ÿç‡ [batch, z_seq_len, 2*vocab_size+1]ï¼ˆæ‰©å±•ç©ºé—´ï¼Œå«gapé‡å¤ï¼‰
            u_mask: æ“ä½œæ©ç  [batch, z_seq_len, 2*vocab_size+1]
            vocab_size: è¯æ±‡è¡¨å¤§å°
            accelerator: AccelerateåŠ é€Ÿå™¨
            logger: æ—¥å¿—è®°å½•å™¨ï¼ˆå¯é€‰ï¼‰

        Returns:
            loss: æ ‡é‡æŸå¤±å€¼
        """
        # NaN/Infæ£€æµ‹ - æ£€æµ‹ä¸¤ä¸ªè¾“å…¥
        nan_count_x = torch.isnan(u_cat_x).sum().item()
        inf_count_x = torch.isinf(u_cat_x).sum().item()
        nan_count_z = torch.isnan(u_z).sum().item()
        inf_count_z = torch.isinf(u_z).sum().item()

        # åˆ†å¸ƒå¼NaNæ£€æµ‹ï¼šå¦‚æœä½¿ç”¨acceleratorï¼Œç¡®ä¿æ‰€æœ‰è¿›ç¨‹åŒæ­¥æ£€æµ‹ç»“æœ
        has_nan_or_inf = torch.tensor(1 if ((nan_count_x > 0 or inf_count_x > 0) or (nan_count_z > 0 or inf_count_z > 0)) else 0,
                                    device=u_cat_x.device)

        if accelerator is not None and accelerator.distributed_type != "NO":
            # åŒæ­¥æ‰€æœ‰è¿›ç¨‹çš„NaNæ£€æµ‹ç»“æœ
            # ä½¿ç”¨accelerator.gatheræ”¶é›†æ‰€æœ‰è¿›ç¨‹çš„NaNæ£€æµ‹ç»“æœ
            gathered_results = accelerator.gather(has_nan_or_inf)
            has_nan_or_inf = gathered_results.sum()
        else:
            has_nan_or_inf = has_nan_or_inf.item()

        # è®°å½•NaN/Infæ£€æµ‹ç»“æœï¼ˆä»…ç”¨äºç›‘æ§ï¼Œä¸å†æŠ›å‡ºå¼‚å¸¸è·³è¿‡ï¼‰
        if has_nan_or_inf > 0 and logger is not None and accelerator.is_local_main_process:
            logger.error("INPUT_NAN_INF",
                        f"æ£€æµ‹åˆ°å¼‚å¸¸å€¼: Xç©ºé—´NaN={nan_count_x}, Inf={inf_count_x}, "
                        f"Zç©ºé—´NaN={nan_count_z}, Inf={inf_count_z}, åˆ†å¸ƒå¼æ£€æµ‹={has_nan_or_inf}",
                        "compute_loss", level=1)

        # è·å–æ“ä½œç©ºé—´ç»´åº¦
        _, _, n_ops = u_z.shape

        # ä½¿ç”¨æ ‡å‡†cross_entropyè®¡ç®—loss
        # æœ‰äº†KEEPæ“ä½œåï¼Œæ¯ä¸ªtokenä½ç½®éƒ½æœ‰ä¸€ä¸ªæ˜ç¡®çš„æ“ä½œæ ‡ç­¾ï¼š
        # - éœ€è¦ç¼–è¾‘çš„ä½ç½®ï¼šæ ‡è®° ins/del/sub
        # - ä¸éœ€è¦ç¼–è¾‘çš„ä½ç½®ï¼šæ ‡è®° KEEP
        # - åªæœ‰paddingä½ç½®æ— æ ‡ç­¾
        #
        # å› æ­¤å¯ä»¥å®‰å…¨åœ°ä½¿ç”¨æ ‡å‡†cross_entropyï¼Œæ— éœ€å¤æ‚çš„æ©ç é€»è¾‘

        # æ­¥éª¤1: å°†one-hotç¼–ç çš„u_maskè½¬æ¢ä¸ºæ ‡ç­¾ç´¢å¼•
        # u_mask: [batch, z_seq_len, n_ops] -> target_ids: [batch, z_seq_len]
        target_ids = u_mask.argmax(dim=-1)

        # æ­¥éª¤2: è®¡ç®—æ ‡å‡†cross_entropyï¼ˆå¯¹æ¯ä¸ªtokenä½ç½®ï¼‰
        # u_z: [batch, z_seq_len, n_ops] -> [batch*z_seq_len, n_ops]
        # target_ids: [batch, z_seq_len] -> [batch*z_seq_len]
        loss_per_token = F.cross_entropy(
            u_z.reshape(-1, n_ops),
            target_ids.reshape(-1),
            reduction='none'  # å…ˆä¸å½’ä¸€åŒ–ï¼Œåç»­æ‰‹åŠ¨å¤„ç†
        )  # [batch*z_seq_len]

        # æ­¥éª¤3: è¿‡æ»¤paddingä½ç½®ï¼ˆä½¿ç”¨u_maskåˆ¤æ–­ï¼‰
        # å¦‚æœæŸä¸ªä½ç½®æ‰€æœ‰æ“ä½œéƒ½æ˜¯0ï¼Œè¯´æ˜æ˜¯padding
        valid_positions_mask = (u_mask.sum(dim=-1) > 0)  # [batch, z_seq_len]
        valid_positions_mask_flat = valid_positions_mask.reshape(-1)  # [batch*z_seq_len]

        # åªå¯¹æœ‰æ•ˆä½ç½®è®¡ç®—loss
        loss_per_token = loss_per_token[valid_positions_mask_flat]

        # æ­¥éª¤4: æŒ‰æ ·æœ¬å½’ä¸€åŒ–ï¼ˆé¿å…é•¿åºåˆ—ä¸»å¯¼lossï¼‰
        # è®°å½•æ¯ä¸ªæ ·æœ¬çš„æœ‰æ•ˆtokenæ•°
        valid_tokens_per_sample = valid_positions_mask.sum(dim=1)  # [batch]
        valid_tokens_per_sample = valid_tokens_per_sample.clamp(min=1)  # é¿å…é™¤0

        # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„å¹³å‡loss
        sample_losses = []
        start_idx = 0
        for num_tokens in valid_tokens_per_sample:
            end_idx = start_idx + num_tokens.item()
            sample_losses.append(loss_per_token[start_idx:end_idx].mean())
            start_idx = end_idx

        cross_entropy = torch.stack(sample_losses)  # [batch]

        # æœ€ç»ˆæŸå¤±ï¼šäº¤å‰ç†µæŸå¤±
        return cross_entropy.mean()


def prepare_dataset_hf(data_file: str, tokenizer, max_expr_length: int = 128,
                       stream: bool = True, num_proc: Optional[int] = None):
    """
    ä½¿ç”¨ Hugging Face datasets åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®

    Args:
        data_file: æ•°æ®æ–‡ä»¶è·¯å¾„ (.parquetæ ¼å¼)
        tokenizer: åˆ†è¯å™¨
        max_expr_length: è¡¨è¾¾å¼æœ€å¤§é•¿åº¦
        stream: æ˜¯å¦ä½¿ç”¨æµå¼åŠ è½½ï¼ˆé»˜è®¤Trueï¼Œé€‚åˆå¤§æ–‡ä»¶ï¼‰
        num_proc: é¢„å¤„ç†æ—¶çš„è¿›ç¨‹æ•°ï¼ŒNoneè¡¨ç¤ºè‡ªåŠ¨é€‰æ‹©

    Returns:
        dataset: Hugging Face Dataset å¯¹è±¡
    """
    data_files = {"train": data_file}

    # åŠ è½½Parquetæ ¼å¼æ•°æ®
    if stream:
        # æµå¼åŠ è½½ï¼šé€‚åˆå¤§æ–‡ä»¶ï¼Œä¸ä¸€æ¬¡æ€§åŠ è½½åˆ°å†…å­˜
        raw_dataset = load_dataset("parquet", data_files=data_files, split="train", streaming=True)
    else:
        # ä¸€æ¬¡æ€§åŠ è½½ï¼šé€‚åˆå°æ–‡ä»¶ï¼Œåç»­å¤„ç†æ›´å¿«
        raw_dataset = load_dataset("parquet", data_files=data_files, split="train")

    # è·å–tokenç›¸å…³ä¿¡æ¯
    pad_token_id = tokenizer.convert_tokens_to_ids('<pad>')
    bos_token_id = tokenizer.convert_tokens_to_ids('<s>')
    gap_token_id = tokenizer.convert_tokens_to_ids('<gap>')

    def process_function(examples):
        """
        é¢„å¤„ç†å‡½æ•°ï¼šå°†Parquetæ•°æ®è½¬æ¢ä¸ºæ¨¡å‹æ‰€éœ€çš„æ ¼å¼
        Parquetç›´æ¥è¿”å›å­—å…¸ï¼Œä¸éœ€è¦json.loads
        """
        # ParquetåŠ è½½åç›´æ¥æ˜¯å­—å…¸åˆ—è¡¨
        # è·å–batch size
        if isinstance(examples['x_values'], list):
            batch_size = len(examples['x_values'])
        else:
            # å•ä¸ªæ ·æœ¬çš„æƒ…å†µ
            batch_size = 1
            examples = {k: [v] for k, v in examples.items()}

        # é¢„åˆ†é…åˆ—è¡¨
        outputs = {
            'x_values': examples['x_values'],
            'y_target': examples['y_target'],
            'residuals': examples['residuals'],
            'z0_token_ids': [],
            'z1_token_ids': [],
            'gap_token': []
        }

        # Tokenå¤„ç†
        def pad_z_sequence(tokens):
            # è¿‡æ»¤æ‰Noneå€¼ï¼Œå¹¶ç¡®ä¿æ‰€æœ‰tokenéƒ½æ˜¯æ•´æ•°
            filtered_tokens = [t for t in tokens if t is not None and isinstance(t, int)]
            if len(filtered_tokens) != len(tokens):
                print(f"è­¦å‘Š: è¿‡æ»¤äº† {len(tokens) - len(filtered_tokens)} ä¸ªæ— æ•ˆtoken")

            # æ·»åŠ BOS tokenå¹¶æˆªæ–­
            tokens = [bos_token_id] + filtered_tokens[:max_expr_length-1]
            # Paddingåˆ°å›ºå®šé•¿åº¦
            tokens.extend([pad_token_id] * (max_expr_length - len(tokens)))
            return tokens

        # è½¬æ¢tokenï¼ˆéœ€è¦é€ä¸ªå¤„ç†å› ä¸ºéœ€è¦paddingï¼‰
        for i in range(batch_size):
            z0_tokens = tokenizer.convert_tokens_to_ids(examples['z0_tokens'][i])
            z1_tokens = tokenizer.convert_tokens_to_ids(examples['z1_tokens'][i])

            outputs['z0_token_ids'].append(pad_z_sequence(z0_tokens))
            outputs['z1_token_ids'].append(pad_z_sequence(z1_tokens))
            outputs['gap_token'].append(gap_token_id)

        return outputs

    # åº”ç”¨é¢„å¤„ç†
    if stream:
        # æµå¼æ¨¡å¼ï¼šä½¿ç”¨map (IterableDatasetä¸æ”¯æŒdescå‚æ•°)
        tokenized_dataset = raw_dataset.map(
            process_function,
            batched=True,
            remove_columns=['x_values', 'y_target', 'residuals', 'z0_tokens', 'z1_tokens']
        )
    else:
        # éæµå¼æ¨¡å¼ï¼šä½¿ç”¨å¤šè¿›ç¨‹åŠ é€Ÿ
        tokenized_dataset = raw_dataset.map(
            process_function,
            batched=True,
            num_proc=num_proc,
            remove_columns=['x_values', 'y_target', 'residuals', 'z0_tokens', 'z1_tokens'],
            desc="Preprocessing dataset"
        )

    # è®¾ç½®æ ¼å¼ä¸ºtorchï¼Œè¿™æ ·DataLoaderæ‹¿åˆ°çš„ç›´æ¥æ˜¯Tensor
    if stream:
        # IterableDatasetä½¿ç”¨with_format (ä¸æ”¯æŒcolumnså‚æ•°)
        tokenized_dataset = tokenized_dataset.with_format(type='torch')
    else:
        # æ™®é€šDatasetä½¿ç”¨set_format
        tokenized_dataset.set_format(type='torch', columns=[
            'x_values', 'y_target', 'residuals',
            'z0_token_ids', 'z1_token_ids', 'gap_token'
        ])

    # ä¿å­˜tokenizerå¼•ç”¨ä¾›åç»­ä½¿ç”¨
    tokenized_dataset.tokenizer = tokenizer

    return tokenized_dataset


class FlowDataset(torch.utils.data.Dataset):
    """è¿ç»­æµæ•°æ®é›†åŒ…è£…å™¨ - å…¼å®¹æ—§æ¥å£ï¼Œå†…éƒ¨ä½¿ç”¨ Hugging Face datasets"""

    def __init__(self, data_file: str, tokenizer, max_dim: int = 10,
                 max_expr_length: int = 128, stream: bool = True,
                 num_proc: Optional[int] = None):
        """
        ä½¿ç”¨ Hugging Face datasets çš„æ•°æ®é›†åŒ…è£…å™¨

        Args:
            data_file: æ•°æ®æ–‡ä»¶è·¯å¾„
            tokenizer: åˆ†è¯å™¨
            max_dim: æœ€å¤§ç»´åº¦ï¼ˆä¿ç•™ç”¨äºå‘åå…¼å®¹ï¼‰
            max_expr_length: è¡¨è¾¾å¼æœ€å¤§é•¿åº¦
            stream: æ˜¯å¦ä½¿ç”¨æµå¼åŠ è½½ï¼ˆé»˜è®¤Trueï¼‰
            num_proc: é¢„å¤„ç†æ—¶çš„è¿›ç¨‹æ•°
        """
        self.tokenizer = tokenizer
        self.max_dim = max_dim
        self.max_expr_length = max_expr_length
        self.vocab_size = len(tokenizer.get_vocab())
        self.pad_token = tokenizer.convert_tokens_to_ids('<pad>')
        self.bos_token = tokenizer.convert_tokens_to_ids('<s>')
        self.gap_token = tokenizer.convert_tokens_to_ids('<gap>')
        self.stream = stream  # ä¿å­˜streamæ¨¡å¼æ ‡å¿—

        # ä½¿ç”¨ Hugging Face datasets åŠ è½½æ•°æ®
        self._hf_dataset = prepare_dataset_hf(
            data_file=data_file,
            tokenizer=tokenizer,
            max_expr_length=max_expr_length,
            stream=stream,
            num_proc=num_proc
        )

        # å¦‚æœæ˜¯éæµå¼æ¨¡å¼ï¼Œç¼“å­˜æ•°æ®åˆ—è¡¨ä»¥ä¾¿å¿«é€Ÿè®¿é—®
        if not stream:
            self._data_list = list(self._hf_dataset)
            self._dataset_length = len(self._data_list)
        else:
            self._data_list = None
            # æµå¼æ¨¡å¼ï¼šä»parquetå…ƒæ•°æ®è·å–è¡Œæ•°ï¼ˆå¿«é€Ÿå‡†ç¡®ï¼‰
            try:
                import pyarrow.parquet as pq
                pf = pq.ParquetFile(data_file)
                self._dataset_length = pf.metadata.num_rows
            except Exception as e:
                # å¦‚æœparquetè¯»å–å¤±è´¥ï¼Œå°è¯•éå†datasetï¼ˆè¾ƒæ…¢ï¼‰
                print(f"è­¦å‘Š: æ— æ³•ä»Parquetå…ƒæ•°æ®è·å–è¡Œæ•°ï¼Œå°è¯•éå†datasetã€‚é”™è¯¯: {e}")
                try:
                    self._dataset_length = sum(1 for _ in self._hf_dataset)
                except:
                    # å¦‚æœéå†ä¹Ÿå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼
                    print(f"è­¦å‘Š: æ— æ³•ç»Ÿè®¡æ•°æ®é›†è¡Œæ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼ã€‚")
                    self._dataset_length = 1000000  # é»˜è®¤å€¼

    def __len__(self):
        """è¿”å›æ•°æ®é›†å¤§å°"""
        return self._dataset_length

    def __iter__(self):
        """æµå¼æ¨¡å¼ä¸‹çš„è¿­ä»£å™¨"""
        if self._data_list is not None:
            # éæµå¼æ¨¡å¼ï¼šè¿­ä»£ç¼“å­˜åˆ—è¡¨
            return iter(self._data_list)
        else:
            # æµå¼æ¨¡å¼ï¼šç›´æ¥è¿­ä»£Hugging Face dataset
            return self._hf_dataset.__iter__()

    def __getitem__(self, idx):
        """è·å–å•ä¸ªæ ·æœ¬ï¼ˆä»…éæµå¼æ¨¡å¼ï¼‰"""
        if self._data_list is not None:
            # éæµå¼æ¨¡å¼ï¼šç›´æ¥ä»ç¼“å­˜åˆ—è¡¨è·å–
            sample = self._data_list[idx]
        else:
            # æµå¼æ¨¡å¼ï¼šä½¿ç”¨isliceè·³è½¬åˆ°æŒ‡å®šä½ç½®
            # æ³¨æ„ï¼šåœ¨DataLoaderä¸­ä½¿ç”¨IterableDatasetæ—¶ï¼Œ__getitem__ä¸åº”è¯¥è¢«è°ƒç”¨
            from itertools import islice
            sample = next(islice(self._hf_dataset, idx, None))

        # è½¬æ¢ä¸ºTensorï¼ˆå¦‚æœæ˜¯numpyçš„è¯ï¼‰
        result = {
            'x_values': torch.FloatTensor(sample['x_values'])
                if not isinstance(sample['x_values'], torch.Tensor) else sample['x_values'],
            'y_target': torch.FloatTensor(sample['y_target'])
                if not isinstance(sample['y_target'], torch.Tensor) else sample['y_target'],
            'residuals': torch.FloatTensor(sample['residuals'])
                if not isinstance(sample['residuals'], torch.Tensor) else sample['residuals'],
            'z0_token_ids': sample['z0_token_ids'].long()
                if not isinstance(sample['z0_token_ids'], torch.Tensor) else sample['z0_token_ids'],
            'z1_token_ids': sample['z1_token_ids'].long()
                if not isinstance(sample['z1_token_ids'], torch.Tensor) else sample['z1_token_ids'],
            'gap_token': sample['gap_token'].item()
                if isinstance(sample['gap_token'], torch.Tensor) else sample['gap_token']
        }

        return result


def custom_collate_fn(batch):
    """å¤„ç†ä¸åŒç»´åº¦æ•°æ®çš„collateå‡½æ•°ï¼Œä½¿ç”¨padding + maskæ–¹æ¡ˆ

    ä¿®æ”¹ï¼šæ·»åŠ y_targetçš„å¤„ç†ï¼ˆæ¶æ„æ”¹è¿›ï¼šä½¿ç”¨ç›®æ ‡å€¼è€Œéæ®‹å·®ä½œä¸ºæ¡ä»¶ï¼‰
    """
    if len(batch) == 0:
        return {
            'x_values': torch.empty(0, 0, 0),
            'y_target': torch.empty(0, 0),  # ä¿®æ”¹ï¼šæ·»åŠ y_target
            'residuals': torch.empty(0, 0),
            'dim_mask': torch.empty(0, 0),
            'z0_token_ids': torch.empty(0, 0),
            'z1_token_ids': torch.empty(0, 0),
            'gap_token': None
        }

    # æ‰¾åˆ°æœ€å¤§ç»´åº¦å’Œn_points
    max_dim = 0
    max_n_points = 0
    original_dims = []

    for item in batch:
        x_val = item['x_values']  # [n_points, current_dim]
        y_tgt = item['y_target']  # [n_points]

        # ç¡®ä¿x_valuesè‡³å°‘æ˜¯2ç»´çš„
        if x_val.dim() == 1:
            x_val = x_val.unsqueeze(1)  # [n_points, 1]

        # ç¡®ä¿y_targetæ˜¯1ç»´çš„
        if y_tgt.dim() > 1:
            y_tgt = y_tgt.squeeze()

        current_dim = x_val.shape[1]
        current_n_points = x_val.shape[0]

        max_dim = max(max_dim, current_dim)
        max_n_points = max(max_n_points, current_n_points)
        original_dims.append(current_dim)

    # Paddingæ‰€æœ‰æ•°æ®åˆ°æœ€å¤§å½¢çŠ¶
    x_values_padded = []
    y_target_padded = []
    residuals_padded = []
    dim_masks = []
    point_masks = []

    for item in batch:
        x_val = item['x_values'].clone()  # [n_points, current_dim]
        y_tgt = item['y_target'].clone()  # [n_points]
        resid = item['residuals'].clone()  # [n_points]

        # ç¡®ä¿x_valuesè‡³å°‘æ˜¯2ç»´çš„
        if x_val.dim() == 1:
            x_val = x_val.unsqueeze(1)

        # ç¡®ä¿y_targetå’Œresidualsæ˜¯1ç»´çš„
        if y_tgt.dim() > 1:
            y_tgt = y_tgt.squeeze()
        if resid.dim() > 1:
            resid = resid.squeeze()

        current_n_points = x_val.shape[0]
        current_dim = x_val.shape[1]

        # Padding n_pointsç»´åº¦ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if current_n_points < max_n_points:
            padding_points = torch.zeros(max_n_points - current_n_points, x_val.shape[1], dtype=x_val.dtype)
            x_val = torch.cat([x_val, padding_points], dim=0)

            padding_y_tgt = torch.zeros(max_n_points - current_n_points, dtype=y_tgt.dtype)
            y_tgt = torch.cat([y_tgt, padding_y_tgt], dim=0)

            padding_resid = torch.zeros(max_n_points - current_n_points, dtype=resid.dtype)
            resid = torch.cat([resid, padding_resid], dim=0)

        # Padding dimç»´åº¦
        if current_dim < max_dim:
            padding_dim = torch.zeros(x_val.shape[0], max_dim - current_dim, dtype=x_val.dtype)
            x_val = torch.cat([x_val, padding_dim], dim=1)

        # åˆ›å»ºç»´åº¦maskï¼š1è¡¨ç¤ºæœ‰æ•ˆç»´åº¦ï¼Œ0è¡¨ç¤ºpadding
        dim_mask = torch.zeros(max_dim, dtype=torch.float32)
        dim_mask[:current_dim] = 1.0

        # åˆ›å»ºç‚¹maskï¼š1è¡¨ç¤ºçœŸå®ç‚¹ï¼Œ0è¡¨ç¤ºå¡«å……ç‚¹
        point_mask = torch.zeros(max_n_points, dtype=torch.float32)
        point_mask[:current_n_points] = 1.0

        x_values_padded.append(x_val)          # [max_n_points, max_dim]
        y_target_padded.append(y_tgt)          # [max_n_points]
        residuals_padded.append(resid)         # [max_n_points]
        dim_masks.append(dim_mask)             # [max_dim]
        point_masks.append(point_mask)         # [max_n_points]

    result = {
        'x_values': torch.stack(x_values_padded),       # [batch_size, max_n_points, max_dim]
        'y_target': torch.stack(y_target_padded),       # [batch_size, max_n_points]  # æ–°å¢
        'residuals': torch.stack(residuals_padded),     # [batch_size, max_n_points]  # ä¿ç•™
        'dim_mask': torch.stack(dim_masks),             # [batch_size, max_dim]
        'point_mask': torch.stack(point_masks),         # [batch_size, max_n_points]
        'z0_token_ids': torch.stack([item['z0_token_ids'] for item in batch]),
        'z1_token_ids': torch.stack([item['z1_token_ids'] for item in batch]),
        'gap_token': batch[0]['gap_token'],
        'original_dims': original_dims  # è®°å½•åŸå§‹ç»´åº¦ï¼Œä¾›è°ƒè¯•ä½¿ç”¨
    }

    return result


